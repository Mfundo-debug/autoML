2023-07-11 07:39:21,349:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-11 07:39:21,350:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-11 07:39:21,350:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-11 07:39:21,350:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-11 07:39:24,078:WARNING:
'prophet' is a soft dependency and not included in the pycaret installation. Please run: `pip install prophet` to install.
2023-07-11 08:38:40,642:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-11 08:38:40,642:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-11 08:38:40,642:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-11 08:38:40,642:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-11 08:38:43,573:WARNING:
'prophet' is a soft dependency and not included in the pycaret installation. Please run: `pip install prophet` to install.
2023-07-11 08:50:13,781:INFO:PyCaret RegressionExperiment
2023-07-11 08:50:13,781:INFO:Logging name: reg-default-name
2023-07-11 08:50:13,781:INFO:ML Usecase: MLUsecase.REGRESSION
2023-07-11 08:50:13,781:INFO:version 3.0.2
2023-07-11 08:50:13,781:INFO:Initializing setup()
2023-07-11 08:50:13,781:INFO:self.USI: ae74
2023-07-11 08:50:13,781:INFO:self._variable_keys: {'n_jobs_param', 'gpu_n_jobs_param', 'pipeline', 'target_param', 'seed', 'logging_param', 'idx', 'transform_target_param', '_available_plots', 'X', 'USI', 'exp_name_log', 'log_plots_param', 'y', 'y_test', 'X_train', 'y_train', 'memory', 'data', 'X_test', 'fold_shuffle_param', 'gpu_param', 'exp_id', 'fold_generator', 'html_param', 'fold_groups_param', '_ml_usecase'}
2023-07-11 08:50:13,781:INFO:Checking environment
2023-07-11 08:50:13,781:INFO:python_version: 3.9.13
2023-07-11 08:50:13,781:INFO:python_build: ('main', 'Oct 13 2022 21:23:06')
2023-07-11 08:50:13,781:INFO:machine: AMD64
2023-07-11 08:50:13,797:INFO:platform: Windows-10-10.0.22621-SP0
2023-07-11 08:50:13,798:INFO:Memory: svmem(total=16893358080, available=3812052992, percent=77.4, used=13081305088, free=3812052992)
2023-07-11 08:50:13,798:INFO:Physical Core: 8
2023-07-11 08:50:13,798:INFO:Logical Core: 16
2023-07-11 08:50:13,798:INFO:Checking libraries
2023-07-11 08:50:13,798:INFO:System:
2023-07-11 08:50:13,798:INFO:    python: 3.9.13 (main, Oct 13 2022, 21:23:06) [MSC v.1916 64 bit (AMD64)]
2023-07-11 08:50:13,798:INFO:executable: C:\Users\didit\anaconda3\python.exe
2023-07-11 08:50:13,798:INFO:   machine: Windows-10-10.0.22621-SP0
2023-07-11 08:50:13,798:INFO:PyCaret required dependencies:
2023-07-11 08:50:13,798:INFO:                 pip: 23.0.1
2023-07-11 08:50:13,798:INFO:          setuptools: 67.8.0
2023-07-11 08:50:13,798:INFO:             pycaret: 3.0.2
2023-07-11 08:50:13,798:INFO:             IPython: 8.12.0
2023-07-11 08:50:13,798:INFO:          ipywidgets: 8.0.4
2023-07-11 08:50:13,798:INFO:                tqdm: 4.65.0
2023-07-11 08:50:13,798:INFO:               numpy: 1.21.5
2023-07-11 08:50:13,798:INFO:              pandas: 1.5.3
2023-07-11 08:50:13,798:INFO:              jinja2: 3.1.2
2023-07-11 08:50:13,798:INFO:               scipy: 1.10.1
2023-07-11 08:50:13,798:INFO:              joblib: 1.2.0
2023-07-11 08:50:13,798:INFO:             sklearn: 1.2.2
2023-07-11 08:50:13,798:INFO:                pyod: 1.0.9
2023-07-11 08:50:13,798:INFO:            imblearn: 0.10.1
2023-07-11 08:50:13,798:INFO:   category_encoders: 2.6.1
2023-07-11 08:50:13,798:INFO:            lightgbm: 3.3.5
2023-07-11 08:50:13,798:INFO:               numba: 0.57.0
2023-07-11 08:50:13,798:INFO:            requests: 2.29.0
2023-07-11 08:50:13,798:INFO:          matplotlib: 3.7.1
2023-07-11 08:50:13,798:INFO:          scikitplot: 0.3.7
2023-07-11 08:50:13,798:INFO:         yellowbrick: 1.5
2023-07-11 08:50:13,798:INFO:              plotly: 5.9.0
2023-07-11 08:50:13,798:INFO:             kaleido: 0.2.1
2023-07-11 08:50:13,798:INFO:         statsmodels: 0.13.5
2023-07-11 08:50:13,798:INFO:              sktime: 0.17.0
2023-07-11 08:50:13,798:INFO:               tbats: 1.1.3
2023-07-11 08:50:13,798:INFO:            pmdarima: 2.0.3
2023-07-11 08:50:13,798:INFO:              psutil: 5.9.0
2023-07-11 08:50:13,798:INFO:PyCaret optional dependencies:
2023-07-11 08:50:16,746:INFO:                shap: 0.41.0
2023-07-11 08:50:16,746:INFO:           interpret: Not installed
2023-07-11 08:50:16,746:INFO:                umap: Not installed
2023-07-11 08:50:16,746:INFO:    pandas_profiling: 4.3.1
2023-07-11 08:50:16,746:INFO:  explainerdashboard: Not installed
2023-07-11 08:50:16,746:INFO:             autoviz: Not installed
2023-07-11 08:50:16,746:INFO:           fairlearn: Not installed
2023-07-11 08:50:16,746:INFO:             xgboost: 1.7.6
2023-07-11 08:50:16,746:INFO:            catboost: Not installed
2023-07-11 08:50:16,746:INFO:              kmodes: Not installed
2023-07-11 08:50:16,746:INFO:             mlxtend: Not installed
2023-07-11 08:50:16,746:INFO:       statsforecast: Not installed
2023-07-11 08:50:16,746:INFO:        tune_sklearn: Not installed
2023-07-11 08:50:16,746:INFO:                 ray: Not installed
2023-07-11 08:50:16,746:INFO:            hyperopt: Not installed
2023-07-11 08:50:16,746:INFO:              optuna: Not installed
2023-07-11 08:50:16,746:INFO:               skopt: Not installed
2023-07-11 08:50:16,746:INFO:              mlflow: 2.4.2
2023-07-11 08:50:16,746:INFO:              gradio: Not installed
2023-07-11 08:50:16,746:INFO:             fastapi: 0.95.2
2023-07-11 08:50:16,746:INFO:             uvicorn: 0.22.0
2023-07-11 08:50:16,746:INFO:              m2cgen: Not installed
2023-07-11 08:50:16,746:INFO:           evidently: Not installed
2023-07-11 08:50:16,746:INFO:               fugue: Not installed
2023-07-11 08:50:16,746:INFO:           streamlit: 1.23.1
2023-07-11 08:50:16,746:INFO:             prophet: Not installed
2023-07-11 08:50:16,746:INFO:None
2023-07-11 08:50:16,746:INFO:Set up data.
2023-07-11 08:50:17,108:INFO:Set up train/test split.
2023-07-11 08:50:17,187:INFO:Set up index.
2023-07-11 08:50:17,204:INFO:Set up folding strategy.
2023-07-11 08:50:17,204:INFO:Assigning column types.
2023-07-11 08:50:17,257:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-07-11 08:50:17,257:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-07-11 08:50:17,266:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-07-11 08:50:17,282:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 08:50:17,535:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 08:50:17,691:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 08:50:17,691:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 08:50:18,447:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 08:50:18,447:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-07-11 08:50:18,463:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-07-11 08:50:18,478:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 08:50:18,700:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 08:50:18,810:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 08:50:18,810:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 08:50:18,826:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 08:50:18,826:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-07-11 08:50:18,826:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-07-11 08:50:18,841:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 08:50:19,093:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 08:50:19,212:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 08:50:19,212:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 08:50:19,227:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 08:50:19,227:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-07-11 08:50:19,243:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 08:50:19,447:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 08:50:19,605:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 08:50:19,605:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 08:50:19,621:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 08:50:19,621:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-07-11 08:50:19,652:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 08:50:19,872:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 08:50:19,967:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 08:50:19,967:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 08:50:19,983:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 08:50:20,014:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 08:50:20,219:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 08:50:20,329:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 08:50:20,329:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 08:50:20,358:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 08:50:20,360:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-07-11 08:50:20,597:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 08:50:20,755:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 08:50:20,755:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 08:50:20,755:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 08:50:20,999:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 08:50:21,109:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 08:50:21,109:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 08:50:21,125:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 08:50:21,125:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-07-11 08:50:21,330:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 08:50:21,440:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 08:50:21,455:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 08:50:21,708:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 08:50:21,802:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 08:50:21,818:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 08:50:21,818:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-07-11 08:50:22,133:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 08:50:22,133:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 08:50:22,463:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 08:50:22,463:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 08:50:22,495:INFO:Preparing preprocessing pipeline...
2023-07-11 08:50:22,495:INFO:Set up simple imputation.
2023-07-11 08:50:22,543:INFO:Set up encoding of ordinal features.
2023-07-11 08:50:22,574:INFO:Set up encoding of categorical features.
2023-07-11 08:50:22,574:INFO:Set up column name cleaning.
2023-07-11 08:50:44,839:INFO:PyCaret RegressionExperiment
2023-07-11 08:50:44,839:INFO:Logging name: reg-default-name
2023-07-11 08:50:44,839:INFO:ML Usecase: MLUsecase.REGRESSION
2023-07-11 08:50:44,839:INFO:version 3.0.2
2023-07-11 08:50:44,839:INFO:Initializing setup()
2023-07-11 08:50:44,839:INFO:self.USI: 6ef0
2023-07-11 08:50:44,839:INFO:self._variable_keys: {'n_jobs_param', 'gpu_n_jobs_param', 'pipeline', 'target_param', 'seed', 'logging_param', 'idx', 'transform_target_param', '_available_plots', 'X', 'USI', 'exp_name_log', 'log_plots_param', 'y', 'y_test', 'X_train', 'y_train', 'memory', 'data', 'X_test', 'fold_shuffle_param', 'gpu_param', 'exp_id', 'fold_generator', 'html_param', 'fold_groups_param', '_ml_usecase'}
2023-07-11 08:50:44,839:INFO:Checking environment
2023-07-11 08:50:44,839:INFO:python_version: 3.9.13
2023-07-11 08:50:44,839:INFO:python_build: ('main', 'Oct 13 2022 21:23:06')
2023-07-11 08:50:44,839:INFO:machine: AMD64
2023-07-11 08:50:44,839:INFO:platform: Windows-10-10.0.22621-SP0
2023-07-11 08:50:44,839:INFO:Memory: svmem(total=16893358080, available=3715887104, percent=78.0, used=13177470976, free=3715887104)
2023-07-11 08:50:44,839:INFO:Physical Core: 8
2023-07-11 08:50:44,839:INFO:Logical Core: 16
2023-07-11 08:50:44,839:INFO:Checking libraries
2023-07-11 08:50:44,839:INFO:System:
2023-07-11 08:50:44,839:INFO:    python: 3.9.13 (main, Oct 13 2022, 21:23:06) [MSC v.1916 64 bit (AMD64)]
2023-07-11 08:50:44,839:INFO:executable: C:\Users\didit\anaconda3\python.exe
2023-07-11 08:50:44,839:INFO:   machine: Windows-10-10.0.22621-SP0
2023-07-11 08:50:44,839:INFO:PyCaret required dependencies:
2023-07-11 08:50:44,839:INFO:                 pip: 23.0.1
2023-07-11 08:50:44,839:INFO:          setuptools: 67.8.0
2023-07-11 08:50:44,839:INFO:             pycaret: 3.0.2
2023-07-11 08:50:44,839:INFO:             IPython: 8.12.0
2023-07-11 08:50:44,839:INFO:          ipywidgets: 8.0.4
2023-07-11 08:50:44,839:INFO:                tqdm: 4.65.0
2023-07-11 08:50:44,839:INFO:               numpy: 1.21.5
2023-07-11 08:50:44,839:INFO:              pandas: 1.5.3
2023-07-11 08:50:44,839:INFO:              jinja2: 3.1.2
2023-07-11 08:50:44,839:INFO:               scipy: 1.10.1
2023-07-11 08:50:44,839:INFO:              joblib: 1.2.0
2023-07-11 08:50:44,839:INFO:             sklearn: 1.2.2
2023-07-11 08:50:44,839:INFO:                pyod: 1.0.9
2023-07-11 08:50:44,839:INFO:            imblearn: 0.10.1
2023-07-11 08:50:44,839:INFO:   category_encoders: 2.6.1
2023-07-11 08:50:44,839:INFO:            lightgbm: 3.3.5
2023-07-11 08:50:44,839:INFO:               numba: 0.57.0
2023-07-11 08:50:44,839:INFO:            requests: 2.29.0
2023-07-11 08:50:44,839:INFO:          matplotlib: 3.7.1
2023-07-11 08:50:44,839:INFO:          scikitplot: 0.3.7
2023-07-11 08:50:44,839:INFO:         yellowbrick: 1.5
2023-07-11 08:50:44,839:INFO:              plotly: 5.9.0
2023-07-11 08:50:44,839:INFO:             kaleido: 0.2.1
2023-07-11 08:50:44,839:INFO:         statsmodels: 0.13.5
2023-07-11 08:50:44,839:INFO:              sktime: 0.17.0
2023-07-11 08:50:44,839:INFO:               tbats: 1.1.3
2023-07-11 08:50:44,839:INFO:            pmdarima: 2.0.3
2023-07-11 08:50:44,839:INFO:              psutil: 5.9.0
2023-07-11 08:50:44,839:INFO:PyCaret optional dependencies:
2023-07-11 08:50:44,839:INFO:                shap: 0.41.0
2023-07-11 08:50:44,839:INFO:           interpret: Not installed
2023-07-11 08:50:44,839:INFO:                umap: Not installed
2023-07-11 08:50:44,839:INFO:    pandas_profiling: 4.3.1
2023-07-11 08:50:44,839:INFO:  explainerdashboard: Not installed
2023-07-11 08:50:44,839:INFO:             autoviz: Not installed
2023-07-11 08:50:44,839:INFO:           fairlearn: Not installed
2023-07-11 08:50:44,839:INFO:             xgboost: 1.7.6
2023-07-11 08:50:44,839:INFO:            catboost: Not installed
2023-07-11 08:50:44,839:INFO:              kmodes: Not installed
2023-07-11 08:50:44,839:INFO:             mlxtend: Not installed
2023-07-11 08:50:44,839:INFO:       statsforecast: Not installed
2023-07-11 08:50:44,839:INFO:        tune_sklearn: Not installed
2023-07-11 08:50:44,839:INFO:                 ray: Not installed
2023-07-11 08:50:44,839:INFO:            hyperopt: Not installed
2023-07-11 08:50:44,839:INFO:              optuna: Not installed
2023-07-11 08:50:44,839:INFO:               skopt: Not installed
2023-07-11 08:50:44,839:INFO:              mlflow: 2.4.2
2023-07-11 08:50:44,839:INFO:              gradio: Not installed
2023-07-11 08:50:44,839:INFO:             fastapi: 0.95.2
2023-07-11 08:50:44,839:INFO:             uvicorn: 0.22.0
2023-07-11 08:50:44,839:INFO:              m2cgen: Not installed
2023-07-11 08:50:44,839:INFO:           evidently: Not installed
2023-07-11 08:50:44,839:INFO:               fugue: Not installed
2023-07-11 08:50:44,839:INFO:           streamlit: 1.23.1
2023-07-11 08:50:44,839:INFO:             prophet: Not installed
2023-07-11 08:50:44,839:INFO:None
2023-07-11 08:50:44,839:INFO:Set up data.
2023-07-11 08:50:45,153:INFO:Set up train/test split.
2023-07-11 08:50:45,232:INFO:Set up index.
2023-07-11 08:50:45,240:INFO:Set up folding strategy.
2023-07-11 08:50:45,240:INFO:Assigning column types.
2023-07-11 08:50:45,295:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-07-11 08:50:45,295:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-07-11 08:50:45,310:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-07-11 08:50:45,310:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 08:50:45,515:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 08:50:45,658:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 08:50:45,658:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 08:50:45,658:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 08:50:45,658:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-07-11 08:50:45,675:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-07-11 08:50:45,692:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 08:50:45,879:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 08:50:45,989:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 08:50:46,005:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 08:50:46,005:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 08:50:46,005:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-07-11 08:50:46,020:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-07-11 08:50:46,020:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 08:50:46,209:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 08:50:46,319:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 08:50:46,319:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 08:50:46,319:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 08:50:46,336:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-07-11 08:50:46,342:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 08:50:46,523:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 08:50:46,633:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 08:50:46,633:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 08:50:46,642:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 08:50:46,642:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-07-11 08:50:46,665:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 08:50:46,901:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 08:50:46,996:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 08:50:46,996:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 08:50:47,011:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 08:50:47,027:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 08:50:47,263:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 08:50:47,373:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 08:50:47,373:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 08:50:47,373:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 08:50:47,373:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-07-11 08:50:47,593:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 08:50:47,688:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 08:50:47,688:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 08:50:47,703:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 08:50:47,946:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 08:50:48,049:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 08:50:48,049:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 08:50:48,049:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 08:50:48,049:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-07-11 08:50:48,270:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 08:50:48,380:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 08:50:48,396:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 08:50:48,601:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 08:50:48,711:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 08:50:48,711:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 08:50:48,711:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-07-11 08:50:49,057:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 08:50:49,057:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 08:50:49,365:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 08:50:49,365:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 08:50:49,380:INFO:Preparing preprocessing pipeline...
2023-07-11 08:50:49,380:INFO:Set up simple imputation.
2023-07-11 08:50:49,412:INFO:Set up encoding of categorical features.
2023-07-11 08:50:49,427:INFO:Set up column name cleaning.
2023-07-11 08:51:05,696:INFO:PyCaret RegressionExperiment
2023-07-11 08:51:05,696:INFO:Logging name: reg-default-name
2023-07-11 08:51:05,696:INFO:ML Usecase: MLUsecase.REGRESSION
2023-07-11 08:51:05,696:INFO:version 3.0.2
2023-07-11 08:51:05,696:INFO:Initializing setup()
2023-07-11 08:51:05,696:INFO:self.USI: 5eb2
2023-07-11 08:51:05,696:INFO:self._variable_keys: {'n_jobs_param', 'gpu_n_jobs_param', 'pipeline', 'target_param', 'seed', 'logging_param', 'idx', 'transform_target_param', '_available_plots', 'X', 'USI', 'exp_name_log', 'log_plots_param', 'y', 'y_test', 'X_train', 'y_train', 'memory', 'data', 'X_test', 'fold_shuffle_param', 'gpu_param', 'exp_id', 'fold_generator', 'html_param', 'fold_groups_param', '_ml_usecase'}
2023-07-11 08:51:05,696:INFO:Checking environment
2023-07-11 08:51:05,696:INFO:python_version: 3.9.13
2023-07-11 08:51:05,696:INFO:python_build: ('main', 'Oct 13 2022 21:23:06')
2023-07-11 08:51:05,708:INFO:machine: AMD64
2023-07-11 08:51:05,709:INFO:platform: Windows-10-10.0.22621-SP0
2023-07-11 08:51:05,710:INFO:Memory: svmem(total=16893358080, available=3740889088, percent=77.9, used=13152468992, free=3740889088)
2023-07-11 08:51:05,710:INFO:Physical Core: 8
2023-07-11 08:51:05,710:INFO:Logical Core: 16
2023-07-11 08:51:05,710:INFO:Checking libraries
2023-07-11 08:51:05,711:INFO:System:
2023-07-11 08:51:05,711:INFO:    python: 3.9.13 (main, Oct 13 2022, 21:23:06) [MSC v.1916 64 bit (AMD64)]
2023-07-11 08:51:05,711:INFO:executable: C:\Users\didit\anaconda3\python.exe
2023-07-11 08:51:05,711:INFO:   machine: Windows-10-10.0.22621-SP0
2023-07-11 08:51:05,711:INFO:PyCaret required dependencies:
2023-07-11 08:51:05,711:INFO:                 pip: 23.0.1
2023-07-11 08:51:05,711:INFO:          setuptools: 67.8.0
2023-07-11 08:51:05,711:INFO:             pycaret: 3.0.2
2023-07-11 08:51:05,713:INFO:             IPython: 8.12.0
2023-07-11 08:51:05,713:INFO:          ipywidgets: 8.0.4
2023-07-11 08:51:05,713:INFO:                tqdm: 4.65.0
2023-07-11 08:51:05,713:INFO:               numpy: 1.21.5
2023-07-11 08:51:05,713:INFO:              pandas: 1.5.3
2023-07-11 08:51:05,714:INFO:              jinja2: 3.1.2
2023-07-11 08:51:05,714:INFO:               scipy: 1.10.1
2023-07-11 08:51:05,715:INFO:              joblib: 1.2.0
2023-07-11 08:51:05,715:INFO:             sklearn: 1.2.2
2023-07-11 08:51:05,715:INFO:                pyod: 1.0.9
2023-07-11 08:51:05,715:INFO:            imblearn: 0.10.1
2023-07-11 08:51:05,715:INFO:   category_encoders: 2.6.1
2023-07-11 08:51:05,715:INFO:            lightgbm: 3.3.5
2023-07-11 08:51:05,715:INFO:               numba: 0.57.0
2023-07-11 08:51:05,715:INFO:            requests: 2.29.0
2023-07-11 08:51:05,715:INFO:          matplotlib: 3.7.1
2023-07-11 08:51:05,715:INFO:          scikitplot: 0.3.7
2023-07-11 08:51:05,715:INFO:         yellowbrick: 1.5
2023-07-11 08:51:05,715:INFO:              plotly: 5.9.0
2023-07-11 08:51:05,715:INFO:             kaleido: 0.2.1
2023-07-11 08:51:05,720:INFO:         statsmodels: 0.13.5
2023-07-11 08:51:05,720:INFO:              sktime: 0.17.0
2023-07-11 08:51:05,720:INFO:               tbats: 1.1.3
2023-07-11 08:51:05,720:INFO:            pmdarima: 2.0.3
2023-07-11 08:51:05,720:INFO:              psutil: 5.9.0
2023-07-11 08:51:05,720:INFO:PyCaret optional dependencies:
2023-07-11 08:51:05,720:INFO:                shap: 0.41.0
2023-07-11 08:51:05,720:INFO:           interpret: Not installed
2023-07-11 08:51:05,720:INFO:                umap: Not installed
2023-07-11 08:51:05,720:INFO:    pandas_profiling: 4.3.1
2023-07-11 08:51:05,720:INFO:  explainerdashboard: Not installed
2023-07-11 08:51:05,720:INFO:             autoviz: Not installed
2023-07-11 08:51:05,720:INFO:           fairlearn: Not installed
2023-07-11 08:51:05,720:INFO:             xgboost: 1.7.6
2023-07-11 08:51:05,720:INFO:            catboost: Not installed
2023-07-11 08:51:05,720:INFO:              kmodes: Not installed
2023-07-11 08:51:05,720:INFO:             mlxtend: Not installed
2023-07-11 08:51:05,720:INFO:       statsforecast: Not installed
2023-07-11 08:51:05,720:INFO:        tune_sklearn: Not installed
2023-07-11 08:51:05,728:INFO:                 ray: Not installed
2023-07-11 08:51:05,729:INFO:            hyperopt: Not installed
2023-07-11 08:51:05,729:INFO:              optuna: Not installed
2023-07-11 08:51:05,729:INFO:               skopt: Not installed
2023-07-11 08:51:05,729:INFO:              mlflow: 2.4.2
2023-07-11 08:51:05,729:INFO:              gradio: Not installed
2023-07-11 08:51:05,729:INFO:             fastapi: 0.95.2
2023-07-11 08:51:05,729:INFO:             uvicorn: 0.22.0
2023-07-11 08:51:05,729:INFO:              m2cgen: Not installed
2023-07-11 08:51:05,729:INFO:           evidently: Not installed
2023-07-11 08:51:05,729:INFO:               fugue: Not installed
2023-07-11 08:51:05,729:INFO:           streamlit: 1.23.1
2023-07-11 08:51:05,729:INFO:             prophet: Not installed
2023-07-11 08:51:05,729:INFO:None
2023-07-11 08:51:05,729:INFO:Set up data.
2023-07-11 08:51:06,029:INFO:Set up train/test split.
2023-07-11 08:51:06,108:INFO:Set up index.
2023-07-11 08:51:06,125:INFO:Set up folding strategy.
2023-07-11 08:51:06,128:INFO:Assigning column types.
2023-07-11 08:51:06,171:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-07-11 08:51:06,171:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-07-11 08:51:06,187:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-07-11 08:51:06,187:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 08:51:06,391:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 08:51:06,517:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 08:51:06,517:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 08:51:06,517:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 08:51:06,517:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-07-11 08:51:06,533:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-07-11 08:51:06,549:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 08:51:06,769:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 08:51:06,895:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 08:51:06,895:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 08:51:06,895:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 08:51:06,895:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-07-11 08:51:06,911:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-07-11 08:51:06,927:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 08:51:07,133:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 08:51:07,242:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 08:51:07,242:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 08:51:07,258:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 08:51:07,273:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-07-11 08:51:07,273:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 08:51:07,463:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 08:51:07,582:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 08:51:07,582:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 08:51:07,591:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 08:51:07,591:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-07-11 08:51:07,613:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 08:51:07,817:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 08:51:07,942:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 08:51:07,942:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 08:51:07,958:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 08:51:07,974:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 08:51:08,159:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 08:51:08,257:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 08:51:08,257:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 08:51:08,257:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 08:51:08,257:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-07-11 08:51:08,478:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 08:51:08,572:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 08:51:08,572:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 08:51:08,592:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 08:51:08,793:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 08:51:08,887:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 08:51:08,887:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 08:51:08,893:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 08:51:08,893:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-07-11 08:51:09,108:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 08:51:09,219:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 08:51:09,219:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 08:51:09,409:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 08:51:09,565:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 08:51:09,565:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 08:51:09,565:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-07-11 08:51:09,959:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 08:51:09,961:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 08:51:10,307:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 08:51:10,307:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 08:51:10,307:INFO:Preparing preprocessing pipeline...
2023-07-11 08:51:10,307:INFO:Set up simple imputation.
2023-07-11 08:51:10,354:INFO:Set up encoding of ordinal features.
2023-07-11 08:51:10,376:INFO:Set up encoding of categorical features.
2023-07-11 08:51:10,386:INFO:Set up column name cleaning.
2023-07-11 08:51:14,908:INFO:Finished creating preprocessing pipeline.
2023-07-11 08:51:14,985:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\didit\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['Postal Code', 'Model Year',
                                             'Base MSRP',
                                             'Legislative District',
                                             'DOL Vehicle ID',
                                             '2020 Census Tract', 'Longitude',
                                             'latitude'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['County', 'City', 'Sta...
                                    transformer=OneHotEncoder(cols=['State',
                                                                    'CAFV'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('rest_encoding',
                 TransformerWrapper(include=['County', 'City', 'Make', 'Model',
                                             'Electric Utility'],
                                    transformer=TargetEncoder(cols=['County',
                                                                    'City',
                                                                    'Make',
                                                                    'Model',
                                                                    'Electric '
                                                                    'Utility'],
                                                              handle_missing='return_nan'))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames()))])
2023-07-11 08:51:14,985:INFO:Creating final display dataframe.
2023-07-11 08:51:18,821:INFO:Setup _display_container:                     Description             Value
0                    Session id              3428
1                        Target    Electric Range
2                   Target type        Regression
3           Original data shape      (134474, 17)
4        Transformed data shape      (134474, 19)
5   Transformed train set shape       (94131, 19)
6    Transformed test set shape       (40343, 19)
7              Ordinal features                 1
8              Numeric features                 8
9          Categorical features                 8
10                   Preprocess              True
11              Imputation type            simple
12           Numeric imputation              mean
13       Categorical imputation              mode
14     Maximum one-hot encoding                25
15              Encoding method              None
16               Fold Generator             KFold
17                  Fold Number                10
18                     CPU Jobs                -1
19                      Use GPU             False
20               Log Experiment             False
21              Experiment Name  reg-default-name
22                          USI              5eb2
2023-07-11 08:51:19,167:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 08:51:19,167:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 08:51:19,466:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 08:51:19,466:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 08:51:19,466:INFO:setup() successfully completed in 13.78s...............
2023-07-11 08:51:19,528:INFO:Initializing compare_models()
2023-07-11 08:51:19,528:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000014F870AF790>, include=None, fold=None, round=4, cross_validation=True, sort=R2, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x0000014F870AF790>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2023-07-11 08:51:19,528:INFO:Checking exceptions
2023-07-11 08:51:19,578:INFO:Preparing display monitor
2023-07-11 08:51:19,592:INFO:Initializing Linear Regression
2023-07-11 08:51:19,592:INFO:Total runtime is 0.0 minutes
2023-07-11 08:51:19,592:INFO:SubProcess create_model() called ==================================
2023-07-11 08:51:19,593:INFO:Initializing create_model()
2023-07-11 08:51:19,593:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000014F870AF790>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000014F85D67F70>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 08:51:19,593:INFO:Checking exceptions
2023-07-11 08:51:19,593:INFO:Importing libraries
2023-07-11 08:51:19,593:INFO:Copying training dataset
2023-07-11 08:51:19,671:INFO:Defining folds
2023-07-11 08:51:19,671:INFO:Declaring metric variables
2023-07-11 08:51:19,671:INFO:Importing untrained model
2023-07-11 08:51:19,671:INFO:Linear Regression Imported successfully
2023-07-11 08:51:19,671:INFO:Starting cross validation
2023-07-11 08:51:19,688:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 08:51:35,088:INFO:Calculating mean and std
2023-07-11 08:51:35,088:INFO:Creating metrics dataframe
2023-07-11 08:51:35,182:INFO:Uploading results into container
2023-07-11 08:51:35,182:INFO:Uploading model into container now
2023-07-11 08:51:35,198:INFO:_master_model_container: 1
2023-07-11 08:51:35,198:INFO:_display_container: 2
2023-07-11 08:51:35,198:INFO:LinearRegression(n_jobs=-1)
2023-07-11 08:51:35,198:INFO:create_model() successfully completed......................................
2023-07-11 08:51:35,561:INFO:SubProcess create_model() end ==================================
2023-07-11 08:51:35,561:INFO:Creating metrics dataframe
2023-07-11 08:51:35,561:INFO:Initializing Lasso Regression
2023-07-11 08:51:35,561:INFO:Total runtime is 0.26615331172943113 minutes
2023-07-11 08:51:35,561:INFO:SubProcess create_model() called ==================================
2023-07-11 08:51:35,561:INFO:Initializing create_model()
2023-07-11 08:51:35,577:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000014F870AF790>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000014F85D67F70>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 08:51:35,577:INFO:Checking exceptions
2023-07-11 08:51:35,577:INFO:Importing libraries
2023-07-11 08:51:35,577:INFO:Copying training dataset
2023-07-11 08:51:35,656:INFO:Defining folds
2023-07-11 08:51:35,656:INFO:Declaring metric variables
2023-07-11 08:51:35,656:INFO:Importing untrained model
2023-07-11 08:51:35,656:INFO:Lasso Regression Imported successfully
2023-07-11 08:51:35,671:INFO:Starting cross validation
2023-07-11 08:51:35,675:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 08:51:47,632:INFO:Calculating mean and std
2023-07-11 08:51:47,632:INFO:Creating metrics dataframe
2023-07-11 08:51:47,726:INFO:Uploading results into container
2023-07-11 08:51:47,726:INFO:Uploading model into container now
2023-07-11 08:51:47,726:INFO:_master_model_container: 2
2023-07-11 08:51:47,726:INFO:_display_container: 2
2023-07-11 08:51:47,726:INFO:Lasso(random_state=3428)
2023-07-11 08:51:47,726:INFO:create_model() successfully completed......................................
2023-07-11 08:51:47,957:INFO:SubProcess create_model() end ==================================
2023-07-11 08:51:47,957:INFO:Creating metrics dataframe
2023-07-11 08:51:47,972:INFO:Initializing Ridge Regression
2023-07-11 08:51:47,972:INFO:Total runtime is 0.4730019092559814 minutes
2023-07-11 08:51:47,972:INFO:SubProcess create_model() called ==================================
2023-07-11 08:51:47,972:INFO:Initializing create_model()
2023-07-11 08:51:47,972:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000014F870AF790>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000014F85D67F70>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 08:51:47,972:INFO:Checking exceptions
2023-07-11 08:51:47,972:INFO:Importing libraries
2023-07-11 08:51:47,972:INFO:Copying training dataset
2023-07-11 08:51:48,067:INFO:Defining folds
2023-07-11 08:51:48,067:INFO:Declaring metric variables
2023-07-11 08:51:48,067:INFO:Importing untrained model
2023-07-11 08:51:48,067:INFO:Ridge Regression Imported successfully
2023-07-11 08:51:48,067:INFO:Starting cross validation
2023-07-11 08:51:48,067:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 08:51:49,734:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.59025e-21): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-11 08:51:49,734:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.58744e-21): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-11 08:51:49,797:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.59338e-21): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-11 08:51:49,860:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.58737e-21): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-11 08:51:49,923:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.58999e-21): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-11 08:51:50,034:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.58455e-21): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-11 08:51:50,152:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.58802e-21): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-11 08:51:50,161:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.58984e-21): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-11 08:51:50,222:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.58959e-21): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-11 08:51:50,316:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.59066e-21): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-11 08:51:51,857:INFO:Calculating mean and std
2023-07-11 08:51:51,857:INFO:Creating metrics dataframe
2023-07-11 08:51:51,968:INFO:Uploading results into container
2023-07-11 08:51:51,968:INFO:Uploading model into container now
2023-07-11 08:51:51,968:INFO:_master_model_container: 3
2023-07-11 08:51:51,968:INFO:_display_container: 2
2023-07-11 08:51:51,968:INFO:Ridge(random_state=3428)
2023-07-11 08:51:51,968:INFO:create_model() successfully completed......................................
2023-07-11 08:51:52,251:INFO:SubProcess create_model() end ==================================
2023-07-11 08:51:52,251:INFO:Creating metrics dataframe
2023-07-11 08:51:52,264:INFO:Initializing Elastic Net
2023-07-11 08:51:52,264:INFO:Total runtime is 0.5445334553718566 minutes
2023-07-11 08:51:52,264:INFO:SubProcess create_model() called ==================================
2023-07-11 08:51:52,264:INFO:Initializing create_model()
2023-07-11 08:51:52,264:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000014F870AF790>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000014F85D67F70>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 08:51:52,264:INFO:Checking exceptions
2023-07-11 08:51:52,264:INFO:Importing libraries
2023-07-11 08:51:52,264:INFO:Copying training dataset
2023-07-11 08:51:52,347:INFO:Defining folds
2023-07-11 08:51:52,347:INFO:Declaring metric variables
2023-07-11 08:51:52,347:INFO:Importing untrained model
2023-07-11 08:51:52,347:INFO:Elastic Net Imported successfully
2023-07-11 08:51:52,347:INFO:Starting cross validation
2023-07-11 08:51:52,347:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 08:51:56,305:INFO:Calculating mean and std
2023-07-11 08:51:56,305:INFO:Creating metrics dataframe
2023-07-11 08:51:56,447:INFO:Uploading results into container
2023-07-11 08:51:56,463:INFO:Uploading model into container now
2023-07-11 08:51:56,463:INFO:_master_model_container: 4
2023-07-11 08:51:56,469:INFO:_display_container: 2
2023-07-11 08:51:56,469:INFO:ElasticNet(random_state=3428)
2023-07-11 08:51:56,469:INFO:create_model() successfully completed......................................
2023-07-11 08:51:56,730:INFO:SubProcess create_model() end ==================================
2023-07-11 08:51:56,730:INFO:Creating metrics dataframe
2023-07-11 08:51:56,748:INFO:Initializing Least Angle Regression
2023-07-11 08:51:56,748:INFO:Total runtime is 0.6192672928174336 minutes
2023-07-11 08:51:56,748:INFO:SubProcess create_model() called ==================================
2023-07-11 08:51:56,748:INFO:Initializing create_model()
2023-07-11 08:51:56,748:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000014F870AF790>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000014F85D67F70>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 08:51:56,748:INFO:Checking exceptions
2023-07-11 08:51:56,748:INFO:Importing libraries
2023-07-11 08:51:56,748:INFO:Copying training dataset
2023-07-11 08:51:56,840:INFO:Defining folds
2023-07-11 08:51:56,840:INFO:Declaring metric variables
2023-07-11 08:51:56,840:INFO:Importing untrained model
2023-07-11 08:51:56,840:INFO:Least Angle Regression Imported successfully
2023-07-11 08:51:56,840:INFO:Starting cross validation
2023-07-11 08:51:56,840:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 08:52:00,654:INFO:Calculating mean and std
2023-07-11 08:52:00,654:INFO:Creating metrics dataframe
2023-07-11 08:52:00,828:INFO:Uploading results into container
2023-07-11 08:52:00,828:INFO:Uploading model into container now
2023-07-11 08:52:00,828:INFO:_master_model_container: 5
2023-07-11 08:52:00,828:INFO:_display_container: 2
2023-07-11 08:52:00,828:INFO:Lars(random_state=3428)
2023-07-11 08:52:00,828:INFO:create_model() successfully completed......................................
2023-07-11 08:52:01,095:INFO:SubProcess create_model() end ==================================
2023-07-11 08:52:01,095:INFO:Creating metrics dataframe
2023-07-11 08:52:01,111:INFO:Initializing Lasso Least Angle Regression
2023-07-11 08:52:01,111:INFO:Total runtime is 0.6919748425483703 minutes
2023-07-11 08:52:01,111:INFO:SubProcess create_model() called ==================================
2023-07-11 08:52:01,111:INFO:Initializing create_model()
2023-07-11 08:52:01,111:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000014F870AF790>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000014F85D67F70>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 08:52:01,111:INFO:Checking exceptions
2023-07-11 08:52:01,111:INFO:Importing libraries
2023-07-11 08:52:01,111:INFO:Copying training dataset
2023-07-11 08:52:01,220:INFO:Defining folds
2023-07-11 08:52:01,220:INFO:Declaring metric variables
2023-07-11 08:52:01,220:INFO:Importing untrained model
2023-07-11 08:52:01,220:INFO:Lasso Least Angle Regression Imported successfully
2023-07-11 08:52:01,220:INFO:Starting cross validation
2023-07-11 08:52:01,236:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 08:52:05,119:INFO:Calculating mean and std
2023-07-11 08:52:05,119:INFO:Creating metrics dataframe
2023-07-11 08:52:05,292:INFO:Uploading results into container
2023-07-11 08:52:05,292:INFO:Uploading model into container now
2023-07-11 08:52:05,292:INFO:_master_model_container: 6
2023-07-11 08:52:05,292:INFO:_display_container: 2
2023-07-11 08:52:05,292:INFO:LassoLars(random_state=3428)
2023-07-11 08:52:05,292:INFO:create_model() successfully completed......................................
2023-07-11 08:52:05,561:INFO:SubProcess create_model() end ==================================
2023-07-11 08:52:05,561:INFO:Creating metrics dataframe
2023-07-11 08:52:05,582:INFO:Initializing Orthogonal Matching Pursuit
2023-07-11 08:52:05,582:INFO:Total runtime is 0.7664927522341409 minutes
2023-07-11 08:52:05,582:INFO:SubProcess create_model() called ==================================
2023-07-11 08:52:05,582:INFO:Initializing create_model()
2023-07-11 08:52:05,582:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000014F870AF790>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000014F85D67F70>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 08:52:05,582:INFO:Checking exceptions
2023-07-11 08:52:05,582:INFO:Importing libraries
2023-07-11 08:52:05,582:INFO:Copying training dataset
2023-07-11 08:52:05,686:INFO:Defining folds
2023-07-11 08:52:05,686:INFO:Declaring metric variables
2023-07-11 08:52:05,686:INFO:Importing untrained model
2023-07-11 08:52:05,686:INFO:Orthogonal Matching Pursuit Imported successfully
2023-07-11 08:52:05,686:INFO:Starting cross validation
2023-07-11 08:52:05,686:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 08:52:09,545:INFO:Calculating mean and std
2023-07-11 08:52:09,545:INFO:Creating metrics dataframe
2023-07-11 08:52:09,689:INFO:Uploading results into container
2023-07-11 08:52:09,689:INFO:Uploading model into container now
2023-07-11 08:52:09,689:INFO:_master_model_container: 7
2023-07-11 08:52:09,689:INFO:_display_container: 2
2023-07-11 08:52:09,689:INFO:OrthogonalMatchingPursuit()
2023-07-11 08:52:09,689:INFO:create_model() successfully completed......................................
2023-07-11 08:52:09,938:INFO:SubProcess create_model() end ==================================
2023-07-11 08:52:09,938:INFO:Creating metrics dataframe
2023-07-11 08:52:09,969:INFO:Initializing Bayesian Ridge
2023-07-11 08:52:09,969:INFO:Total runtime is 0.8396197915077208 minutes
2023-07-11 08:52:09,969:INFO:SubProcess create_model() called ==================================
2023-07-11 08:52:09,969:INFO:Initializing create_model()
2023-07-11 08:52:09,969:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000014F870AF790>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000014F85D67F70>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 08:52:09,969:INFO:Checking exceptions
2023-07-11 08:52:09,969:INFO:Importing libraries
2023-07-11 08:52:09,969:INFO:Copying training dataset
2023-07-11 08:52:10,064:INFO:Defining folds
2023-07-11 08:52:10,064:INFO:Declaring metric variables
2023-07-11 08:52:10,064:INFO:Importing untrained model
2023-07-11 08:52:10,064:INFO:Bayesian Ridge Imported successfully
2023-07-11 08:52:10,064:INFO:Starting cross validation
2023-07-11 08:52:10,064:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 08:52:14,421:INFO:Calculating mean and std
2023-07-11 08:52:14,421:INFO:Creating metrics dataframe
2023-07-11 08:52:14,609:INFO:Uploading results into container
2023-07-11 08:52:14,609:INFO:Uploading model into container now
2023-07-11 08:52:14,609:INFO:_master_model_container: 8
2023-07-11 08:52:14,609:INFO:_display_container: 2
2023-07-11 08:52:14,609:INFO:BayesianRidge()
2023-07-11 08:52:14,609:INFO:create_model() successfully completed......................................
2023-07-11 08:52:14,876:INFO:SubProcess create_model() end ==================================
2023-07-11 08:52:14,876:INFO:Creating metrics dataframe
2023-07-11 08:52:14,876:INFO:Initializing Passive Aggressive Regressor
2023-07-11 08:52:14,876:INFO:Total runtime is 0.9214059750239053 minutes
2023-07-11 08:52:14,876:INFO:SubProcess create_model() called ==================================
2023-07-11 08:52:14,892:INFO:Initializing create_model()
2023-07-11 08:52:14,892:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000014F870AF790>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000014F85D67F70>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 08:52:14,892:INFO:Checking exceptions
2023-07-11 08:52:14,892:INFO:Importing libraries
2023-07-11 08:52:14,892:INFO:Copying training dataset
2023-07-11 08:52:14,965:INFO:Defining folds
2023-07-11 08:52:14,965:INFO:Declaring metric variables
2023-07-11 08:52:14,965:INFO:Importing untrained model
2023-07-11 08:52:14,965:INFO:Passive Aggressive Regressor Imported successfully
2023-07-11 08:52:14,965:INFO:Starting cross validation
2023-07-11 08:52:14,980:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 08:52:19,474:INFO:Calculating mean and std
2023-07-11 08:52:19,474:INFO:Creating metrics dataframe
2023-07-11 08:52:19,679:INFO:Uploading results into container
2023-07-11 08:52:19,695:INFO:Uploading model into container now
2023-07-11 08:52:19,695:INFO:_master_model_container: 9
2023-07-11 08:52:19,695:INFO:_display_container: 2
2023-07-11 08:52:19,695:INFO:PassiveAggressiveRegressor(random_state=3428)
2023-07-11 08:52:19,695:INFO:create_model() successfully completed......................................
2023-07-11 08:52:19,925:INFO:SubProcess create_model() end ==================================
2023-07-11 08:52:19,925:INFO:Creating metrics dataframe
2023-07-11 08:52:19,941:INFO:Initializing Huber Regressor
2023-07-11 08:52:19,941:INFO:Total runtime is 1.0058178186416624 minutes
2023-07-11 08:52:19,941:INFO:SubProcess create_model() called ==================================
2023-07-11 08:52:19,941:INFO:Initializing create_model()
2023-07-11 08:52:19,941:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000014F870AF790>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000014F85D67F70>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 08:52:19,941:INFO:Checking exceptions
2023-07-11 08:52:19,941:INFO:Importing libraries
2023-07-11 08:52:19,941:INFO:Copying training dataset
2023-07-11 08:52:20,036:INFO:Defining folds
2023-07-11 08:52:20,036:INFO:Declaring metric variables
2023-07-11 08:52:20,036:INFO:Importing untrained model
2023-07-11 08:52:20,036:INFO:Huber Regressor Imported successfully
2023-07-11 08:52:20,036:INFO:Starting cross validation
2023-07-11 08:52:20,051:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 08:52:25,281:INFO:Calculating mean and std
2023-07-11 08:52:25,281:INFO:Creating metrics dataframe
2023-07-11 08:52:25,511:INFO:Uploading results into container
2023-07-11 08:52:25,516:INFO:Uploading model into container now
2023-07-11 08:52:25,517:INFO:_master_model_container: 10
2023-07-11 08:52:25,517:INFO:_display_container: 2
2023-07-11 08:52:25,517:INFO:HuberRegressor()
2023-07-11 08:52:25,517:INFO:create_model() successfully completed......................................
2023-07-11 08:52:25,769:INFO:SubProcess create_model() end ==================================
2023-07-11 08:52:25,769:INFO:Creating metrics dataframe
2023-07-11 08:52:25,784:INFO:Initializing K Neighbors Regressor
2023-07-11 08:52:25,784:INFO:Total runtime is 1.1032062212626137 minutes
2023-07-11 08:52:25,784:INFO:SubProcess create_model() called ==================================
2023-07-11 08:52:25,784:INFO:Initializing create_model()
2023-07-11 08:52:25,784:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000014F870AF790>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000014F85D67F70>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 08:52:25,784:INFO:Checking exceptions
2023-07-11 08:52:25,784:INFO:Importing libraries
2023-07-11 08:52:25,784:INFO:Copying training dataset
2023-07-11 08:52:25,880:INFO:Defining folds
2023-07-11 08:52:25,880:INFO:Declaring metric variables
2023-07-11 08:52:25,880:INFO:Importing untrained model
2023-07-11 08:52:25,880:INFO:K Neighbors Regressor Imported successfully
2023-07-11 08:52:25,880:INFO:Starting cross validation
2023-07-11 08:52:25,880:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 08:52:46,055:INFO:Calculating mean and std
2023-07-11 08:52:46,055:INFO:Creating metrics dataframe
2023-07-11 08:52:46,338:INFO:Uploading results into container
2023-07-11 08:52:46,338:INFO:Uploading model into container now
2023-07-11 08:52:46,338:INFO:_master_model_container: 11
2023-07-11 08:52:46,338:INFO:_display_container: 2
2023-07-11 08:52:46,338:INFO:KNeighborsRegressor(n_jobs=-1)
2023-07-11 08:52:46,338:INFO:create_model() successfully completed......................................
2023-07-11 08:52:46,605:INFO:SubProcess create_model() end ==================================
2023-07-11 08:52:46,621:INFO:Creating metrics dataframe
2023-07-11 08:52:46,647:INFO:Initializing Decision Tree Regressor
2023-07-11 08:52:46,647:INFO:Total runtime is 1.4509094516436256 minutes
2023-07-11 08:52:46,647:INFO:SubProcess create_model() called ==================================
2023-07-11 08:52:46,647:INFO:Initializing create_model()
2023-07-11 08:52:46,647:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000014F870AF790>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000014F85D67F70>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 08:52:46,647:INFO:Checking exceptions
2023-07-11 08:52:46,647:INFO:Importing libraries
2023-07-11 08:52:46,647:INFO:Copying training dataset
2023-07-11 08:52:46,749:INFO:Defining folds
2023-07-11 08:52:46,749:INFO:Declaring metric variables
2023-07-11 08:52:46,749:INFO:Importing untrained model
2023-07-11 08:52:46,749:INFO:Decision Tree Regressor Imported successfully
2023-07-11 08:52:46,749:INFO:Starting cross validation
2023-07-11 08:52:46,749:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 08:52:51,896:INFO:Calculating mean and std
2023-07-11 08:52:51,896:INFO:Creating metrics dataframe
2023-07-11 08:52:52,154:INFO:Uploading results into container
2023-07-11 08:52:52,154:INFO:Uploading model into container now
2023-07-11 08:52:52,154:INFO:_master_model_container: 12
2023-07-11 08:52:52,154:INFO:_display_container: 2
2023-07-11 08:52:52,154:INFO:DecisionTreeRegressor(random_state=3428)
2023-07-11 08:52:52,154:INFO:create_model() successfully completed......................................
2023-07-11 08:52:52,400:INFO:SubProcess create_model() end ==================================
2023-07-11 08:52:52,400:INFO:Creating metrics dataframe
2023-07-11 08:52:52,416:INFO:Initializing Random Forest Regressor
2023-07-11 08:52:52,416:INFO:Total runtime is 1.5470661600430804 minutes
2023-07-11 08:52:52,416:INFO:SubProcess create_model() called ==================================
2023-07-11 08:52:52,416:INFO:Initializing create_model()
2023-07-11 08:52:52,416:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000014F870AF790>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000014F85D67F70>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 08:52:52,416:INFO:Checking exceptions
2023-07-11 08:52:52,416:INFO:Importing libraries
2023-07-11 08:52:52,416:INFO:Copying training dataset
2023-07-11 08:52:52,510:INFO:Defining folds
2023-07-11 08:52:52,510:INFO:Declaring metric variables
2023-07-11 08:52:52,510:INFO:Importing untrained model
2023-07-11 08:52:52,510:INFO:Random Forest Regressor Imported successfully
2023-07-11 08:52:52,510:INFO:Starting cross validation
2023-07-11 08:52:52,526:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 08:52:56,611:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.64s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-07-11 08:52:57,526:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.85s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-11 08:52:58,171:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 2.16s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-07-11 08:52:58,249:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 1.07s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-11 08:52:58,469:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 2.15s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-07-11 08:52:59,538:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.85s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-11 08:53:18,365:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 5.49s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-11 08:53:19,355:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 6.30s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-11 08:53:19,371:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 5.97s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-11 08:53:25,700:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:108: UserWarning: Persisting input arguments took 5.38s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-07-11 08:53:25,714:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:108: UserWarning: Persisting input arguments took 2.34s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-07-11 08:53:27,074:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:108: UserWarning: Persisting input arguments took 1.01s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-07-11 08:53:28,281:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:108: UserWarning: Persisting input arguments took 1.27s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-07-11 08:53:28,863:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:108: UserWarning: Persisting input arguments took 1.98s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-07-11 08:53:29,651:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:108: UserWarning: Persisting input arguments took 2.18s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-07-11 08:53:30,025:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:108: UserWarning: Persisting input arguments took 0.62s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-07-11 08:53:30,421:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 1.12s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-11 08:53:30,520:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.76s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-11 08:53:30,620:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.62s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-11 08:53:30,733:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:108: UserWarning: Persisting input arguments took 1.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-07-11 08:53:33,669:INFO:Calculating mean and std
2023-07-11 08:53:33,669:INFO:Creating metrics dataframe
2023-07-11 08:53:34,001:INFO:Uploading results into container
2023-07-11 08:53:34,001:INFO:Uploading model into container now
2023-07-11 08:53:34,001:INFO:_master_model_container: 13
2023-07-11 08:53:34,001:INFO:_display_container: 2
2023-07-11 08:53:34,001:INFO:RandomForestRegressor(n_jobs=-1, random_state=3428)
2023-07-11 08:53:34,001:INFO:create_model() successfully completed......................................
2023-07-11 08:53:34,285:INFO:SubProcess create_model() end ==================================
2023-07-11 08:53:34,285:INFO:Creating metrics dataframe
2023-07-11 08:53:34,300:INFO:Initializing Extra Trees Regressor
2023-07-11 08:53:34,300:INFO:Total runtime is 2.2451366066932676 minutes
2023-07-11 08:53:34,300:INFO:SubProcess create_model() called ==================================
2023-07-11 08:53:34,300:INFO:Initializing create_model()
2023-07-11 08:53:34,300:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000014F870AF790>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000014F85D67F70>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 08:53:34,300:INFO:Checking exceptions
2023-07-11 08:53:34,300:INFO:Importing libraries
2023-07-11 08:53:34,300:INFO:Copying training dataset
2023-07-11 08:53:34,394:INFO:Defining folds
2023-07-11 08:53:34,394:INFO:Declaring metric variables
2023-07-11 08:53:34,394:INFO:Importing untrained model
2023-07-11 08:53:34,394:INFO:Extra Trees Regressor Imported successfully
2023-07-11 08:53:34,394:INFO:Starting cross validation
2023-07-11 08:53:34,394:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 08:53:41,224:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 2.32s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-11 08:53:44,057:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 1.77s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-11 08:53:44,403:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 1.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-11 08:53:45,252:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 1.76s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-11 08:53:45,405:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:108: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-07-11 08:53:45,718:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:108: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-07-11 08:53:47,127:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:108: UserWarning: Persisting input arguments took 1.35s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-07-11 08:53:47,241:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 1.80s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-11 08:53:47,963:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-11 08:53:50,588:INFO:Calculating mean and std
2023-07-11 08:53:50,589:INFO:Creating metrics dataframe
2023-07-11 08:53:50,886:INFO:Uploading results into container
2023-07-11 08:53:50,886:INFO:Uploading model into container now
2023-07-11 08:53:50,886:INFO:_master_model_container: 14
2023-07-11 08:53:50,886:INFO:_display_container: 2
2023-07-11 08:53:50,901:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=3428)
2023-07-11 08:53:50,901:INFO:create_model() successfully completed......................................
2023-07-11 08:53:51,154:INFO:SubProcess create_model() end ==================================
2023-07-11 08:53:51,154:INFO:Creating metrics dataframe
2023-07-11 08:53:51,169:INFO:Initializing AdaBoost Regressor
2023-07-11 08:53:51,169:INFO:Total runtime is 2.526286689440409 minutes
2023-07-11 08:53:51,169:INFO:SubProcess create_model() called ==================================
2023-07-11 08:53:51,169:INFO:Initializing create_model()
2023-07-11 08:53:51,169:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000014F870AF790>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000014F85D67F70>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 08:53:51,169:INFO:Checking exceptions
2023-07-11 08:53:51,169:INFO:Importing libraries
2023-07-11 08:53:51,169:INFO:Copying training dataset
2023-07-11 08:53:51,279:INFO:Defining folds
2023-07-11 08:53:51,279:INFO:Declaring metric variables
2023-07-11 08:53:51,279:INFO:Importing untrained model
2023-07-11 08:53:51,279:INFO:AdaBoost Regressor Imported successfully
2023-07-11 08:53:51,279:INFO:Starting cross validation
2023-07-11 08:53:51,279:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 08:54:13,678:INFO:Calculating mean and std
2023-07-11 08:54:13,682:INFO:Creating metrics dataframe
2023-07-11 08:54:14,123:INFO:Uploading results into container
2023-07-11 08:54:14,123:INFO:Uploading model into container now
2023-07-11 08:54:14,123:INFO:_master_model_container: 15
2023-07-11 08:54:14,123:INFO:_display_container: 2
2023-07-11 08:54:14,123:INFO:AdaBoostRegressor(random_state=3428)
2023-07-11 08:54:14,123:INFO:create_model() successfully completed......................................
2023-07-11 08:54:14,374:INFO:SubProcess create_model() end ==================================
2023-07-11 08:54:14,374:INFO:Creating metrics dataframe
2023-07-11 08:54:14,390:INFO:Initializing Gradient Boosting Regressor
2023-07-11 08:54:14,390:INFO:Total runtime is 2.9132977843284604 minutes
2023-07-11 08:54:14,390:INFO:SubProcess create_model() called ==================================
2023-07-11 08:54:14,390:INFO:Initializing create_model()
2023-07-11 08:54:14,390:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000014F870AF790>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000014F85D67F70>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 08:54:14,390:INFO:Checking exceptions
2023-07-11 08:54:14,390:INFO:Importing libraries
2023-07-11 08:54:14,390:INFO:Copying training dataset
2023-07-11 08:54:14,488:INFO:Defining folds
2023-07-11 08:54:14,488:INFO:Declaring metric variables
2023-07-11 08:54:14,488:INFO:Importing untrained model
2023-07-11 08:54:14,496:INFO:Gradient Boosting Regressor Imported successfully
2023-07-11 08:54:14,496:INFO:Starting cross validation
2023-07-11 08:54:14,505:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 08:54:57,037:INFO:Calculating mean and std
2023-07-11 08:54:57,037:INFO:Creating metrics dataframe
2023-07-11 08:54:57,454:INFO:Uploading results into container
2023-07-11 08:54:57,454:INFO:Uploading model into container now
2023-07-11 08:54:57,457:INFO:_master_model_container: 16
2023-07-11 08:54:57,457:INFO:_display_container: 2
2023-07-11 08:54:57,457:INFO:GradientBoostingRegressor(random_state=3428)
2023-07-11 08:54:57,457:INFO:create_model() successfully completed......................................
2023-07-11 08:54:57,742:INFO:SubProcess create_model() end ==================================
2023-07-11 08:54:57,742:INFO:Creating metrics dataframe
2023-07-11 08:54:57,762:INFO:Initializing Extreme Gradient Boosting
2023-07-11 08:54:57,762:INFO:Total runtime is 3.636173637708028 minutes
2023-07-11 08:54:57,762:INFO:SubProcess create_model() called ==================================
2023-07-11 08:54:57,762:INFO:Initializing create_model()
2023-07-11 08:54:57,762:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000014F870AF790>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000014F85D67F70>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 08:54:57,766:INFO:Checking exceptions
2023-07-11 08:54:57,766:INFO:Importing libraries
2023-07-11 08:54:57,766:INFO:Copying training dataset
2023-07-11 08:54:57,882:INFO:Defining folds
2023-07-11 08:54:57,882:INFO:Declaring metric variables
2023-07-11 08:54:57,882:INFO:Importing untrained model
2023-07-11 08:54:57,887:INFO:Extreme Gradient Boosting Imported successfully
2023-07-11 08:54:57,887:INFO:Starting cross validation
2023-07-11 08:54:57,892:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 08:55:28,382:INFO:Calculating mean and std
2023-07-11 08:55:28,382:INFO:Creating metrics dataframe
2023-07-11 08:55:28,793:INFO:Uploading results into container
2023-07-11 08:55:28,793:INFO:Uploading model into container now
2023-07-11 08:55:28,793:INFO:_master_model_container: 17
2023-07-11 08:55:28,793:INFO:_display_container: 2
2023-07-11 08:55:28,809:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             n_estimators=100, n_jobs=-1, num_parallel_tree=None,
             predictor=None, random_state=3428, ...)
2023-07-11 08:55:28,809:INFO:create_model() successfully completed......................................
2023-07-11 08:55:29,086:INFO:SubProcess create_model() end ==================================
2023-07-11 08:55:29,086:INFO:Creating metrics dataframe
2023-07-11 08:55:29,112:INFO:Initializing Light Gradient Boosting Machine
2023-07-11 08:55:29,112:INFO:Total runtime is 4.158664091428121 minutes
2023-07-11 08:55:29,117:INFO:SubProcess create_model() called ==================================
2023-07-11 08:55:29,117:INFO:Initializing create_model()
2023-07-11 08:55:29,117:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000014F870AF790>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000014F85D67F70>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 08:55:29,117:INFO:Checking exceptions
2023-07-11 08:55:29,117:INFO:Importing libraries
2023-07-11 08:55:29,117:INFO:Copying training dataset
2023-07-11 08:55:29,223:INFO:Defining folds
2023-07-11 08:55:29,223:INFO:Declaring metric variables
2023-07-11 08:55:29,223:INFO:Importing untrained model
2023-07-11 08:55:29,231:INFO:Light Gradient Boosting Machine Imported successfully
2023-07-11 08:55:29,231:INFO:Starting cross validation
2023-07-11 08:55:29,232:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 08:55:40,053:INFO:Calculating mean and std
2023-07-11 08:55:40,053:INFO:Creating metrics dataframe
2023-07-11 08:55:40,515:INFO:Uploading results into container
2023-07-11 08:55:40,516:INFO:Uploading model into container now
2023-07-11 08:55:40,516:INFO:_master_model_container: 18
2023-07-11 08:55:40,516:INFO:_display_container: 2
2023-07-11 08:55:40,518:INFO:LGBMRegressor(random_state=3428)
2023-07-11 08:55:40,518:INFO:create_model() successfully completed......................................
2023-07-11 08:55:40,777:INFO:SubProcess create_model() end ==================================
2023-07-11 08:55:40,777:INFO:Creating metrics dataframe
2023-07-11 08:55:40,803:INFO:Initializing Dummy Regressor
2023-07-11 08:55:40,803:INFO:Total runtime is 4.353522662321726 minutes
2023-07-11 08:55:40,803:INFO:SubProcess create_model() called ==================================
2023-07-11 08:55:40,803:INFO:Initializing create_model()
2023-07-11 08:55:40,803:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000014F870AF790>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000014F85D67F70>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 08:55:40,803:INFO:Checking exceptions
2023-07-11 08:55:40,803:INFO:Importing libraries
2023-07-11 08:55:40,803:INFO:Copying training dataset
2023-07-11 08:55:40,901:INFO:Defining folds
2023-07-11 08:55:40,901:INFO:Declaring metric variables
2023-07-11 08:55:40,901:INFO:Importing untrained model
2023-07-11 08:55:40,901:INFO:Dummy Regressor Imported successfully
2023-07-11 08:55:40,901:INFO:Starting cross validation
2023-07-11 08:55:40,918:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 08:55:46,088:INFO:Calculating mean and std
2023-07-11 08:55:46,088:INFO:Creating metrics dataframe
2023-07-11 08:55:46,518:INFO:Uploading results into container
2023-07-11 08:55:46,527:INFO:Uploading model into container now
2023-07-11 08:55:46,527:INFO:_master_model_container: 19
2023-07-11 08:55:46,528:INFO:_display_container: 2
2023-07-11 08:55:46,528:INFO:DummyRegressor()
2023-07-11 08:55:46,528:INFO:create_model() successfully completed......................................
2023-07-11 08:55:46,798:INFO:SubProcess create_model() end ==================================
2023-07-11 08:55:46,798:INFO:Creating metrics dataframe
2023-07-11 08:55:46,818:INFO:Initializing create_model()
2023-07-11 08:55:46,818:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000014F870AF790>, estimator=LGBMRegressor(random_state=3428), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-07-11 08:55:46,818:INFO:Checking exceptions
2023-07-11 08:55:46,823:INFO:Importing libraries
2023-07-11 08:55:46,823:INFO:Copying training dataset
2023-07-11 08:55:46,938:INFO:Defining folds
2023-07-11 08:55:46,938:INFO:Declaring metric variables
2023-07-11 08:55:46,938:INFO:Importing untrained model
2023-07-11 08:55:46,938:INFO:Declaring custom model
2023-07-11 08:55:46,938:INFO:Light Gradient Boosting Machine Imported successfully
2023-07-11 08:55:46,943:INFO:Cross validation set to False
2023-07-11 08:55:46,943:INFO:Fitting Model
2023-07-11 08:55:52,598:INFO:LGBMRegressor(random_state=3428)
2023-07-11 08:55:52,598:INFO:create_model() successfully completed......................................
2023-07-11 08:55:52,943:INFO:_master_model_container: 19
2023-07-11 08:55:52,949:INFO:_display_container: 2
2023-07-11 08:55:52,949:INFO:LGBMRegressor(random_state=3428)
2023-07-11 08:55:52,949:INFO:compare_models() successfully completed......................................
2023-07-11 08:57:31,129:INFO:PyCaret RegressionExperiment
2023-07-11 08:57:31,145:INFO:Logging name: reg-default-name
2023-07-11 08:57:31,145:INFO:ML Usecase: MLUsecase.REGRESSION
2023-07-11 08:57:31,145:INFO:version 3.0.2
2023-07-11 08:57:31,145:INFO:Initializing setup()
2023-07-11 08:57:31,145:INFO:self.USI: 8734
2023-07-11 08:57:31,145:INFO:self._variable_keys: {'n_jobs_param', 'gpu_n_jobs_param', 'pipeline', 'target_param', 'seed', 'logging_param', 'idx', 'transform_target_param', '_available_plots', 'X', 'USI', 'exp_name_log', 'log_plots_param', 'y', 'y_test', 'X_train', 'y_train', 'memory', 'data', 'X_test', 'fold_shuffle_param', 'gpu_param', 'exp_id', 'fold_generator', 'html_param', 'fold_groups_param', '_ml_usecase'}
2023-07-11 08:57:31,145:INFO:Checking environment
2023-07-11 08:57:31,145:INFO:python_version: 3.9.13
2023-07-11 08:57:31,145:INFO:python_build: ('main', 'Oct 13 2022 21:23:06')
2023-07-11 08:57:31,145:INFO:machine: AMD64
2023-07-11 08:57:31,145:INFO:platform: Windows-10-10.0.22621-SP0
2023-07-11 08:57:31,145:INFO:Memory: svmem(total=16893358080, available=2337128448, percent=86.2, used=14556229632, free=2337128448)
2023-07-11 08:57:31,145:INFO:Physical Core: 8
2023-07-11 08:57:31,145:INFO:Logical Core: 16
2023-07-11 08:57:31,145:INFO:Checking libraries
2023-07-11 08:57:31,145:INFO:System:
2023-07-11 08:57:31,145:INFO:    python: 3.9.13 (main, Oct 13 2022, 21:23:06) [MSC v.1916 64 bit (AMD64)]
2023-07-11 08:57:31,145:INFO:executable: C:\Users\didit\anaconda3\python.exe
2023-07-11 08:57:31,145:INFO:   machine: Windows-10-10.0.22621-SP0
2023-07-11 08:57:31,145:INFO:PyCaret required dependencies:
2023-07-11 08:57:31,145:INFO:                 pip: 23.0.1
2023-07-11 08:57:31,145:INFO:          setuptools: 67.8.0
2023-07-11 08:57:31,145:INFO:             pycaret: 3.0.2
2023-07-11 08:57:31,145:INFO:             IPython: 8.12.0
2023-07-11 08:57:31,145:INFO:          ipywidgets: 8.0.4
2023-07-11 08:57:31,145:INFO:                tqdm: 4.65.0
2023-07-11 08:57:31,145:INFO:               numpy: 1.21.5
2023-07-11 08:57:31,145:INFO:              pandas: 1.5.3
2023-07-11 08:57:31,145:INFO:              jinja2: 3.1.2
2023-07-11 08:57:31,145:INFO:               scipy: 1.10.1
2023-07-11 08:57:31,145:INFO:              joblib: 1.2.0
2023-07-11 08:57:31,145:INFO:             sklearn: 1.2.2
2023-07-11 08:57:31,145:INFO:                pyod: 1.0.9
2023-07-11 08:57:31,145:INFO:            imblearn: 0.10.1
2023-07-11 08:57:31,145:INFO:   category_encoders: 2.6.1
2023-07-11 08:57:31,145:INFO:            lightgbm: 3.3.5
2023-07-11 08:57:31,145:INFO:               numba: 0.57.0
2023-07-11 08:57:31,145:INFO:            requests: 2.29.0
2023-07-11 08:57:31,145:INFO:          matplotlib: 3.7.1
2023-07-11 08:57:31,145:INFO:          scikitplot: 0.3.7
2023-07-11 08:57:31,145:INFO:         yellowbrick: 1.5
2023-07-11 08:57:31,145:INFO:              plotly: 5.9.0
2023-07-11 08:57:31,145:INFO:             kaleido: 0.2.1
2023-07-11 08:57:31,145:INFO:         statsmodels: 0.13.5
2023-07-11 08:57:31,145:INFO:              sktime: 0.17.0
2023-07-11 08:57:31,145:INFO:               tbats: 1.1.3
2023-07-11 08:57:31,145:INFO:            pmdarima: 2.0.3
2023-07-11 08:57:31,145:INFO:              psutil: 5.9.0
2023-07-11 08:57:31,145:INFO:PyCaret optional dependencies:
2023-07-11 08:57:31,145:INFO:                shap: 0.41.0
2023-07-11 08:57:31,145:INFO:           interpret: Not installed
2023-07-11 08:57:31,145:INFO:                umap: Not installed
2023-07-11 08:57:31,145:INFO:    pandas_profiling: 4.3.1
2023-07-11 08:57:31,145:INFO:  explainerdashboard: Not installed
2023-07-11 08:57:31,145:INFO:             autoviz: Not installed
2023-07-11 08:57:31,145:INFO:           fairlearn: Not installed
2023-07-11 08:57:31,145:INFO:             xgboost: 1.7.6
2023-07-11 08:57:31,145:INFO:            catboost: Not installed
2023-07-11 08:57:31,145:INFO:              kmodes: Not installed
2023-07-11 08:57:31,145:INFO:             mlxtend: Not installed
2023-07-11 08:57:31,145:INFO:       statsforecast: Not installed
2023-07-11 08:57:31,145:INFO:        tune_sklearn: Not installed
2023-07-11 08:57:31,145:INFO:                 ray: Not installed
2023-07-11 08:57:31,145:INFO:            hyperopt: Not installed
2023-07-11 08:57:31,145:INFO:              optuna: Not installed
2023-07-11 08:57:31,145:INFO:               skopt: Not installed
2023-07-11 08:57:31,145:INFO:              mlflow: 2.4.2
2023-07-11 08:57:31,145:INFO:              gradio: Not installed
2023-07-11 08:57:31,145:INFO:             fastapi: 0.95.2
2023-07-11 08:57:31,145:INFO:             uvicorn: 0.22.0
2023-07-11 08:57:31,145:INFO:              m2cgen: Not installed
2023-07-11 08:57:31,145:INFO:           evidently: Not installed
2023-07-11 08:57:31,145:INFO:               fugue: Not installed
2023-07-11 08:57:31,145:INFO:           streamlit: 1.23.1
2023-07-11 08:57:31,145:INFO:             prophet: Not installed
2023-07-11 08:57:31,145:INFO:None
2023-07-11 08:57:31,145:INFO:Set up data.
2023-07-11 08:57:31,461:INFO:Set up train/test split.
2023-07-11 08:57:31,563:INFO:Set up index.
2023-07-11 08:57:31,569:INFO:Set up folding strategy.
2023-07-11 08:57:31,569:INFO:Assigning column types.
2023-07-11 08:57:31,632:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-07-11 08:57:31,632:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-07-11 08:57:31,648:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-07-11 08:57:31,669:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 08:57:31,884:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 08:57:32,041:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 08:57:32,041:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 08:57:32,041:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 08:57:32,041:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-07-11 08:57:32,056:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-07-11 08:57:32,064:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 08:57:32,277:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 08:57:32,372:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 08:57:32,388:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 08:57:32,388:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 08:57:32,388:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-07-11 08:57:32,403:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-07-11 08:57:32,403:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 08:57:32,624:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 08:57:32,749:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 08:57:32,749:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 08:57:32,765:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 08:57:32,781:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-07-11 08:57:32,797:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 08:57:33,017:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 08:57:33,143:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 08:57:33,143:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 08:57:33,158:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 08:57:33,158:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-07-11 08:57:33,174:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 08:57:33,380:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 08:57:33,474:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 08:57:33,490:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 08:57:33,490:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 08:57:33,521:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 08:57:33,740:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 08:57:33,852:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 08:57:33,852:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 08:57:33,866:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 08:57:33,866:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-07-11 08:57:34,101:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 08:57:34,211:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 08:57:34,211:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 08:57:34,227:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 08:57:34,431:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 08:57:34,557:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 08:57:34,557:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 08:57:34,557:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 08:57:34,557:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-07-11 08:57:34,855:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 08:57:34,998:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 08:57:34,998:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 08:57:35,249:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 08:57:35,343:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 08:57:35,343:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 08:57:35,343:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-07-11 08:57:35,689:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 08:57:35,689:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 08:57:36,033:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 08:57:36,033:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 08:57:36,033:INFO:Preparing preprocessing pipeline...
2023-07-11 08:57:36,033:INFO:Set up simple imputation.
2023-07-11 08:57:36,096:INFO:Set up encoding of ordinal features.
2023-07-11 08:57:36,112:INFO:Set up encoding of categorical features.
2023-07-11 08:57:36,127:INFO:Set up column name cleaning.
2023-07-11 08:57:40,896:INFO:Finished creating preprocessing pipeline.
2023-07-11 08:57:40,978:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\didit\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['Postal Code', 'Model Year',
                                             'Electric Range',
                                             'Legislative District',
                                             'DOL Vehicle ID',
                                             '2020 Census Tract', 'Longitude',
                                             'latitude'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['County', 'City'...
                                    transformer=OneHotEncoder(cols=['State',
                                                                    'CAFV'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('rest_encoding',
                 TransformerWrapper(include=['County', 'City', 'Make', 'Model',
                                             'Electric Utility'],
                                    transformer=TargetEncoder(cols=['County',
                                                                    'City',
                                                                    'Make',
                                                                    'Model',
                                                                    'Electric '
                                                                    'Utility'],
                                                              handle_missing='return_nan'))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames()))])
2023-07-11 08:57:40,978:INFO:Creating final display dataframe.
2023-07-11 08:57:44,711:INFO:Setup _display_container:                     Description             Value
0                    Session id              5992
1                        Target         Base MSRP
2                   Target type        Regression
3           Original data shape      (134474, 17)
4        Transformed data shape      (134474, 19)
5   Transformed train set shape       (94131, 19)
6    Transformed test set shape       (40343, 19)
7              Ordinal features                 1
8              Numeric features                 8
9          Categorical features                 8
10                   Preprocess              True
11              Imputation type            simple
12           Numeric imputation              mean
13       Categorical imputation              mode
14     Maximum one-hot encoding                25
15              Encoding method              None
16               Fold Generator             KFold
17                  Fold Number                10
18                     CPU Jobs                -1
19                      Use GPU             False
20               Log Experiment             False
21              Experiment Name  reg-default-name
22                          USI              8734
2023-07-11 08:57:45,085:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 08:57:45,089:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 08:57:45,450:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 08:57:45,450:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 08:57:45,466:INFO:setup() successfully completed in 14.6s...............
2023-07-11 08:57:45,513:INFO:Initializing compare_models()
2023-07-11 08:57:45,513:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000014F84749700>, include=None, fold=None, round=4, cross_validation=True, sort=R2, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x0000014F84749700>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2023-07-11 08:57:45,513:INFO:Checking exceptions
2023-07-11 08:57:45,546:INFO:Preparing display monitor
2023-07-11 08:57:45,554:INFO:Initializing Linear Regression
2023-07-11 08:57:45,554:INFO:Total runtime is 0.0 minutes
2023-07-11 08:57:45,554:INFO:SubProcess create_model() called ==================================
2023-07-11 08:57:45,554:INFO:Initializing create_model()
2023-07-11 08:57:45,554:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000014F84749700>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000014F85D641F0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 08:57:45,554:INFO:Checking exceptions
2023-07-11 08:57:45,554:INFO:Importing libraries
2023-07-11 08:57:45,554:INFO:Copying training dataset
2023-07-11 08:57:45,633:INFO:Defining folds
2023-07-11 08:57:45,633:INFO:Declaring metric variables
2023-07-11 08:57:45,633:INFO:Importing untrained model
2023-07-11 08:57:45,633:INFO:Linear Regression Imported successfully
2023-07-11 08:57:45,633:INFO:Starting cross validation
2023-07-11 08:57:45,633:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 08:57:55,660:INFO:Calculating mean and std
2023-07-11 08:57:55,660:INFO:Creating metrics dataframe
2023-07-11 08:57:56,180:INFO:Uploading results into container
2023-07-11 08:57:56,180:INFO:Uploading model into container now
2023-07-11 08:57:56,180:INFO:_master_model_container: 1
2023-07-11 08:57:56,180:INFO:_display_container: 2
2023-07-11 08:57:56,180:INFO:LinearRegression(n_jobs=-1)
2023-07-11 08:57:56,180:INFO:create_model() successfully completed......................................
2023-07-11 08:57:56,447:INFO:SubProcess create_model() end ==================================
2023-07-11 08:57:56,447:INFO:Creating metrics dataframe
2023-07-11 08:57:56,447:INFO:Initializing Lasso Regression
2023-07-11 08:57:56,447:INFO:Total runtime is 0.18155688842137654 minutes
2023-07-11 08:57:56,447:INFO:SubProcess create_model() called ==================================
2023-07-11 08:57:56,447:INFO:Initializing create_model()
2023-07-11 08:57:56,447:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000014F84749700>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000014F85D641F0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 08:57:56,447:INFO:Checking exceptions
2023-07-11 08:57:56,447:INFO:Importing libraries
2023-07-11 08:57:56,447:INFO:Copying training dataset
2023-07-11 08:57:56,558:INFO:Defining folds
2023-07-11 08:57:56,558:INFO:Declaring metric variables
2023-07-11 08:57:56,558:INFO:Importing untrained model
2023-07-11 08:57:56,558:INFO:Lasso Regression Imported successfully
2023-07-11 08:57:56,558:INFO:Starting cross validation
2023-07-11 08:57:56,558:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 08:58:07,931:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.391e+12, tolerance: 7.424e+08
  model = cd_fast.enet_coordinate_descent(

2023-07-11 08:58:08,191:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.647e+12, tolerance: 8.173e+08
  model = cd_fast.enet_coordinate_descent(

2023-07-11 08:58:12,190:INFO:Calculating mean and std
2023-07-11 08:58:12,190:INFO:Creating metrics dataframe
2023-07-11 08:58:12,756:INFO:Uploading results into container
2023-07-11 08:58:12,756:INFO:Uploading model into container now
2023-07-11 08:58:12,756:INFO:_master_model_container: 2
2023-07-11 08:58:12,756:INFO:_display_container: 2
2023-07-11 08:58:12,756:INFO:Lasso(random_state=5992)
2023-07-11 08:58:12,756:INFO:create_model() successfully completed......................................
2023-07-11 08:58:13,001:INFO:SubProcess create_model() end ==================================
2023-07-11 08:58:13,001:INFO:Creating metrics dataframe
2023-07-11 08:58:13,017:INFO:Initializing Ridge Regression
2023-07-11 08:58:13,017:INFO:Total runtime is 0.4577200969060262 minutes
2023-07-11 08:58:13,017:INFO:SubProcess create_model() called ==================================
2023-07-11 08:58:13,017:INFO:Initializing create_model()
2023-07-11 08:58:13,017:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000014F84749700>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000014F85D641F0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 08:58:13,017:INFO:Checking exceptions
2023-07-11 08:58:13,017:INFO:Importing libraries
2023-07-11 08:58:13,017:INFO:Copying training dataset
2023-07-11 08:58:13,096:INFO:Defining folds
2023-07-11 08:58:13,096:INFO:Declaring metric variables
2023-07-11 08:58:13,096:INFO:Importing untrained model
2023-07-11 08:58:13,096:INFO:Ridge Regression Imported successfully
2023-07-11 08:58:13,096:INFO:Starting cross validation
2023-07-11 08:58:13,111:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 08:58:14,890:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.59889e-21): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-11 08:58:14,905:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.59825e-21): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-11 08:58:14,968:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.60308e-21): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-11 08:58:15,031:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.59354e-21): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-11 08:58:15,047:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.59899e-21): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-11 08:58:15,141:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.59398e-21): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-11 08:58:15,204:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.59607e-21): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-11 08:58:15,243:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.59761e-21): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-11 08:58:15,291:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.60419e-21): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-11 08:58:15,385:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.60099e-21): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-11 08:58:19,252:INFO:Calculating mean and std
2023-07-11 08:58:19,252:INFO:Creating metrics dataframe
2023-07-11 08:58:19,818:INFO:Uploading results into container
2023-07-11 08:58:19,818:INFO:Uploading model into container now
2023-07-11 08:58:19,818:INFO:_master_model_container: 3
2023-07-11 08:58:19,818:INFO:_display_container: 2
2023-07-11 08:58:19,818:INFO:Ridge(random_state=5992)
2023-07-11 08:58:19,818:INFO:create_model() successfully completed......................................
2023-07-11 08:58:20,070:INFO:SubProcess create_model() end ==================================
2023-07-11 08:58:20,070:INFO:Creating metrics dataframe
2023-07-11 08:58:20,086:INFO:Initializing Elastic Net
2023-07-11 08:58:20,086:INFO:Total runtime is 0.575538444519043 minutes
2023-07-11 08:58:20,086:INFO:SubProcess create_model() called ==================================
2023-07-11 08:58:20,086:INFO:Initializing create_model()
2023-07-11 08:58:20,086:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000014F84749700>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000014F85D641F0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 08:58:20,086:INFO:Checking exceptions
2023-07-11 08:58:20,086:INFO:Importing libraries
2023-07-11 08:58:20,086:INFO:Copying training dataset
2023-07-11 08:58:20,180:INFO:Defining folds
2023-07-11 08:58:20,180:INFO:Declaring metric variables
2023-07-11 08:58:20,180:INFO:Importing untrained model
2023-07-11 08:58:20,180:INFO:Elastic Net Imported successfully
2023-07-11 08:58:20,180:INFO:Starting cross validation
2023-07-11 08:58:20,180:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 08:58:26,669:INFO:Calculating mean and std
2023-07-11 08:58:26,669:INFO:Creating metrics dataframe
2023-07-11 08:58:27,258:INFO:Uploading results into container
2023-07-11 08:58:27,258:INFO:Uploading model into container now
2023-07-11 08:58:27,258:INFO:_master_model_container: 4
2023-07-11 08:58:27,258:INFO:_display_container: 2
2023-07-11 08:58:27,258:INFO:ElasticNet(random_state=5992)
2023-07-11 08:58:27,258:INFO:create_model() successfully completed......................................
2023-07-11 08:58:27,509:INFO:SubProcess create_model() end ==================================
2023-07-11 08:58:27,509:INFO:Creating metrics dataframe
2023-07-11 08:58:27,524:INFO:Initializing Least Angle Regression
2023-07-11 08:58:27,524:INFO:Total runtime is 0.699511988957723 minutes
2023-07-11 08:58:27,524:INFO:SubProcess create_model() called ==================================
2023-07-11 08:58:27,524:INFO:Initializing create_model()
2023-07-11 08:58:27,524:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000014F84749700>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000014F85D641F0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 08:58:27,524:INFO:Checking exceptions
2023-07-11 08:58:27,524:INFO:Importing libraries
2023-07-11 08:58:27,524:INFO:Copying training dataset
2023-07-11 08:58:27,618:INFO:Defining folds
2023-07-11 08:58:27,618:INFO:Declaring metric variables
2023-07-11 08:58:27,618:INFO:Importing untrained model
2023-07-11 08:58:27,618:INFO:Least Angle Regression Imported successfully
2023-07-11 08:58:27,618:INFO:Starting cross validation
2023-07-11 08:58:27,618:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 08:58:33,802:INFO:Calculating mean and std
2023-07-11 08:58:33,802:INFO:Creating metrics dataframe
2023-07-11 08:58:34,430:INFO:Uploading results into container
2023-07-11 08:58:34,430:INFO:Uploading model into container now
2023-07-11 08:58:34,430:INFO:_master_model_container: 5
2023-07-11 08:58:34,430:INFO:_display_container: 2
2023-07-11 08:58:34,430:INFO:Lars(random_state=5992)
2023-07-11 08:58:34,430:INFO:create_model() successfully completed......................................
2023-07-11 08:58:34,697:INFO:SubProcess create_model() end ==================================
2023-07-11 08:58:34,697:INFO:Creating metrics dataframe
2023-07-11 08:58:34,713:INFO:Initializing Lasso Least Angle Regression
2023-07-11 08:58:34,713:INFO:Total runtime is 0.8193184018135071 minutes
2023-07-11 08:58:34,713:INFO:SubProcess create_model() called ==================================
2023-07-11 08:58:34,713:INFO:Initializing create_model()
2023-07-11 08:58:34,713:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000014F84749700>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000014F85D641F0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 08:58:34,713:INFO:Checking exceptions
2023-07-11 08:58:34,713:INFO:Importing libraries
2023-07-11 08:58:34,713:INFO:Copying training dataset
2023-07-11 08:58:34,791:INFO:Defining folds
2023-07-11 08:58:34,791:INFO:Declaring metric variables
2023-07-11 08:58:34,791:INFO:Importing untrained model
2023-07-11 08:58:34,807:INFO:Lasso Least Angle Regression Imported successfully
2023-07-11 08:58:34,807:INFO:Starting cross validation
2023-07-11 08:58:34,807:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 08:58:40,851:INFO:Calculating mean and std
2023-07-11 08:58:40,851:INFO:Creating metrics dataframe
2023-07-11 08:58:41,492:INFO:Uploading results into container
2023-07-11 08:58:41,498:INFO:Uploading model into container now
2023-07-11 08:58:41,498:INFO:_master_model_container: 6
2023-07-11 08:58:41,498:INFO:_display_container: 2
2023-07-11 08:58:41,498:INFO:LassoLars(random_state=5992)
2023-07-11 08:58:41,498:INFO:create_model() successfully completed......................................
2023-07-11 08:58:41,750:INFO:SubProcess create_model() end ==================================
2023-07-11 08:58:41,750:INFO:Creating metrics dataframe
2023-07-11 08:58:41,767:INFO:Initializing Orthogonal Matching Pursuit
2023-07-11 08:58:41,767:INFO:Total runtime is 0.936882774035136 minutes
2023-07-11 08:58:41,767:INFO:SubProcess create_model() called ==================================
2023-07-11 08:58:41,767:INFO:Initializing create_model()
2023-07-11 08:58:41,767:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000014F84749700>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000014F85D641F0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 08:58:41,767:INFO:Checking exceptions
2023-07-11 08:58:41,767:INFO:Importing libraries
2023-07-11 08:58:41,767:INFO:Copying training dataset
2023-07-11 08:58:41,845:INFO:Defining folds
2023-07-11 08:58:41,845:INFO:Declaring metric variables
2023-07-11 08:58:41,845:INFO:Importing untrained model
2023-07-11 08:58:41,845:INFO:Orthogonal Matching Pursuit Imported successfully
2023-07-11 08:58:41,845:INFO:Starting cross validation
2023-07-11 08:58:41,845:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 08:58:48,170:INFO:Calculating mean and std
2023-07-11 08:58:48,170:INFO:Creating metrics dataframe
2023-07-11 08:58:48,862:INFO:Uploading results into container
2023-07-11 08:58:48,862:INFO:Uploading model into container now
2023-07-11 08:58:48,862:INFO:_master_model_container: 7
2023-07-11 08:58:48,862:INFO:_display_container: 2
2023-07-11 08:58:48,862:INFO:OrthogonalMatchingPursuit()
2023-07-11 08:58:48,862:INFO:create_model() successfully completed......................................
2023-07-11 08:58:49,114:INFO:SubProcess create_model() end ==================================
2023-07-11 08:58:49,114:INFO:Creating metrics dataframe
2023-07-11 08:58:49,130:INFO:Initializing Bayesian Ridge
2023-07-11 08:58:49,130:INFO:Total runtime is 1.0596044262250266 minutes
2023-07-11 08:58:49,130:INFO:SubProcess create_model() called ==================================
2023-07-11 08:58:49,130:INFO:Initializing create_model()
2023-07-11 08:58:49,130:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000014F84749700>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000014F85D641F0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 08:58:49,130:INFO:Checking exceptions
2023-07-11 08:58:49,130:INFO:Importing libraries
2023-07-11 08:58:49,130:INFO:Copying training dataset
2023-07-11 08:58:49,240:INFO:Defining folds
2023-07-11 08:58:49,240:INFO:Declaring metric variables
2023-07-11 08:58:49,240:INFO:Importing untrained model
2023-07-11 08:58:49,240:INFO:Bayesian Ridge Imported successfully
2023-07-11 08:58:49,240:INFO:Starting cross validation
2023-07-11 08:58:49,240:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 08:58:56,103:INFO:Calculating mean and std
2023-07-11 08:58:56,103:INFO:Creating metrics dataframe
2023-07-11 08:58:56,764:INFO:Uploading results into container
2023-07-11 08:58:56,764:INFO:Uploading model into container now
2023-07-11 08:58:56,764:INFO:_master_model_container: 8
2023-07-11 08:58:56,764:INFO:_display_container: 2
2023-07-11 08:58:56,764:INFO:BayesianRidge()
2023-07-11 08:58:56,764:INFO:create_model() successfully completed......................................
2023-07-11 08:58:57,002:INFO:SubProcess create_model() end ==================================
2023-07-11 08:58:57,002:INFO:Creating metrics dataframe
2023-07-11 08:58:57,013:INFO:Initializing Passive Aggressive Regressor
2023-07-11 08:58:57,013:INFO:Total runtime is 1.1909937858581545 minutes
2023-07-11 08:58:57,013:INFO:SubProcess create_model() called ==================================
2023-07-11 08:58:57,017:INFO:Initializing create_model()
2023-07-11 08:58:57,017:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000014F84749700>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000014F85D641F0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 08:58:57,017:INFO:Checking exceptions
2023-07-11 08:58:57,017:INFO:Importing libraries
2023-07-11 08:58:57,017:INFO:Copying training dataset
2023-07-11 08:58:57,113:INFO:Defining folds
2023-07-11 08:58:57,113:INFO:Declaring metric variables
2023-07-11 08:58:57,113:INFO:Importing untrained model
2023-07-11 08:58:57,113:INFO:Passive Aggressive Regressor Imported successfully
2023-07-11 08:58:57,113:INFO:Starting cross validation
2023-07-11 08:58:57,113:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 08:59:03,976:INFO:Calculating mean and std
2023-07-11 08:59:03,976:INFO:Creating metrics dataframe
2023-07-11 08:59:04,637:INFO:Uploading results into container
2023-07-11 08:59:04,637:INFO:Uploading model into container now
2023-07-11 08:59:04,637:INFO:_master_model_container: 9
2023-07-11 08:59:04,637:INFO:_display_container: 2
2023-07-11 08:59:04,637:INFO:PassiveAggressiveRegressor(random_state=5992)
2023-07-11 08:59:04,637:INFO:create_model() successfully completed......................................
2023-07-11 08:59:04,886:INFO:SubProcess create_model() end ==================================
2023-07-11 08:59:04,886:INFO:Creating metrics dataframe
2023-07-11 08:59:04,889:INFO:Initializing Huber Regressor
2023-07-11 08:59:04,889:INFO:Total runtime is 1.3222592035929364 minutes
2023-07-11 08:59:04,889:INFO:SubProcess create_model() called ==================================
2023-07-11 08:59:04,889:INFO:Initializing create_model()
2023-07-11 08:59:04,889:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000014F84749700>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000014F85D641F0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 08:59:04,889:INFO:Checking exceptions
2023-07-11 08:59:04,889:INFO:Importing libraries
2023-07-11 08:59:04,889:INFO:Copying training dataset
2023-07-11 08:59:04,968:INFO:Defining folds
2023-07-11 08:59:04,968:INFO:Declaring metric variables
2023-07-11 08:59:04,968:INFO:Importing untrained model
2023-07-11 08:59:04,968:INFO:Huber Regressor Imported successfully
2023-07-11 08:59:04,968:INFO:Starting cross validation
2023-07-11 08:59:04,968:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 08:59:12,673:INFO:Calculating mean and std
2023-07-11 08:59:12,673:INFO:Creating metrics dataframe
2023-07-11 08:59:13,303:INFO:Uploading results into container
2023-07-11 08:59:13,303:INFO:Uploading model into container now
2023-07-11 08:59:13,303:INFO:_master_model_container: 10
2023-07-11 08:59:13,303:INFO:_display_container: 2
2023-07-11 08:59:13,303:INFO:HuberRegressor()
2023-07-11 08:59:13,303:INFO:create_model() successfully completed......................................
2023-07-11 08:59:13,543:INFO:SubProcess create_model() end ==================================
2023-07-11 08:59:13,543:INFO:Creating metrics dataframe
2023-07-11 08:59:13,554:INFO:Initializing K Neighbors Regressor
2023-07-11 08:59:13,554:INFO:Total runtime is 1.4666656692822775 minutes
2023-07-11 08:59:13,554:INFO:SubProcess create_model() called ==================================
2023-07-11 08:59:13,554:INFO:Initializing create_model()
2023-07-11 08:59:13,554:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000014F84749700>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000014F85D641F0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 08:59:13,554:INFO:Checking exceptions
2023-07-11 08:59:13,554:INFO:Importing libraries
2023-07-11 08:59:13,554:INFO:Copying training dataset
2023-07-11 08:59:13,648:INFO:Defining folds
2023-07-11 08:59:13,648:INFO:Declaring metric variables
2023-07-11 08:59:13,648:INFO:Importing untrained model
2023-07-11 08:59:13,648:INFO:K Neighbors Regressor Imported successfully
2023-07-11 08:59:13,648:INFO:Starting cross validation
2023-07-11 08:59:13,648:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 08:59:36,298:INFO:Calculating mean and std
2023-07-11 08:59:36,298:INFO:Creating metrics dataframe
2023-07-11 08:59:36,990:INFO:Uploading results into container
2023-07-11 08:59:36,990:INFO:Uploading model into container now
2023-07-11 08:59:36,990:INFO:_master_model_container: 11
2023-07-11 08:59:36,990:INFO:_display_container: 2
2023-07-11 08:59:36,990:INFO:KNeighborsRegressor(n_jobs=-1)
2023-07-11 08:59:36,990:INFO:create_model() successfully completed......................................
2023-07-11 08:59:37,257:INFO:SubProcess create_model() end ==================================
2023-07-11 08:59:37,257:INFO:Creating metrics dataframe
2023-07-11 08:59:37,273:INFO:Initializing Decision Tree Regressor
2023-07-11 08:59:37,273:INFO:Total runtime is 1.8619917074839274 minutes
2023-07-11 08:59:37,273:INFO:SubProcess create_model() called ==================================
2023-07-11 08:59:37,273:INFO:Initializing create_model()
2023-07-11 08:59:37,273:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000014F84749700>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000014F85D641F0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 08:59:37,273:INFO:Checking exceptions
2023-07-11 08:59:37,273:INFO:Importing libraries
2023-07-11 08:59:37,273:INFO:Copying training dataset
2023-07-11 08:59:37,351:INFO:Defining folds
2023-07-11 08:59:37,351:INFO:Declaring metric variables
2023-07-11 08:59:37,351:INFO:Importing untrained model
2023-07-11 08:59:37,351:INFO:Decision Tree Regressor Imported successfully
2023-07-11 08:59:37,351:INFO:Starting cross validation
2023-07-11 08:59:37,367:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 08:59:44,270:INFO:Calculating mean and std
2023-07-11 08:59:44,270:INFO:Creating metrics dataframe
2023-07-11 08:59:44,970:INFO:Uploading results into container
2023-07-11 08:59:44,970:INFO:Uploading model into container now
2023-07-11 08:59:44,970:INFO:_master_model_container: 12
2023-07-11 08:59:44,970:INFO:_display_container: 2
2023-07-11 08:59:44,970:INFO:DecisionTreeRegressor(random_state=5992)
2023-07-11 08:59:44,970:INFO:create_model() successfully completed......................................
2023-07-11 08:59:45,237:INFO:SubProcess create_model() end ==================================
2023-07-11 08:59:45,237:INFO:Creating metrics dataframe
2023-07-11 08:59:45,253:INFO:Initializing Random Forest Regressor
2023-07-11 08:59:45,253:INFO:Total runtime is 1.9949887156486512 minutes
2023-07-11 08:59:45,254:INFO:SubProcess create_model() called ==================================
2023-07-11 08:59:45,254:INFO:Initializing create_model()
2023-07-11 08:59:45,254:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000014F84749700>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000014F85D641F0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 08:59:45,254:INFO:Checking exceptions
2023-07-11 08:59:45,254:INFO:Importing libraries
2023-07-11 08:59:45,254:INFO:Copying training dataset
2023-07-11 08:59:45,340:INFO:Defining folds
2023-07-11 08:59:45,340:INFO:Declaring metric variables
2023-07-11 08:59:45,340:INFO:Importing untrained model
2023-07-11 08:59:45,340:INFO:Random Forest Regressor Imported successfully
2023-07-11 08:59:45,340:INFO:Starting cross validation
2023-07-11 08:59:45,340:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 08:59:52,369:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 1.02s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-11 08:59:55,525:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 1.15s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-11 08:59:57,173:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:108: UserWarning: Persisting input arguments took 0.86s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-07-11 08:59:58,429:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:108: UserWarning: Persisting input arguments took 0.57s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-07-11 08:59:59,637:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 1.13s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-11 09:00:00,227:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.81s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-11 09:00:00,642:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 6.25s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-11 09:00:01,066:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.88s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-11 09:00:01,552:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.88s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-11 09:00:00,956:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 6.37s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-11 09:00:02,525:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 3.08s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-11 09:00:02,871:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.50s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-11 09:00:02,856:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.57s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-11 09:00:03,171:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.23s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-11 09:00:03,202:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.49s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-11 09:00:03,218:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.88s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-11 09:00:03,265:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.93s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-11 09:00:07,445:INFO:Calculating mean and std
2023-07-11 09:00:07,445:INFO:Creating metrics dataframe
2023-07-11 09:00:08,184:INFO:Uploading results into container
2023-07-11 09:00:08,184:INFO:Uploading model into container now
2023-07-11 09:00:08,184:INFO:_master_model_container: 13
2023-07-11 09:00:08,184:INFO:_display_container: 2
2023-07-11 09:00:08,184:INFO:RandomForestRegressor(n_jobs=-1, random_state=5992)
2023-07-11 09:00:08,184:INFO:create_model() successfully completed......................................
2023-07-11 09:00:08,435:INFO:SubProcess create_model() end ==================================
2023-07-11 09:00:08,435:INFO:Creating metrics dataframe
2023-07-11 09:00:08,442:INFO:Initializing Extra Trees Regressor
2023-07-11 09:00:08,442:INFO:Total runtime is 2.381468995412191 minutes
2023-07-11 09:00:08,451:INFO:SubProcess create_model() called ==================================
2023-07-11 09:00:08,451:INFO:Initializing create_model()
2023-07-11 09:00:08,451:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000014F84749700>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000014F85D641F0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:00:08,451:INFO:Checking exceptions
2023-07-11 09:00:08,451:INFO:Importing libraries
2023-07-11 09:00:08,451:INFO:Copying training dataset
2023-07-11 09:00:08,546:INFO:Defining folds
2023-07-11 09:00:08,546:INFO:Declaring metric variables
2023-07-11 09:00:08,546:INFO:Importing untrained model
2023-07-11 09:00:08,546:INFO:Extra Trees Regressor Imported successfully
2023-07-11 09:00:08,546:INFO:Starting cross validation
2023-07-11 09:00:08,546:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:00:13,873:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.67s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-11 09:00:13,974:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.81s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-11 09:00:15,554:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.95s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-11 09:00:17,069:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:108: UserWarning: Persisting input arguments took 1.60s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-07-11 09:00:18,121:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.85s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-11 09:00:18,246:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 3.64s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-11 09:00:18,278:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 3.61s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-11 09:00:18,467:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 3.58s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-11 09:00:18,577:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.57s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-11 09:00:18,592:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.93s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-11 09:00:23,320:INFO:Calculating mean and std
2023-07-11 09:00:23,320:INFO:Creating metrics dataframe
2023-07-11 09:00:24,090:INFO:Uploading results into container
2023-07-11 09:00:24,090:INFO:Uploading model into container now
2023-07-11 09:00:24,090:INFO:_master_model_container: 14
2023-07-11 09:00:24,090:INFO:_display_container: 2
2023-07-11 09:00:24,090:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=5992)
2023-07-11 09:00:24,090:INFO:create_model() successfully completed......................................
2023-07-11 09:00:24,341:INFO:SubProcess create_model() end ==================================
2023-07-11 09:00:24,341:INFO:Creating metrics dataframe
2023-07-11 09:00:24,357:INFO:Initializing AdaBoost Regressor
2023-07-11 09:00:24,357:INFO:Total runtime is 2.646721049149831 minutes
2023-07-11 09:00:24,357:INFO:SubProcess create_model() called ==================================
2023-07-11 09:00:24,357:INFO:Initializing create_model()
2023-07-11 09:00:24,357:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000014F84749700>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000014F85D641F0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:00:24,357:INFO:Checking exceptions
2023-07-11 09:00:24,357:INFO:Importing libraries
2023-07-11 09:00:24,357:INFO:Copying training dataset
2023-07-11 09:00:24,435:INFO:Defining folds
2023-07-11 09:00:24,435:INFO:Declaring metric variables
2023-07-11 09:00:24,435:INFO:Importing untrained model
2023-07-11 09:00:24,435:INFO:AdaBoost Regressor Imported successfully
2023-07-11 09:00:24,435:INFO:Starting cross validation
2023-07-11 09:00:24,451:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:00:42,919:INFO:Calculating mean and std
2023-07-11 09:00:42,919:INFO:Creating metrics dataframe
2023-07-11 09:00:43,752:INFO:Uploading results into container
2023-07-11 09:00:43,752:INFO:Uploading model into container now
2023-07-11 09:00:43,752:INFO:_master_model_container: 15
2023-07-11 09:00:43,752:INFO:_display_container: 2
2023-07-11 09:00:43,752:INFO:AdaBoostRegressor(random_state=5992)
2023-07-11 09:00:43,752:INFO:create_model() successfully completed......................................
2023-07-11 09:00:44,019:INFO:SubProcess create_model() end ==================================
2023-07-11 09:00:44,019:INFO:Creating metrics dataframe
2023-07-11 09:00:44,035:INFO:Initializing Gradient Boosting Regressor
2023-07-11 09:00:44,035:INFO:Total runtime is 2.9746907075246174 minutes
2023-07-11 09:00:44,035:INFO:SubProcess create_model() called ==================================
2023-07-11 09:00:44,035:INFO:Initializing create_model()
2023-07-11 09:00:44,035:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000014F84749700>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000014F85D641F0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:00:44,035:INFO:Checking exceptions
2023-07-11 09:00:44,035:INFO:Importing libraries
2023-07-11 09:00:44,035:INFO:Copying training dataset
2023-07-11 09:00:44,114:INFO:Defining folds
2023-07-11 09:00:44,114:INFO:Declaring metric variables
2023-07-11 09:00:44,114:INFO:Importing untrained model
2023-07-11 09:00:44,114:INFO:Gradient Boosting Regressor Imported successfully
2023-07-11 09:00:44,114:INFO:Starting cross validation
2023-07-11 09:00:44,114:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:01:27,085:INFO:PyCaret RegressionExperiment
2023-07-11 09:01:27,085:INFO:Logging name: reg-default-name
2023-07-11 09:01:27,085:INFO:ML Usecase: MLUsecase.REGRESSION
2023-07-11 09:01:27,085:INFO:version 3.0.2
2023-07-11 09:01:27,085:INFO:Initializing setup()
2023-07-11 09:01:27,085:INFO:self.USI: 3e7f
2023-07-11 09:01:27,085:INFO:self._variable_keys: {'n_jobs_param', 'gpu_n_jobs_param', 'pipeline', 'target_param', 'seed', 'logging_param', 'idx', 'transform_target_param', '_available_plots', 'X', 'USI', 'exp_name_log', 'log_plots_param', 'y', 'y_test', 'X_train', 'y_train', 'memory', 'data', 'X_test', 'fold_shuffle_param', 'gpu_param', 'exp_id', 'fold_generator', 'html_param', 'fold_groups_param', '_ml_usecase'}
2023-07-11 09:01:27,085:INFO:Checking environment
2023-07-11 09:01:27,085:INFO:python_version: 3.9.13
2023-07-11 09:01:27,085:INFO:python_build: ('main', 'Oct 13 2022 21:23:06')
2023-07-11 09:01:27,085:INFO:machine: AMD64
2023-07-11 09:01:27,085:INFO:platform: Windows-10-10.0.22621-SP0
2023-07-11 09:01:27,085:INFO:Memory: svmem(total=16893358080, available=3645030400, percent=78.4, used=13248327680, free=3645030400)
2023-07-11 09:01:27,085:INFO:Physical Core: 8
2023-07-11 09:01:27,085:INFO:Logical Core: 16
2023-07-11 09:01:27,085:INFO:Checking libraries
2023-07-11 09:01:27,085:INFO:System:
2023-07-11 09:01:27,085:INFO:    python: 3.9.13 (main, Oct 13 2022, 21:23:06) [MSC v.1916 64 bit (AMD64)]
2023-07-11 09:01:27,085:INFO:executable: C:\Users\didit\anaconda3\python.exe
2023-07-11 09:01:27,085:INFO:   machine: Windows-10-10.0.22621-SP0
2023-07-11 09:01:27,085:INFO:PyCaret required dependencies:
2023-07-11 09:01:27,085:INFO:                 pip: 23.0.1
2023-07-11 09:01:27,085:INFO:          setuptools: 67.8.0
2023-07-11 09:01:27,085:INFO:             pycaret: 3.0.2
2023-07-11 09:01:27,085:INFO:             IPython: 8.12.0
2023-07-11 09:01:27,085:INFO:          ipywidgets: 8.0.4
2023-07-11 09:01:27,085:INFO:                tqdm: 4.65.0
2023-07-11 09:01:27,085:INFO:               numpy: 1.21.5
2023-07-11 09:01:27,085:INFO:              pandas: 1.5.3
2023-07-11 09:01:27,085:INFO:              jinja2: 3.1.2
2023-07-11 09:01:27,085:INFO:               scipy: 1.10.1
2023-07-11 09:01:27,085:INFO:              joblib: 1.2.0
2023-07-11 09:01:27,085:INFO:             sklearn: 1.2.2
2023-07-11 09:01:27,085:INFO:                pyod: 1.0.9
2023-07-11 09:01:27,085:INFO:            imblearn: 0.10.1
2023-07-11 09:01:27,085:INFO:   category_encoders: 2.6.1
2023-07-11 09:01:27,085:INFO:            lightgbm: 3.3.5
2023-07-11 09:01:27,095:INFO:               numba: 0.57.0
2023-07-11 09:01:27,095:INFO:            requests: 2.29.0
2023-07-11 09:01:27,095:INFO:          matplotlib: 3.7.1
2023-07-11 09:01:27,095:INFO:          scikitplot: 0.3.7
2023-07-11 09:01:27,095:INFO:         yellowbrick: 1.5
2023-07-11 09:01:27,095:INFO:              plotly: 5.9.0
2023-07-11 09:01:27,095:INFO:             kaleido: 0.2.1
2023-07-11 09:01:27,095:INFO:         statsmodels: 0.13.5
2023-07-11 09:01:27,095:INFO:              sktime: 0.17.0
2023-07-11 09:01:27,095:INFO:               tbats: 1.1.3
2023-07-11 09:01:27,095:INFO:            pmdarima: 2.0.3
2023-07-11 09:01:27,095:INFO:              psutil: 5.9.0
2023-07-11 09:01:27,095:INFO:PyCaret optional dependencies:
2023-07-11 09:01:27,095:INFO:                shap: 0.41.0
2023-07-11 09:01:27,095:INFO:           interpret: Not installed
2023-07-11 09:01:27,095:INFO:                umap: Not installed
2023-07-11 09:01:27,095:INFO:    pandas_profiling: 4.3.1
2023-07-11 09:01:27,095:INFO:  explainerdashboard: Not installed
2023-07-11 09:01:27,095:INFO:             autoviz: Not installed
2023-07-11 09:01:27,095:INFO:           fairlearn: Not installed
2023-07-11 09:01:27,095:INFO:             xgboost: 1.7.6
2023-07-11 09:01:27,095:INFO:            catboost: Not installed
2023-07-11 09:01:27,095:INFO:              kmodes: Not installed
2023-07-11 09:01:27,095:INFO:             mlxtend: Not installed
2023-07-11 09:01:27,095:INFO:       statsforecast: Not installed
2023-07-11 09:01:27,101:INFO:        tune_sklearn: Not installed
2023-07-11 09:01:27,101:INFO:                 ray: Not installed
2023-07-11 09:01:27,101:INFO:            hyperopt: Not installed
2023-07-11 09:01:27,101:INFO:              optuna: Not installed
2023-07-11 09:01:27,101:INFO:               skopt: Not installed
2023-07-11 09:01:27,101:INFO:              mlflow: 2.4.2
2023-07-11 09:01:27,101:INFO:              gradio: Not installed
2023-07-11 09:01:27,101:INFO:             fastapi: 0.95.2
2023-07-11 09:01:27,101:INFO:             uvicorn: 0.22.0
2023-07-11 09:01:27,101:INFO:              m2cgen: Not installed
2023-07-11 09:01:27,101:INFO:           evidently: Not installed
2023-07-11 09:01:27,101:INFO:               fugue: Not installed
2023-07-11 09:01:27,101:INFO:           streamlit: 1.23.1
2023-07-11 09:01:27,101:INFO:             prophet: Not installed
2023-07-11 09:01:27,101:INFO:None
2023-07-11 09:01:27,101:INFO:Set up data.
2023-07-11 09:01:27,446:INFO:Set up train/test split.
2023-07-11 09:01:27,525:INFO:Set up index.
2023-07-11 09:01:27,540:INFO:Set up folding strategy.
2023-07-11 09:01:27,540:INFO:Assigning column types.
2023-07-11 09:01:27,596:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-07-11 09:01:27,596:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-07-11 09:01:27,603:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-07-11 09:01:27,619:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 09:01:27,807:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 09:01:27,918:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 09:01:27,918:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:01:27,933:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:01:27,933:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-07-11 09:01:27,949:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-07-11 09:01:27,965:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 09:01:28,184:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 09:01:28,297:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 09:01:28,297:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:01:28,297:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:01:28,297:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-07-11 09:01:28,311:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-07-11 09:01:28,327:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 09:01:28,515:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 09:01:28,625:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 09:01:28,625:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:01:28,641:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:01:28,641:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-07-11 09:01:28,657:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 09:01:28,892:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 09:01:28,987:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 09:01:28,999:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:01:29,003:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:01:29,003:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-07-11 09:01:29,019:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 09:01:29,273:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 09:01:29,381:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 09:01:29,381:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:01:29,381:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:01:29,413:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 09:01:29,649:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 09:01:29,774:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 09:01:29,774:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:01:29,774:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:01:29,790:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-07-11 09:01:30,010:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 09:01:30,121:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 09:01:30,121:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:01:30,136:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:01:30,372:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 09:01:30,500:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 09:01:30,500:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:01:30,513:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:01:30,513:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-07-11 09:01:30,780:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 09:01:30,900:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:01:30,906:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:01:31,142:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 09:01:31,284:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:01:31,290:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:01:31,290:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-07-11 09:01:31,646:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:01:31,646:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:01:31,976:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:01:31,991:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:01:31,991:INFO:Preparing preprocessing pipeline...
2023-07-11 09:01:31,991:INFO:Set up simple imputation.
2023-07-11 09:01:32,070:INFO:Set up encoding of ordinal features.
2023-07-11 09:01:32,086:INFO:Set up encoding of categorical features.
2023-07-11 09:01:32,086:INFO:Set up column name cleaning.
2023-07-11 09:01:36,629:INFO:Finished creating preprocessing pipeline.
2023-07-11 09:01:36,713:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\didit\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['Postal Code', 'Model Year',
                                             'Electric Range',
                                             'Legislative District',
                                             'DOL Vehicle ID',
                                             '2020 Census Tract', 'Longitude',
                                             'latitude'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['County', 'City'...
                                    transformer=OneHotEncoder(cols=['State',
                                                                    'CAFV'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('rest_encoding',
                 TransformerWrapper(include=['County', 'City', 'Make', 'Model',
                                             'Electric Utility'],
                                    transformer=TargetEncoder(cols=['County',
                                                                    'City',
                                                                    'Make',
                                                                    'Model',
                                                                    'Electric '
                                                                    'Utility'],
                                                              handle_missing='return_nan'))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames()))])
2023-07-11 09:01:36,713:INFO:Creating final display dataframe.
2023-07-11 09:01:39,774:INFO:Calculating mean and std
2023-07-11 09:01:39,774:INFO:Creating metrics dataframe
2023-07-11 09:01:40,747:INFO:Setup _display_container:                     Description             Value
0                    Session id               992
1                        Target         Base MSRP
2                   Target type        Regression
3           Original data shape      (134474, 17)
4        Transformed data shape      (134474, 19)
5   Transformed train set shape       (94131, 19)
6    Transformed test set shape       (40343, 19)
7              Ordinal features                 1
8              Numeric features                 8
9          Categorical features                 8
10                   Preprocess              True
11              Imputation type            simple
12           Numeric imputation              mean
13       Categorical imputation              mode
14     Maximum one-hot encoding                25
15              Encoding method              None
16               Fold Generator             KFold
17                  Fold Number                10
18                     CPU Jobs                -1
19                      Use GPU             False
20               Log Experiment             False
21              Experiment Name  reg-default-name
22                          USI              3e7f
2023-07-11 09:01:41,125:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:01:41,125:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:01:41,486:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:01:41,501:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:01:41,501:INFO:setup() successfully completed in 15.41s...............
2023-07-11 09:01:41,519:INFO:Initializing compare_models()
2023-07-11 09:01:41,519:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000014F84698940>, include=None, fold=None, round=4, cross_validation=True, sort=R2, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x0000014F84698940>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2023-07-11 09:01:41,519:INFO:Checking exceptions
2023-07-11 09:01:41,564:INFO:Preparing display monitor
2023-07-11 09:01:41,564:INFO:Initializing Linear Regression
2023-07-11 09:01:41,564:INFO:Total runtime is 0.0 minutes
2023-07-11 09:01:41,564:INFO:SubProcess create_model() called ==================================
2023-07-11 09:01:41,564:INFO:Initializing create_model()
2023-07-11 09:01:41,564:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000014F84698940>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000014F86F29CA0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:01:41,564:INFO:Checking exceptions
2023-07-11 09:01:41,564:INFO:Importing libraries
2023-07-11 09:01:41,564:INFO:Copying training dataset
2023-07-11 09:01:41,674:INFO:Defining folds
2023-07-11 09:01:41,674:INFO:Declaring metric variables
2023-07-11 09:01:41,674:INFO:Importing untrained model
2023-07-11 09:01:41,674:INFO:Linear Regression Imported successfully
2023-07-11 09:01:41,674:INFO:Starting cross validation
2023-07-11 09:01:41,690:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:01:42,571:INFO:Uploading results into container
2023-07-11 09:01:42,571:INFO:Uploading model into container now
2023-07-11 09:01:42,571:INFO:_master_model_container: 16
2023-07-11 09:01:42,571:INFO:_display_container: 2
2023-07-11 09:01:42,571:INFO:GradientBoostingRegressor(random_state=5992)
2023-07-11 09:01:42,571:INFO:create_model() successfully completed......................................
2023-07-11 09:01:42,868:INFO:SubProcess create_model() end ==================================
2023-07-11 09:01:42,868:INFO:Creating metrics dataframe
2023-07-11 09:01:42,890:INFO:Initializing Extreme Gradient Boosting
2023-07-11 09:01:42,890:INFO:Total runtime is 3.955612258116404 minutes
2023-07-11 09:01:42,890:INFO:SubProcess create_model() called ==================================
2023-07-11 09:01:42,900:INFO:Initializing create_model()
2023-07-11 09:01:42,916:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000014F84749700>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000014F85D641F0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:01:42,932:INFO:Checking exceptions
2023-07-11 09:01:42,932:INFO:Importing libraries
2023-07-11 09:01:42,932:INFO:Copying training dataset
2023-07-11 09:01:43,230:INFO:Defining folds
2023-07-11 09:01:43,230:INFO:Declaring metric variables
2023-07-11 09:01:43,230:INFO:Importing untrained model
2023-07-11 09:01:43,230:INFO:Extreme Gradient Boosting Imported successfully
2023-07-11 09:01:43,230:INFO:Starting cross validation
2023-07-11 09:01:43,230:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:01:50,415:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.58s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-11 09:01:50,572:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.66s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-11 09:01:51,282:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-11 09:01:56,904:INFO:Calculating mean and std
2023-07-11 09:01:56,904:INFO:Creating metrics dataframe
2023-07-11 09:01:57,878:INFO:Uploading results into container
2023-07-11 09:01:57,878:INFO:Uploading model into container now
2023-07-11 09:01:57,878:INFO:_master_model_container: 1
2023-07-11 09:01:57,878:INFO:_display_container: 2
2023-07-11 09:01:57,878:INFO:LinearRegression(n_jobs=-1)
2023-07-11 09:01:57,878:INFO:create_model() successfully completed......................................
2023-07-11 09:01:58,161:INFO:SubProcess create_model() end ==================================
2023-07-11 09:01:58,161:INFO:Creating metrics dataframe
2023-07-11 09:01:58,176:INFO:Initializing Lasso Regression
2023-07-11 09:01:58,176:INFO:Total runtime is 0.27686367829640707 minutes
2023-07-11 09:01:58,176:INFO:SubProcess create_model() called ==================================
2023-07-11 09:01:58,176:INFO:Initializing create_model()
2023-07-11 09:01:58,176:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000014F84698940>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000014F86F29CA0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:01:58,176:INFO:Checking exceptions
2023-07-11 09:01:58,176:INFO:Importing libraries
2023-07-11 09:01:58,176:INFO:Copying training dataset
2023-07-11 09:01:58,270:INFO:Defining folds
2023-07-11 09:01:58,270:INFO:Declaring metric variables
2023-07-11 09:01:58,286:INFO:Importing untrained model
2023-07-11 09:01:58,286:INFO:Lasso Regression Imported successfully
2023-07-11 09:01:58,286:INFO:Starting cross validation
2023-07-11 09:01:58,286:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:02:00,407:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.58s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-07-11 09:02:01,571:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.60s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-11 09:02:07,023:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.63s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-11 09:02:07,133:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.65s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-11 09:02:08,343:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.10s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-11 09:02:08,814:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.58s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-07-11 09:02:09,307:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.71s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-07-11 09:02:09,396:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.57s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-07-11 09:02:09,459:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-11 09:02:09,970:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.70s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-11 09:02:17,340:INFO:Calculating mean and std
2023-07-11 09:02:17,340:INFO:Creating metrics dataframe
2023-07-11 09:02:18,518:INFO:Uploading results into container
2023-07-11 09:02:18,518:INFO:Uploading model into container now
2023-07-11 09:02:18,518:INFO:_master_model_container: 2
2023-07-11 09:02:18,518:INFO:_display_container: 2
2023-07-11 09:02:18,518:INFO:Lasso(random_state=992)
2023-07-11 09:02:18,518:INFO:create_model() successfully completed......................................
2023-07-11 09:02:18,787:INFO:SubProcess create_model() end ==================================
2023-07-11 09:02:18,787:INFO:Creating metrics dataframe
2023-07-11 09:02:18,801:INFO:Initializing Ridge Regression
2023-07-11 09:02:18,801:INFO:Total runtime is 0.6206051031748454 minutes
2023-07-11 09:02:18,801:INFO:SubProcess create_model() called ==================================
2023-07-11 09:02:18,801:INFO:Initializing create_model()
2023-07-11 09:02:18,801:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000014F84698940>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000014F86F29CA0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:02:18,801:INFO:Checking exceptions
2023-07-11 09:02:18,801:INFO:Importing libraries
2023-07-11 09:02:18,801:INFO:Copying training dataset
2023-07-11 09:02:18,926:INFO:Defining folds
2023-07-11 09:02:18,926:INFO:Declaring metric variables
2023-07-11 09:02:18,926:INFO:Importing untrained model
2023-07-11 09:02:18,926:INFO:Ridge Regression Imported successfully
2023-07-11 09:02:18,926:INFO:Starting cross validation
2023-07-11 09:02:18,926:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:02:21,167:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.59539e-21): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-11 09:02:21,183:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.5901e-21): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-11 09:02:21,232:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.58672e-21): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-11 09:02:21,262:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.59692e-21): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-11 09:02:21,357:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.5917e-21): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-11 09:02:21,436:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.59342e-21): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-11 09:02:21,499:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.59505e-21): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-11 09:02:21,514:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.5937e-21): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-11 09:02:21,593:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.59299e-21): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-11 09:02:21,719:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.58737e-21): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-11 09:02:23,558:INFO:Calculating mean and std
2023-07-11 09:02:23,558:INFO:Creating metrics dataframe
2023-07-11 09:02:24,642:INFO:Uploading results into container
2023-07-11 09:02:24,642:INFO:Uploading model into container now
2023-07-11 09:02:24,642:INFO:_master_model_container: 17
2023-07-11 09:02:24,642:INFO:_display_container: 2
2023-07-11 09:02:24,642:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             n_estimators=100, n_jobs=-1, num_parallel_tree=None,
             predictor=None, random_state=5992, ...)
2023-07-11 09:02:24,642:INFO:create_model() successfully completed......................................
2023-07-11 09:02:24,910:INFO:SubProcess create_model() end ==================================
2023-07-11 09:02:24,910:INFO:Creating metrics dataframe
2023-07-11 09:02:24,941:INFO:Initializing Light Gradient Boosting Machine
2023-07-11 09:02:24,941:INFO:Total runtime is 4.656459395090739 minutes
2023-07-11 09:02:24,941:INFO:SubProcess create_model() called ==================================
2023-07-11 09:02:24,941:INFO:Initializing create_model()
2023-07-11 09:02:24,941:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000014F84749700>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000014F85D641F0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:02:24,941:INFO:Checking exceptions
2023-07-11 09:02:24,941:INFO:Importing libraries
2023-07-11 09:02:24,941:INFO:Copying training dataset
2023-07-11 09:02:25,051:INFO:Defining folds
2023-07-11 09:02:25,051:INFO:Declaring metric variables
2023-07-11 09:02:25,051:INFO:Importing untrained model
2023-07-11 09:02:25,051:INFO:Light Gradient Boosting Machine Imported successfully
2023-07-11 09:02:25,051:INFO:Starting cross validation
2023-07-11 09:02:25,051:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:02:32,350:INFO:Calculating mean and std
2023-07-11 09:02:32,350:INFO:Creating metrics dataframe
2023-07-11 09:02:33,631:INFO:Uploading results into container
2023-07-11 09:02:33,631:INFO:Uploading model into container now
2023-07-11 09:02:33,631:INFO:_master_model_container: 3
2023-07-11 09:02:33,631:INFO:_display_container: 2
2023-07-11 09:02:33,631:INFO:Ridge(random_state=992)
2023-07-11 09:02:33,631:INFO:create_model() successfully completed......................................
2023-07-11 09:02:33,899:INFO:SubProcess create_model() end ==================================
2023-07-11 09:02:33,899:INFO:Creating metrics dataframe
2023-07-11 09:02:33,915:INFO:Initializing Elastic Net
2023-07-11 09:02:33,915:INFO:Total runtime is 0.8725131869316101 minutes
2023-07-11 09:02:33,915:INFO:SubProcess create_model() called ==================================
2023-07-11 09:02:33,915:INFO:Initializing create_model()
2023-07-11 09:02:33,915:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000014F84698940>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000014F86F29CA0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:02:33,915:INFO:Checking exceptions
2023-07-11 09:02:33,915:INFO:Importing libraries
2023-07-11 09:02:33,915:INFO:Copying training dataset
2023-07-11 09:02:34,009:INFO:Defining folds
2023-07-11 09:02:34,009:INFO:Declaring metric variables
2023-07-11 09:02:34,009:INFO:Importing untrained model
2023-07-11 09:02:34,009:INFO:Elastic Net Imported successfully
2023-07-11 09:02:34,009:INFO:Starting cross validation
2023-07-11 09:02:34,025:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:02:41,166:INFO:Calculating mean and std
2023-07-11 09:02:41,166:INFO:Creating metrics dataframe
2023-07-11 09:02:42,362:INFO:Uploading results into container
2023-07-11 09:02:42,362:INFO:Uploading model into container now
2023-07-11 09:02:42,362:INFO:_master_model_container: 18
2023-07-11 09:02:42,362:INFO:_display_container: 2
2023-07-11 09:02:42,362:INFO:LGBMRegressor(random_state=5992)
2023-07-11 09:02:42,362:INFO:create_model() successfully completed......................................
2023-07-11 09:02:42,623:INFO:SubProcess create_model() end ==================================
2023-07-11 09:02:42,623:INFO:Creating metrics dataframe
2023-07-11 09:02:42,630:INFO:Initializing Dummy Regressor
2023-07-11 09:02:42,630:INFO:Total runtime is 4.95127078294754 minutes
2023-07-11 09:02:42,630:INFO:SubProcess create_model() called ==================================
2023-07-11 09:02:42,630:INFO:Initializing create_model()
2023-07-11 09:02:42,630:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000014F84749700>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000014F85D641F0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:02:42,630:INFO:Checking exceptions
2023-07-11 09:02:42,630:INFO:Importing libraries
2023-07-11 09:02:42,630:INFO:Copying training dataset
2023-07-11 09:02:42,740:INFO:Defining folds
2023-07-11 09:02:42,740:INFO:Declaring metric variables
2023-07-11 09:02:42,740:INFO:Importing untrained model
2023-07-11 09:02:42,740:INFO:Dummy Regressor Imported successfully
2023-07-11 09:02:42,740:INFO:Starting cross validation
2023-07-11 09:02:42,755:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:02:49,773:INFO:Calculating mean and std
2023-07-11 09:02:49,773:INFO:Creating metrics dataframe
2023-07-11 09:02:51,031:INFO:Uploading results into container
2023-07-11 09:02:51,031:INFO:Uploading model into container now
2023-07-11 09:02:51,039:INFO:_master_model_container: 4
2023-07-11 09:02:51,039:INFO:_display_container: 2
2023-07-11 09:02:51,039:INFO:ElasticNet(random_state=992)
2023-07-11 09:02:51,039:INFO:create_model() successfully completed......................................
2023-07-11 09:02:51,299:INFO:SubProcess create_model() end ==================================
2023-07-11 09:02:51,299:INFO:Creating metrics dataframe
2023-07-11 09:02:51,302:INFO:Initializing Least Angle Regression
2023-07-11 09:02:51,302:INFO:Total runtime is 1.1622877240180969 minutes
2023-07-11 09:02:51,302:INFO:SubProcess create_model() called ==================================
2023-07-11 09:02:51,302:INFO:Initializing create_model()
2023-07-11 09:02:51,302:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000014F84698940>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000014F86F29CA0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:02:51,302:INFO:Checking exceptions
2023-07-11 09:02:51,302:INFO:Importing libraries
2023-07-11 09:02:51,302:INFO:Copying training dataset
2023-07-11 09:02:51,410:INFO:Defining folds
2023-07-11 09:02:51,410:INFO:Declaring metric variables
2023-07-11 09:02:51,410:INFO:Importing untrained model
2023-07-11 09:02:51,410:INFO:Least Angle Regression Imported successfully
2023-07-11 09:02:51,410:INFO:Starting cross validation
2023-07-11 09:02:51,425:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:02:58,464:INFO:Calculating mean and std
2023-07-11 09:02:58,464:INFO:Creating metrics dataframe
2023-07-11 09:02:59,614:INFO:Uploading results into container
2023-07-11 09:02:59,629:INFO:Uploading model into container now
2023-07-11 09:02:59,629:INFO:_master_model_container: 19
2023-07-11 09:02:59,629:INFO:_display_container: 2
2023-07-11 09:02:59,629:INFO:DummyRegressor()
2023-07-11 09:02:59,629:INFO:create_model() successfully completed......................................
2023-07-11 09:02:59,881:INFO:SubProcess create_model() end ==================================
2023-07-11 09:02:59,881:INFO:Creating metrics dataframe
2023-07-11 09:02:59,896:INFO:Initializing create_model()
2023-07-11 09:02:59,896:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000014F84749700>, estimator=ExtraTreesRegressor(n_jobs=-1, random_state=5992), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:02:59,896:INFO:Checking exceptions
2023-07-11 09:02:59,896:INFO:Importing libraries
2023-07-11 09:02:59,896:INFO:Copying training dataset
2023-07-11 09:03:00,006:INFO:Defining folds
2023-07-11 09:03:00,006:INFO:Declaring metric variables
2023-07-11 09:03:00,006:INFO:Importing untrained model
2023-07-11 09:03:00,006:INFO:Declaring custom model
2023-07-11 09:03:00,006:INFO:Extra Trees Regressor Imported successfully
2023-07-11 09:03:00,006:INFO:Cross validation set to False
2023-07-11 09:03:00,006:INFO:Fitting Model
2023-07-11 09:03:05,824:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=5992)
2023-07-11 09:03:05,824:INFO:create_model() successfully completed......................................
2023-07-11 09:03:06,139:INFO:                                    Model         MAE           MSE  \
2023-07-11 09:03:06,139:INFO:et                  Extra Trees Regressor     32.7814  8.634143e+06   
2023-07-11 09:03:06,139:INFO:xgboost         Extreme Gradient Boosting     63.8847  9.172378e+06   
2023-07-11 09:03:06,139:INFO:rf                Random Forest Regressor     43.2120  9.435058e+06   
2023-07-11 09:03:06,139:INFO:lightgbm  Light Gradient Boosting Machine    102.8055  1.002164e+07   
2023-07-11 09:03:06,139:INFO:gbr           Gradient Boosting Regressor    328.8876  1.363560e+07   
2023-07-11 09:03:06,139:INFO:dt                Decision Tree Regressor     39.1905  1.710203e+07   
2023-07-11 09:03:06,139:INFO:ada                    AdaBoost Regressor   1550.7958  3.597573e+07   
2023-07-11 09:03:06,139:INFO:br                         Bayesian Ridge   2785.9634  6.482939e+07   
2023-07-11 09:03:06,139:INFO:llar         Lasso Least Angle Regression   2786.8783  6.482905e+07   
2023-07-11 09:03:06,139:INFO:ridge                    Ridge Regression   2789.1615  6.482870e+07   
2023-07-11 09:03:06,139:INFO:lasso                    Lasso Regression   2786.9666  6.482917e+07   
2023-07-11 09:03:06,139:INFO:lr                      Linear Regression   2789.2313  6.482869e+07   
2023-07-11 09:03:06,139:INFO:en                            Elastic Net   2684.5860  6.542628e+07   
2023-07-11 09:03:06,139:INFO:omp           Orthogonal Matching Pursuit   2816.3389  9.570543e+07   
2023-07-11 09:03:06,139:INFO:dummy                     Dummy Regressor   2816.3586  9.571344e+07   
2023-07-11 09:03:06,139:INFO:par          Passive Aggressive Regressor   1624.4913  9.742992e+07   
2023-07-11 09:03:06,139:INFO:huber                     Huber Regressor   1444.5743  9.779650e+07   
2023-07-11 09:03:06,139:INFO:knn                 K Neighbors Regressor   2356.6619  1.046075e+08   
2023-07-11 09:03:06,139:INFO:lar                Least Angle Regression  32313.8810  3.933113e+09   
2023-07-11 09:03:06,139:INFO:
2023-07-11 09:03:06,139:INFO:                RMSE       R2   RMSLE    MAPE  TT (Sec)  
2023-07-11 09:03:06,139:INFO:et         1624.8129   0.9454  0.3749  0.0079     1.477  
2023-07-11 09:03:06,139:INFO:xgboost    1862.4142   0.9397  1.6750  0.0117     4.033  
2023-07-11 09:03:06,139:INFO:rf         1948.1651   0.9364  0.2767  0.0096     2.210  
2023-07-11 09:03:06,139:INFO:lightgbm   2244.3547   0.9292  1.6008  0.0294     1.611  
2023-07-11 09:03:06,139:INFO:gbr        3101.6414   0.8894  3.5430  0.1046     5.566  
2023-07-11 09:03:06,139:INFO:dt         2630.0973   0.8537  0.1928  0.0069     0.690  
2023-07-11 09:03:06,139:INFO:ada        5633.9622   0.6643  3.2211  0.3203     1.847  
2023-07-11 09:03:06,139:INFO:br         7952.4027   0.3333  6.7808  0.5118     0.686  
2023-07-11 09:03:06,139:INFO:llar       7952.3851   0.3333  6.7809  0.5117     0.604  
2023-07-11 09:03:06,139:INFO:ridge      7952.3718   0.3333  6.7817  0.5116     0.614  
2023-07-11 09:03:06,139:INFO:lasso      7952.3932   0.3333  6.7810  0.5117     1.563  
2023-07-11 09:03:06,139:INFO:lr         7952.3716   0.3333  6.7818  0.5116     1.003  
2023-07-11 09:03:06,139:INFO:en         7989.6448   0.3270  6.7242  0.5234     0.649  
2023-07-11 09:03:06,139:INFO:omp        9712.9591  -0.0001  7.2048  0.9719     0.633  
2023-07-11 09:03:06,139:INFO:dummy      9713.3923  -0.0002  7.2072  0.9718     1.571  
2023-07-11 09:03:06,139:INFO:par        9800.0013  -0.0181  4.7911  0.9971     0.686  
2023-07-11 09:03:06,139:INFO:huber      9819.8176  -0.0224  1.7246  1.0000     0.770  
2023-07-11 09:03:06,139:INFO:knn       10166.0336  -0.0980  3.0310  0.9393     2.265  
2023-07-11 09:03:06,139:INFO:lar       46832.2468 -44.1469  9.0933  1.2162     0.618  
2023-07-11 09:03:06,139:INFO:_master_model_container: 19
2023-07-11 09:03:06,139:INFO:_display_container: 2
2023-07-11 09:03:06,139:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=5992)
2023-07-11 09:03:06,139:INFO:compare_models() successfully completed......................................
2023-07-11 09:03:10,510:INFO:Calculating mean and std
2023-07-11 09:03:10,510:INFO:Creating metrics dataframe
2023-07-11 09:03:11,611:INFO:Uploading results into container
2023-07-11 09:03:11,611:INFO:Uploading model into container now
2023-07-11 09:03:11,611:INFO:_master_model_container: 5
2023-07-11 09:03:11,611:INFO:_display_container: 2
2023-07-11 09:03:11,611:INFO:Lars(random_state=992)
2023-07-11 09:03:11,611:INFO:create_model() successfully completed......................................
2023-07-11 09:03:11,878:INFO:SubProcess create_model() end ==================================
2023-07-11 09:03:11,878:INFO:Creating metrics dataframe
2023-07-11 09:03:11,892:INFO:Initializing Lasso Least Angle Regression
2023-07-11 09:03:11,892:INFO:Total runtime is 1.505465813477834 minutes
2023-07-11 09:03:11,892:INFO:SubProcess create_model() called ==================================
2023-07-11 09:03:11,892:INFO:Initializing create_model()
2023-07-11 09:03:11,892:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000014F84698940>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000014F86F29CA0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:03:11,892:INFO:Checking exceptions
2023-07-11 09:03:11,892:INFO:Importing libraries
2023-07-11 09:03:11,892:INFO:Copying training dataset
2023-07-11 09:03:12,013:INFO:Defining folds
2023-07-11 09:03:12,013:INFO:Declaring metric variables
2023-07-11 09:03:12,013:INFO:Importing untrained model
2023-07-11 09:03:12,013:INFO:Lasso Least Angle Regression Imported successfully
2023-07-11 09:03:12,013:INFO:Starting cross validation
2023-07-11 09:03:12,013:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:03:20,947:INFO:Calculating mean and std
2023-07-11 09:03:20,947:INFO:Creating metrics dataframe
2023-07-11 09:03:22,068:INFO:Uploading results into container
2023-07-11 09:03:22,068:INFO:Uploading model into container now
2023-07-11 09:03:22,068:INFO:_master_model_container: 6
2023-07-11 09:03:22,068:INFO:_display_container: 2
2023-07-11 09:03:22,068:INFO:LassoLars(random_state=992)
2023-07-11 09:03:22,068:INFO:create_model() successfully completed......................................
2023-07-11 09:03:22,274:INFO:SubProcess create_model() end ==================================
2023-07-11 09:03:22,274:INFO:Creating metrics dataframe
2023-07-11 09:03:22,276:INFO:Initializing Orthogonal Matching Pursuit
2023-07-11 09:03:22,276:INFO:Total runtime is 1.678520349661509 minutes
2023-07-11 09:03:22,276:INFO:SubProcess create_model() called ==================================
2023-07-11 09:03:22,276:INFO:Initializing create_model()
2023-07-11 09:03:22,276:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000014F84698940>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000014F86F29CA0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:03:22,276:INFO:Checking exceptions
2023-07-11 09:03:22,276:INFO:Importing libraries
2023-07-11 09:03:22,276:INFO:Copying training dataset
2023-07-11 09:03:22,336:INFO:Defining folds
2023-07-11 09:03:22,336:INFO:Declaring metric variables
2023-07-11 09:03:22,336:INFO:Importing untrained model
2023-07-11 09:03:22,352:INFO:Orthogonal Matching Pursuit Imported successfully
2023-07-11 09:03:22,352:INFO:Starting cross validation
2023-07-11 09:03:22,352:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:03:31,031:INFO:Calculating mean and std
2023-07-11 09:03:31,031:INFO:Creating metrics dataframe
2023-07-11 09:03:32,166:INFO:Uploading results into container
2023-07-11 09:03:32,166:INFO:Uploading model into container now
2023-07-11 09:03:32,166:INFO:_master_model_container: 7
2023-07-11 09:03:32,166:INFO:_display_container: 2
2023-07-11 09:03:32,166:INFO:OrthogonalMatchingPursuit()
2023-07-11 09:03:32,166:INFO:create_model() successfully completed......................................
2023-07-11 09:03:32,419:INFO:SubProcess create_model() end ==================================
2023-07-11 09:03:32,419:INFO:Creating metrics dataframe
2023-07-11 09:03:32,433:INFO:Initializing Bayesian Ridge
2023-07-11 09:03:32,433:INFO:Total runtime is 1.8478113532066345 minutes
2023-07-11 09:03:32,433:INFO:SubProcess create_model() called ==================================
2023-07-11 09:03:32,435:INFO:Initializing create_model()
2023-07-11 09:03:32,435:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000014F84698940>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000014F86F29CA0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:03:32,435:INFO:Checking exceptions
2023-07-11 09:03:32,435:INFO:Importing libraries
2023-07-11 09:03:32,435:INFO:Copying training dataset
2023-07-11 09:03:32,543:INFO:Defining folds
2023-07-11 09:03:32,543:INFO:Declaring metric variables
2023-07-11 09:03:32,543:INFO:Importing untrained model
2023-07-11 09:03:32,543:INFO:Bayesian Ridge Imported successfully
2023-07-11 09:03:32,543:INFO:Starting cross validation
2023-07-11 09:03:32,543:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:03:41,751:INFO:Calculating mean and std
2023-07-11 09:03:41,751:INFO:Creating metrics dataframe
2023-07-11 09:03:42,859:INFO:Uploading results into container
2023-07-11 09:03:42,859:INFO:Uploading model into container now
2023-07-11 09:03:42,859:INFO:_master_model_container: 8
2023-07-11 09:03:42,859:INFO:_display_container: 2
2023-07-11 09:03:42,859:INFO:BayesianRidge()
2023-07-11 09:03:42,859:INFO:create_model() successfully completed......................................
2023-07-11 09:03:43,110:INFO:SubProcess create_model() end ==================================
2023-07-11 09:03:43,110:INFO:Creating metrics dataframe
2023-07-11 09:03:43,126:INFO:Initializing Passive Aggressive Regressor
2023-07-11 09:03:43,126:INFO:Total runtime is 2.0260347763697304 minutes
2023-07-11 09:03:43,126:INFO:SubProcess create_model() called ==================================
2023-07-11 09:03:43,126:INFO:Initializing create_model()
2023-07-11 09:03:43,126:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000014F84698940>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000014F86F29CA0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:03:43,126:INFO:Checking exceptions
2023-07-11 09:03:43,126:INFO:Importing libraries
2023-07-11 09:03:43,126:INFO:Copying training dataset
2023-07-11 09:03:43,237:INFO:Defining folds
2023-07-11 09:03:43,237:INFO:Declaring metric variables
2023-07-11 09:03:43,237:INFO:Importing untrained model
2023-07-11 09:03:43,237:INFO:Passive Aggressive Regressor Imported successfully
2023-07-11 09:03:43,237:INFO:Starting cross validation
2023-07-11 09:03:43,237:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:03:51,604:INFO:Calculating mean and std
2023-07-11 09:03:51,604:INFO:Creating metrics dataframe
2023-07-11 09:03:52,753:INFO:Uploading results into container
2023-07-11 09:03:52,753:INFO:Uploading model into container now
2023-07-11 09:03:52,753:INFO:_master_model_container: 9
2023-07-11 09:03:52,753:INFO:_display_container: 2
2023-07-11 09:03:52,753:INFO:PassiveAggressiveRegressor(random_state=992)
2023-07-11 09:03:52,753:INFO:create_model() successfully completed......................................
2023-07-11 09:03:53,004:INFO:SubProcess create_model() end ==================================
2023-07-11 09:03:53,004:INFO:Creating metrics dataframe
2023-07-11 09:03:53,020:INFO:Initializing Huber Regressor
2023-07-11 09:03:53,020:INFO:Total runtime is 2.1909206628799436 minutes
2023-07-11 09:03:53,020:INFO:SubProcess create_model() called ==================================
2023-07-11 09:03:53,020:INFO:Initializing create_model()
2023-07-11 09:03:53,020:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000014F84698940>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000014F86F29CA0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:03:53,020:INFO:Checking exceptions
2023-07-11 09:03:53,020:INFO:Importing libraries
2023-07-11 09:03:53,020:INFO:Copying training dataset
2023-07-11 09:03:53,115:INFO:Defining folds
2023-07-11 09:03:53,115:INFO:Declaring metric variables
2023-07-11 09:03:53,115:INFO:Importing untrained model
2023-07-11 09:03:53,115:INFO:Huber Regressor Imported successfully
2023-07-11 09:03:53,130:INFO:Starting cross validation
2023-07-11 09:03:53,130:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:04:03,111:INFO:Calculating mean and std
2023-07-11 09:04:03,111:INFO:Creating metrics dataframe
2023-07-11 09:04:04,261:INFO:Uploading results into container
2023-07-11 09:04:04,261:INFO:Uploading model into container now
2023-07-11 09:04:04,261:INFO:_master_model_container: 10
2023-07-11 09:04:04,261:INFO:_display_container: 2
2023-07-11 09:04:04,261:INFO:HuberRegressor()
2023-07-11 09:04:04,261:INFO:create_model() successfully completed......................................
2023-07-11 09:04:04,535:INFO:SubProcess create_model() end ==================================
2023-07-11 09:04:04,535:INFO:Creating metrics dataframe
2023-07-11 09:04:04,550:INFO:Initializing K Neighbors Regressor
2023-07-11 09:04:04,550:INFO:Total runtime is 2.3831006805102026 minutes
2023-07-11 09:04:04,550:INFO:SubProcess create_model() called ==================================
2023-07-11 09:04:04,550:INFO:Initializing create_model()
2023-07-11 09:04:04,550:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000014F84698940>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000014F86F29CA0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:04:04,550:INFO:Checking exceptions
2023-07-11 09:04:04,550:INFO:Importing libraries
2023-07-11 09:04:04,550:INFO:Copying training dataset
2023-07-11 09:04:04,631:INFO:Defining folds
2023-07-11 09:04:04,631:INFO:Declaring metric variables
2023-07-11 09:04:04,631:INFO:Importing untrained model
2023-07-11 09:04:04,631:INFO:K Neighbors Regressor Imported successfully
2023-07-11 09:04:04,631:INFO:Starting cross validation
2023-07-11 09:04:04,645:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:04:30,086:INFO:Calculating mean and std
2023-07-11 09:04:30,086:INFO:Creating metrics dataframe
2023-07-11 09:04:31,222:INFO:Uploading results into container
2023-07-11 09:04:31,222:INFO:Uploading model into container now
2023-07-11 09:04:31,237:INFO:_master_model_container: 11
2023-07-11 09:04:31,237:INFO:_display_container: 2
2023-07-11 09:04:31,237:INFO:KNeighborsRegressor(n_jobs=-1)
2023-07-11 09:04:31,237:INFO:create_model() successfully completed......................................
2023-07-11 09:04:31,484:INFO:SubProcess create_model() end ==================================
2023-07-11 09:04:31,484:INFO:Creating metrics dataframe
2023-07-11 09:04:31,495:INFO:Initializing Decision Tree Regressor
2023-07-11 09:04:31,495:INFO:Total runtime is 2.83218183517456 minutes
2023-07-11 09:04:31,495:INFO:SubProcess create_model() called ==================================
2023-07-11 09:04:31,495:INFO:Initializing create_model()
2023-07-11 09:04:31,495:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000014F84698940>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000014F86F29CA0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:04:31,495:INFO:Checking exceptions
2023-07-11 09:04:31,495:INFO:Importing libraries
2023-07-11 09:04:31,495:INFO:Copying training dataset
2023-07-11 09:04:31,595:INFO:Defining folds
2023-07-11 09:04:31,595:INFO:Declaring metric variables
2023-07-11 09:04:31,595:INFO:Importing untrained model
2023-07-11 09:04:31,595:INFO:Decision Tree Regressor Imported successfully
2023-07-11 09:04:31,595:INFO:Starting cross validation
2023-07-11 09:04:31,595:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:04:41,151:INFO:Calculating mean and std
2023-07-11 09:04:41,151:INFO:Creating metrics dataframe
2023-07-11 09:04:42,330:INFO:Uploading results into container
2023-07-11 09:04:42,330:INFO:Uploading model into container now
2023-07-11 09:04:42,330:INFO:_master_model_container: 12
2023-07-11 09:04:42,330:INFO:_display_container: 2
2023-07-11 09:04:42,330:INFO:DecisionTreeRegressor(random_state=992)
2023-07-11 09:04:42,330:INFO:create_model() successfully completed......................................
2023-07-11 09:04:42,582:INFO:SubProcess create_model() end ==================================
2023-07-11 09:04:42,582:INFO:Creating metrics dataframe
2023-07-11 09:04:42,598:INFO:Initializing Random Forest Regressor
2023-07-11 09:04:42,598:INFO:Total runtime is 3.0172310471534725 minutes
2023-07-11 09:04:42,598:INFO:SubProcess create_model() called ==================================
2023-07-11 09:04:42,598:INFO:Initializing create_model()
2023-07-11 09:04:42,598:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000014F84698940>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000014F86F29CA0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:04:42,598:INFO:Checking exceptions
2023-07-11 09:04:42,598:INFO:Importing libraries
2023-07-11 09:04:42,598:INFO:Copying training dataset
2023-07-11 09:04:42,676:INFO:Defining folds
2023-07-11 09:04:42,676:INFO:Declaring metric variables
2023-07-11 09:04:42,676:INFO:Importing untrained model
2023-07-11 09:04:42,676:INFO:Random Forest Regressor Imported successfully
2023-07-11 09:04:42,676:INFO:Starting cross validation
2023-07-11 09:04:42,692:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:04:53,199:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 2.31s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-11 09:04:54,394:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 1.93s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-11 09:04:54,692:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 1.43s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-11 09:04:55,635:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 1.38s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-11 09:04:56,170:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 1.07s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-11 09:04:56,468:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.90s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-11 09:04:56,609:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:108: UserWarning: Persisting input arguments took 2.00s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-07-11 09:04:57,348:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.86s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-11 09:04:57,725:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:108: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-07-11 09:04:58,290:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:108: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-07-11 09:04:58,337:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:108: UserWarning: Persisting input arguments took 1.93s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-07-11 09:04:58,526:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:108: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-07-11 09:04:58,673:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:108: UserWarning: Persisting input arguments took 1.64s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-07-11 09:04:58,966:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.18s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-11 09:04:59,139:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.32s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-11 09:04:59,155:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.63s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-11 09:04:59,926:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.04s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-11 09:04:59,973:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.07s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-11 09:05:00,084:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.12s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-11 09:05:00,868:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.48s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-11 09:05:01,121:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.10s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-11 09:05:01,137:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.79s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-11 09:05:07,513:INFO:Calculating mean and std
2023-07-11 09:05:07,513:INFO:Creating metrics dataframe
2023-07-11 09:05:08,764:INFO:Uploading results into container
2023-07-11 09:05:08,764:INFO:Uploading model into container now
2023-07-11 09:05:08,764:INFO:_master_model_container: 13
2023-07-11 09:05:08,764:INFO:_display_container: 2
2023-07-11 09:05:08,764:INFO:RandomForestRegressor(n_jobs=-1, random_state=992)
2023-07-11 09:05:08,764:INFO:create_model() successfully completed......................................
2023-07-11 09:05:09,015:INFO:SubProcess create_model() end ==================================
2023-07-11 09:05:09,015:INFO:Creating metrics dataframe
2023-07-11 09:05:09,015:INFO:Initializing Extra Trees Regressor
2023-07-11 09:05:09,015:INFO:Total runtime is 3.457515982786814 minutes
2023-07-11 09:05:09,015:INFO:SubProcess create_model() called ==================================
2023-07-11 09:05:09,015:INFO:Initializing create_model()
2023-07-11 09:05:09,015:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000014F84698940>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000014F86F29CA0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:05:09,015:INFO:Checking exceptions
2023-07-11 09:05:09,015:INFO:Importing libraries
2023-07-11 09:05:09,015:INFO:Copying training dataset
2023-07-11 09:05:09,094:INFO:Defining folds
2023-07-11 09:05:09,094:INFO:Declaring metric variables
2023-07-11 09:05:09,094:INFO:Importing untrained model
2023-07-11 09:05:09,094:INFO:Extra Trees Regressor Imported successfully
2023-07-11 09:05:09,094:INFO:Starting cross validation
2023-07-11 09:05:09,094:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:05:14,747:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 2.39s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-11 09:05:16,538:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-11 09:05:16,598:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.57s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-11 09:05:18,101:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.89s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-11 09:05:18,458:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.98s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-11 09:05:18,505:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.98s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-11 09:05:18,568:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.84s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-11 09:05:18,584:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.91s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-11 09:05:18,774:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.86s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-11 09:05:19,106:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.72s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-11 09:05:26,287:INFO:Calculating mean and std
2023-07-11 09:05:26,287:INFO:Creating metrics dataframe
2023-07-11 09:05:27,564:INFO:Uploading results into container
2023-07-11 09:05:27,564:INFO:Uploading model into container now
2023-07-11 09:05:27,564:INFO:_master_model_container: 14
2023-07-11 09:05:27,564:INFO:_display_container: 2
2023-07-11 09:05:27,564:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=992)
2023-07-11 09:05:27,564:INFO:create_model() successfully completed......................................
2023-07-11 09:05:27,833:INFO:SubProcess create_model() end ==================================
2023-07-11 09:05:27,833:INFO:Creating metrics dataframe
2023-07-11 09:05:27,833:INFO:Initializing AdaBoost Regressor
2023-07-11 09:05:27,833:INFO:Total runtime is 3.77114907503128 minutes
2023-07-11 09:05:27,833:INFO:SubProcess create_model() called ==================================
2023-07-11 09:05:27,833:INFO:Initializing create_model()
2023-07-11 09:05:27,833:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000014F84698940>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000014F86F29CA0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:05:27,833:INFO:Checking exceptions
2023-07-11 09:05:27,849:INFO:Importing libraries
2023-07-11 09:05:27,849:INFO:Copying training dataset
2023-07-11 09:05:27,944:INFO:Defining folds
2023-07-11 09:05:27,944:INFO:Declaring metric variables
2023-07-11 09:05:27,944:INFO:Importing untrained model
2023-07-11 09:05:27,944:INFO:AdaBoost Regressor Imported successfully
2023-07-11 09:05:27,944:INFO:Starting cross validation
2023-07-11 09:05:27,944:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:05:45,519:INFO:Calculating mean and std
2023-07-11 09:05:45,519:INFO:Creating metrics dataframe
2023-07-11 09:05:46,882:INFO:Uploading results into container
2023-07-11 09:05:46,882:INFO:Uploading model into container now
2023-07-11 09:05:46,882:INFO:_master_model_container: 15
2023-07-11 09:05:46,882:INFO:_display_container: 2
2023-07-11 09:05:46,882:INFO:AdaBoostRegressor(random_state=992)
2023-07-11 09:05:46,882:INFO:create_model() successfully completed......................................
2023-07-11 09:05:47,128:INFO:SubProcess create_model() end ==================================
2023-07-11 09:05:47,128:INFO:Creating metrics dataframe
2023-07-11 09:05:47,149:INFO:Initializing Gradient Boosting Regressor
2023-07-11 09:05:47,149:INFO:Total runtime is 4.093080671628316 minutes
2023-07-11 09:05:47,149:INFO:SubProcess create_model() called ==================================
2023-07-11 09:05:47,149:INFO:Initializing create_model()
2023-07-11 09:05:47,149:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000014F84698940>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000014F86F29CA0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:05:47,149:INFO:Checking exceptions
2023-07-11 09:05:47,149:INFO:Importing libraries
2023-07-11 09:05:47,149:INFO:Copying training dataset
2023-07-11 09:05:47,239:INFO:Defining folds
2023-07-11 09:05:47,239:INFO:Declaring metric variables
2023-07-11 09:05:47,239:INFO:Importing untrained model
2023-07-11 09:05:47,239:INFO:Gradient Boosting Regressor Imported successfully
2023-07-11 09:05:47,239:INFO:Starting cross validation
2023-07-11 09:05:47,249:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:06:34,745:INFO:Calculating mean and std
2023-07-11 09:06:34,745:INFO:Creating metrics dataframe
2023-07-11 09:06:36,059:INFO:Uploading results into container
2023-07-11 09:06:36,059:INFO:Uploading model into container now
2023-07-11 09:06:36,059:INFO:_master_model_container: 16
2023-07-11 09:06:36,059:INFO:_display_container: 2
2023-07-11 09:06:36,064:INFO:GradientBoostingRegressor(random_state=992)
2023-07-11 09:06:36,064:INFO:create_model() successfully completed......................................
2023-07-11 09:06:36,353:INFO:SubProcess create_model() end ==================================
2023-07-11 09:06:36,353:INFO:Creating metrics dataframe
2023-07-11 09:06:36,366:INFO:Initializing Extreme Gradient Boosting
2023-07-11 09:06:36,366:INFO:Total runtime is 4.91335258881251 minutes
2023-07-11 09:06:36,366:INFO:SubProcess create_model() called ==================================
2023-07-11 09:06:36,366:INFO:Initializing create_model()
2023-07-11 09:06:36,366:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000014F84698940>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000014F86F29CA0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:06:36,366:INFO:Checking exceptions
2023-07-11 09:06:36,366:INFO:Importing libraries
2023-07-11 09:06:36,366:INFO:Copying training dataset
2023-07-11 09:06:36,455:INFO:Defining folds
2023-07-11 09:06:36,455:INFO:Declaring metric variables
2023-07-11 09:06:36,455:INFO:Importing untrained model
2023-07-11 09:06:36,455:INFO:Extreme Gradient Boosting Imported successfully
2023-07-11 09:06:36,455:INFO:Starting cross validation
2023-07-11 09:06:36,464:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:07:09,348:INFO:Calculating mean and std
2023-07-11 09:07:09,348:INFO:Creating metrics dataframe
2023-07-11 09:07:10,645:INFO:Uploading results into container
2023-07-11 09:07:10,645:INFO:Uploading model into container now
2023-07-11 09:07:10,645:INFO:_master_model_container: 17
2023-07-11 09:07:10,645:INFO:_display_container: 2
2023-07-11 09:07:10,645:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             n_estimators=100, n_jobs=-1, num_parallel_tree=None,
             predictor=None, random_state=992, ...)
2023-07-11 09:07:10,645:INFO:create_model() successfully completed......................................
2023-07-11 09:07:10,896:INFO:SubProcess create_model() end ==================================
2023-07-11 09:07:10,896:INFO:Creating metrics dataframe
2023-07-11 09:07:10,904:INFO:Initializing Light Gradient Boosting Machine
2023-07-11 09:07:10,904:INFO:Total runtime is 5.488997872670491 minutes
2023-07-11 09:07:10,904:INFO:SubProcess create_model() called ==================================
2023-07-11 09:07:10,904:INFO:Initializing create_model()
2023-07-11 09:07:10,904:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000014F84698940>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000014F86F29CA0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:07:10,904:INFO:Checking exceptions
2023-07-11 09:07:10,904:INFO:Importing libraries
2023-07-11 09:07:10,904:INFO:Copying training dataset
2023-07-11 09:07:10,999:INFO:Defining folds
2023-07-11 09:07:10,999:INFO:Declaring metric variables
2023-07-11 09:07:10,999:INFO:Importing untrained model
2023-07-11 09:07:10,999:INFO:Light Gradient Boosting Machine Imported successfully
2023-07-11 09:07:10,999:INFO:Starting cross validation
2023-07-11 09:07:11,015:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:07:23,500:INFO:Calculating mean and std
2023-07-11 09:07:23,500:INFO:Creating metrics dataframe
2023-07-11 09:07:24,849:INFO:Uploading results into container
2023-07-11 09:07:24,849:INFO:Uploading model into container now
2023-07-11 09:07:24,864:INFO:_master_model_container: 18
2023-07-11 09:07:24,864:INFO:_display_container: 2
2023-07-11 09:07:24,864:INFO:LGBMRegressor(random_state=992)
2023-07-11 09:07:24,864:INFO:create_model() successfully completed......................................
2023-07-11 09:07:25,114:INFO:SubProcess create_model() end ==================================
2023-07-11 09:07:25,114:INFO:Creating metrics dataframe
2023-07-11 09:07:25,114:INFO:Initializing Dummy Regressor
2023-07-11 09:07:25,114:INFO:Total runtime is 5.725823767979939 minutes
2023-07-11 09:07:25,114:INFO:SubProcess create_model() called ==================================
2023-07-11 09:07:25,114:INFO:Initializing create_model()
2023-07-11 09:07:25,114:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000014F84698940>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000014F86F29CA0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:07:25,114:INFO:Checking exceptions
2023-07-11 09:07:25,114:INFO:Importing libraries
2023-07-11 09:07:25,114:INFO:Copying training dataset
2023-07-11 09:07:25,208:INFO:Defining folds
2023-07-11 09:07:25,208:INFO:Declaring metric variables
2023-07-11 09:07:25,208:INFO:Importing untrained model
2023-07-11 09:07:25,208:INFO:Dummy Regressor Imported successfully
2023-07-11 09:07:25,208:INFO:Starting cross validation
2023-07-11 09:07:25,224:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:07:34,668:INFO:Calculating mean and std
2023-07-11 09:07:34,668:INFO:Creating metrics dataframe
2023-07-11 09:07:36,007:INFO:Uploading results into container
2023-07-11 09:07:36,007:INFO:Uploading model into container now
2023-07-11 09:07:36,007:INFO:_master_model_container: 19
2023-07-11 09:07:36,007:INFO:_display_container: 2
2023-07-11 09:07:36,007:INFO:DummyRegressor()
2023-07-11 09:07:36,007:INFO:create_model() successfully completed......................................
2023-07-11 09:07:36,261:INFO:SubProcess create_model() end ==================================
2023-07-11 09:07:36,261:INFO:Creating metrics dataframe
2023-07-11 09:07:36,276:INFO:Initializing create_model()
2023-07-11 09:07:36,282:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000014F84698940>, estimator=DecisionTreeRegressor(random_state=992), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:07:36,282:INFO:Checking exceptions
2023-07-11 09:07:36,282:INFO:Importing libraries
2023-07-11 09:07:36,282:INFO:Copying training dataset
2023-07-11 09:07:36,355:INFO:Defining folds
2023-07-11 09:07:36,355:INFO:Declaring metric variables
2023-07-11 09:07:36,355:INFO:Importing untrained model
2023-07-11 09:07:36,355:INFO:Declaring custom model
2023-07-11 09:07:36,355:INFO:Decision Tree Regressor Imported successfully
2023-07-11 09:07:36,355:INFO:Cross validation set to False
2023-07-11 09:07:36,355:INFO:Fitting Model
2023-07-11 09:07:40,588:INFO:DecisionTreeRegressor(random_state=992)
2023-07-11 09:07:40,588:INFO:create_model() successfully completed......................................
2023-07-11 09:07:40,879:INFO:_master_model_container: 19
2023-07-11 09:07:40,879:INFO:_display_container: 2
2023-07-11 09:07:40,879:INFO:DecisionTreeRegressor(random_state=992)
2023-07-11 09:07:40,879:INFO:compare_models() successfully completed......................................
2023-07-11 09:15:18,036:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-11 09:15:18,036:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-11 09:15:18,036:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-11 09:15:18,036:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-11 09:15:18,908:WARNING:
'prophet' is a soft dependency and not included in the pycaret installation. Please run: `pip install prophet` to install.
2023-07-11 09:29:37,779:WARNING:C:\Users\didit\anaconda3\lib\site-packages\numba\core\decorators.py:262: NumbaDeprecationWarning: [1mnumba.generated_jit is deprecated. Please see the documentation at: https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-generated-jit for more information and advice on a suitable replacement.[0m
  warnings.warn(msg, NumbaDeprecationWarning)

2023-07-11 09:29:37,828:WARNING:C:\Users\didit\anaconda3\lib\site-packages\visions\backends\shared\nan_handling.py:51: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def hasna(x: np.ndarray) -> bool:

2023-07-11 09:29:37,993:WARNING:C:\Users\didit\Downloads\AutoML\autoML\app.py:4: DeprecationWarning: `import pandas_profiling` is going to be deprecated by April 1st. Please use `import ydata_profiling` instead.
  from streamlit_pandas_profiling import st_profile_report

2023-07-11 09:30:23,187:INFO:PyCaret RegressionExperiment
2023-07-11 09:30:23,187:INFO:Logging name: reg-default-name
2023-07-11 09:30:23,187:INFO:ML Usecase: MLUsecase.REGRESSION
2023-07-11 09:30:23,187:INFO:version 3.0.2
2023-07-11 09:30:23,188:INFO:Initializing setup()
2023-07-11 09:30:23,188:INFO:self.USI: e63c
2023-07-11 09:30:23,188:INFO:self._variable_keys: {'_ml_usecase', 'log_plots_param', 'USI', 'X_train', 'pipeline', 'gpu_param', 'exp_name_log', 'y', 'html_param', 'X', 'data', 'exp_id', 'y_train', 'target_param', 'X_test', 'fold_groups_param', 'n_jobs_param', 'idx', 'fold_generator', 'logging_param', 'memory', 'gpu_n_jobs_param', 'fold_shuffle_param', '_available_plots', 'seed', 'y_test', 'transform_target_param'}
2023-07-11 09:30:23,188:INFO:Checking environment
2023-07-11 09:30:23,188:INFO:python_version: 3.9.13
2023-07-11 09:30:23,189:INFO:python_build: ('main', 'Oct 13 2022 21:23:06')
2023-07-11 09:30:23,189:INFO:machine: AMD64
2023-07-11 09:30:23,197:INFO:platform: Windows-10-10.0.22621-SP0
2023-07-11 09:30:23,197:INFO:Memory: svmem(total=16893358080, available=3850801152, percent=77.2, used=13042556928, free=3850801152)
2023-07-11 09:30:23,197:INFO:Physical Core: 8
2023-07-11 09:30:23,197:INFO:Logical Core: 16
2023-07-11 09:30:23,197:INFO:Checking libraries
2023-07-11 09:30:23,197:INFO:System:
2023-07-11 09:30:23,197:INFO:    python: 3.9.13 (main, Oct 13 2022, 21:23:06) [MSC v.1916 64 bit (AMD64)]
2023-07-11 09:30:23,197:INFO:executable: C:\Users\didit\anaconda3\python.exe
2023-07-11 09:30:23,197:INFO:   machine: Windows-10-10.0.22621-SP0
2023-07-11 09:30:23,197:INFO:PyCaret required dependencies:
2023-07-11 09:30:23,197:INFO:                 pip: 23.0.1
2023-07-11 09:30:23,197:INFO:          setuptools: 67.8.0
2023-07-11 09:30:23,197:INFO:             pycaret: 3.0.2
2023-07-11 09:30:23,197:INFO:             IPython: 8.12.0
2023-07-11 09:30:23,197:INFO:          ipywidgets: 8.0.4
2023-07-11 09:30:23,197:INFO:                tqdm: 4.65.0
2023-07-11 09:30:23,197:INFO:               numpy: 1.21.5
2023-07-11 09:30:23,197:INFO:              pandas: 1.5.3
2023-07-11 09:30:23,197:INFO:              jinja2: 3.1.2
2023-07-11 09:30:23,197:INFO:               scipy: 1.10.1
2023-07-11 09:30:23,197:INFO:              joblib: 1.2.0
2023-07-11 09:30:23,197:INFO:             sklearn: 1.2.2
2023-07-11 09:30:23,197:INFO:                pyod: 1.0.9
2023-07-11 09:30:23,197:INFO:            imblearn: 0.10.1
2023-07-11 09:30:23,197:INFO:   category_encoders: 2.6.1
2023-07-11 09:30:23,197:INFO:            lightgbm: 3.3.5
2023-07-11 09:30:23,197:INFO:               numba: 0.57.0
2023-07-11 09:30:23,197:INFO:            requests: 2.29.0
2023-07-11 09:30:23,197:INFO:          matplotlib: 3.7.1
2023-07-11 09:30:23,197:INFO:          scikitplot: 0.3.7
2023-07-11 09:30:23,197:INFO:         yellowbrick: 1.5
2023-07-11 09:30:23,197:INFO:              plotly: 5.9.0
2023-07-11 09:30:23,197:INFO:             kaleido: 0.2.1
2023-07-11 09:30:23,197:INFO:         statsmodels: 0.13.5
2023-07-11 09:30:23,197:INFO:              sktime: 0.17.0
2023-07-11 09:30:23,197:INFO:               tbats: 1.1.3
2023-07-11 09:30:23,197:INFO:            pmdarima: 2.0.3
2023-07-11 09:30:23,197:INFO:              psutil: 5.9.0
2023-07-11 09:30:23,197:INFO:PyCaret optional dependencies:
2023-07-11 09:30:23,375:INFO:                shap: 0.41.0
2023-07-11 09:30:23,375:INFO:           interpret: Not installed
2023-07-11 09:30:23,375:INFO:                umap: Not installed
2023-07-11 09:30:23,375:INFO:    pandas_profiling: 4.3.1
2023-07-11 09:30:23,375:INFO:  explainerdashboard: Not installed
2023-07-11 09:30:23,375:INFO:             autoviz: Not installed
2023-07-11 09:30:23,375:INFO:           fairlearn: Not installed
2023-07-11 09:30:23,375:INFO:             xgboost: 1.7.6
2023-07-11 09:30:23,375:INFO:            catboost: Not installed
2023-07-11 09:30:23,375:INFO:              kmodes: Not installed
2023-07-11 09:30:23,375:INFO:             mlxtend: Not installed
2023-07-11 09:30:23,375:INFO:       statsforecast: Not installed
2023-07-11 09:30:23,375:INFO:        tune_sklearn: Not installed
2023-07-11 09:30:23,375:INFO:                 ray: Not installed
2023-07-11 09:30:23,375:INFO:            hyperopt: Not installed
2023-07-11 09:30:23,375:INFO:              optuna: Not installed
2023-07-11 09:30:23,375:INFO:               skopt: Not installed
2023-07-11 09:30:23,375:INFO:              mlflow: 2.4.2
2023-07-11 09:30:23,375:INFO:              gradio: Not installed
2023-07-11 09:30:23,375:INFO:             fastapi: 0.95.2
2023-07-11 09:30:23,375:INFO:             uvicorn: 0.22.0
2023-07-11 09:30:23,375:INFO:              m2cgen: Not installed
2023-07-11 09:30:23,375:INFO:           evidently: Not installed
2023-07-11 09:30:23,375:INFO:               fugue: Not installed
2023-07-11 09:30:23,375:INFO:           streamlit: 1.23.1
2023-07-11 09:30:23,375:INFO:             prophet: Not installed
2023-07-11 09:30:23,375:INFO:None
2023-07-11 09:30:23,375:INFO:Set up data.
2023-07-11 09:30:23,375:INFO:Set up train/test split.
2023-07-11 09:30:23,391:INFO:Set up index.
2023-07-11 09:30:23,391:INFO:Set up folding strategy.
2023-07-11 09:30:23,391:INFO:Assigning column types.
2023-07-11 09:30:23,391:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-07-11 09:30:23,391:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-07-11 09:30:23,391:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-07-11 09:30:23,391:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 09:30:23,439:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 09:30:23,470:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 09:30:23,470:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:30:23,659:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:30:23,659:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-07-11 09:30:23,677:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-07-11 09:30:23,680:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 09:30:23,729:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 09:30:23,746:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 09:30:23,746:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:30:23,762:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:30:23,762:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-07-11 09:30:23,762:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-07-11 09:30:23,762:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 09:30:23,809:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 09:30:23,840:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 09:30:23,840:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:30:23,840:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:30:23,840:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-07-11 09:30:23,856:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 09:30:23,895:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 09:30:23,929:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 09:30:23,929:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:30:23,929:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:30:23,929:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-07-11 09:30:23,929:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 09:30:23,976:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 09:30:24,023:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 09:30:24,023:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:30:24,023:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:30:24,039:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 09:30:24,070:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 09:30:24,122:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 09:30:24,122:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:30:24,124:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:30:24,124:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-07-11 09:30:24,177:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 09:30:24,209:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 09:30:24,209:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:30:24,209:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:30:24,255:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 09:30:24,298:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 09:30:24,298:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:30:24,300:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:30:24,300:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-07-11 09:30:24,347:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 09:30:24,379:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:30:24,379:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:30:24,426:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 09:30:24,457:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:30:24,457:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:30:24,457:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-07-11 09:30:24,547:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:30:24,547:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:30:24,626:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:30:24,626:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:30:24,626:INFO:Preparing preprocessing pipeline...
2023-07-11 09:30:24,626:INFO:Set up simple imputation.
2023-07-11 09:30:24,642:INFO:Set up encoding of categorical features.
2023-07-11 09:30:32,944:INFO:PyCaret RegressionExperiment
2023-07-11 09:30:32,944:INFO:Logging name: reg-default-name
2023-07-11 09:30:32,944:INFO:ML Usecase: MLUsecase.REGRESSION
2023-07-11 09:30:32,944:INFO:version 3.0.2
2023-07-11 09:30:32,944:INFO:Initializing setup()
2023-07-11 09:30:32,944:INFO:self.USI: 28d7
2023-07-11 09:30:32,944:INFO:self._variable_keys: {'_ml_usecase', 'log_plots_param', 'USI', 'X_train', 'pipeline', 'gpu_param', 'exp_name_log', 'y', 'html_param', 'X', 'data', 'exp_id', 'y_train', 'target_param', 'X_test', 'fold_groups_param', 'n_jobs_param', 'idx', 'fold_generator', 'logging_param', 'memory', 'gpu_n_jobs_param', 'fold_shuffle_param', '_available_plots', 'seed', 'y_test', 'transform_target_param'}
2023-07-11 09:30:32,944:INFO:Checking environment
2023-07-11 09:30:32,944:INFO:python_version: 3.9.13
2023-07-11 09:30:32,944:INFO:python_build: ('main', 'Oct 13 2022 21:23:06')
2023-07-11 09:30:32,944:INFO:machine: AMD64
2023-07-11 09:30:32,944:INFO:platform: Windows-10-10.0.22621-SP0
2023-07-11 09:30:32,944:INFO:Memory: svmem(total=16893358080, available=3892662272, percent=77.0, used=13000695808, free=3892662272)
2023-07-11 09:30:32,944:INFO:Physical Core: 8
2023-07-11 09:30:32,944:INFO:Logical Core: 16
2023-07-11 09:30:32,944:INFO:Checking libraries
2023-07-11 09:30:32,944:INFO:System:
2023-07-11 09:30:32,944:INFO:    python: 3.9.13 (main, Oct 13 2022, 21:23:06) [MSC v.1916 64 bit (AMD64)]
2023-07-11 09:30:32,944:INFO:executable: C:\Users\didit\anaconda3\python.exe
2023-07-11 09:30:32,944:INFO:   machine: Windows-10-10.0.22621-SP0
2023-07-11 09:30:32,944:INFO:PyCaret required dependencies:
2023-07-11 09:30:32,944:INFO:                 pip: 23.0.1
2023-07-11 09:30:32,944:INFO:          setuptools: 67.8.0
2023-07-11 09:30:32,944:INFO:             pycaret: 3.0.2
2023-07-11 09:30:32,944:INFO:             IPython: 8.12.0
2023-07-11 09:30:32,944:INFO:          ipywidgets: 8.0.4
2023-07-11 09:30:32,944:INFO:                tqdm: 4.65.0
2023-07-11 09:30:32,944:INFO:               numpy: 1.21.5
2023-07-11 09:30:32,944:INFO:              pandas: 1.5.3
2023-07-11 09:30:32,944:INFO:              jinja2: 3.1.2
2023-07-11 09:30:32,944:INFO:               scipy: 1.10.1
2023-07-11 09:30:32,944:INFO:              joblib: 1.2.0
2023-07-11 09:30:32,944:INFO:             sklearn: 1.2.2
2023-07-11 09:30:32,944:INFO:                pyod: 1.0.9
2023-07-11 09:30:32,944:INFO:            imblearn: 0.10.1
2023-07-11 09:30:32,944:INFO:   category_encoders: 2.6.1
2023-07-11 09:30:32,944:INFO:            lightgbm: 3.3.5
2023-07-11 09:30:32,944:INFO:               numba: 0.57.0
2023-07-11 09:30:32,944:INFO:            requests: 2.29.0
2023-07-11 09:30:32,944:INFO:          matplotlib: 3.7.1
2023-07-11 09:30:32,944:INFO:          scikitplot: 0.3.7
2023-07-11 09:30:32,944:INFO:         yellowbrick: 1.5
2023-07-11 09:30:32,944:INFO:              plotly: 5.9.0
2023-07-11 09:30:32,944:INFO:             kaleido: 0.2.1
2023-07-11 09:30:32,944:INFO:         statsmodels: 0.13.5
2023-07-11 09:30:32,944:INFO:              sktime: 0.17.0
2023-07-11 09:30:32,944:INFO:               tbats: 1.1.3
2023-07-11 09:30:32,944:INFO:            pmdarima: 2.0.3
2023-07-11 09:30:32,944:INFO:              psutil: 5.9.0
2023-07-11 09:30:32,944:INFO:PyCaret optional dependencies:
2023-07-11 09:30:32,944:INFO:                shap: 0.41.0
2023-07-11 09:30:32,944:INFO:           interpret: Not installed
2023-07-11 09:30:32,944:INFO:                umap: Not installed
2023-07-11 09:30:32,944:INFO:    pandas_profiling: 4.3.1
2023-07-11 09:30:32,944:INFO:  explainerdashboard: Not installed
2023-07-11 09:30:32,944:INFO:             autoviz: Not installed
2023-07-11 09:30:32,944:INFO:           fairlearn: Not installed
2023-07-11 09:30:32,944:INFO:             xgboost: 1.7.6
2023-07-11 09:30:32,944:INFO:            catboost: Not installed
2023-07-11 09:30:32,944:INFO:              kmodes: Not installed
2023-07-11 09:30:32,944:INFO:             mlxtend: Not installed
2023-07-11 09:30:32,944:INFO:       statsforecast: Not installed
2023-07-11 09:30:32,944:INFO:        tune_sklearn: Not installed
2023-07-11 09:30:32,944:INFO:                 ray: Not installed
2023-07-11 09:30:32,944:INFO:            hyperopt: Not installed
2023-07-11 09:30:32,944:INFO:              optuna: Not installed
2023-07-11 09:30:32,944:INFO:               skopt: Not installed
2023-07-11 09:30:32,944:INFO:              mlflow: 2.4.2
2023-07-11 09:30:32,944:INFO:              gradio: Not installed
2023-07-11 09:30:32,944:INFO:             fastapi: 0.95.2
2023-07-11 09:30:32,944:INFO:             uvicorn: 0.22.0
2023-07-11 09:30:32,944:INFO:              m2cgen: Not installed
2023-07-11 09:30:32,944:INFO:           evidently: Not installed
2023-07-11 09:30:32,944:INFO:               fugue: Not installed
2023-07-11 09:30:32,944:INFO:           streamlit: 1.23.1
2023-07-11 09:30:32,944:INFO:             prophet: Not installed
2023-07-11 09:30:32,944:INFO:None
2023-07-11 09:30:32,944:INFO:Set up data.
2023-07-11 09:30:32,960:INFO:Set up train/test split.
2023-07-11 09:30:32,960:INFO:Set up index.
2023-07-11 09:30:32,960:INFO:Set up folding strategy.
2023-07-11 09:30:32,960:INFO:Assigning column types.
2023-07-11 09:30:32,960:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-07-11 09:30:32,960:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-07-11 09:30:32,960:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-07-11 09:30:32,975:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 09:30:33,007:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 09:30:33,054:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 09:30:33,054:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:30:33,054:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:30:33,054:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-07-11 09:30:33,054:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-07-11 09:30:33,054:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 09:30:33,105:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 09:30:33,130:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 09:30:33,130:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:30:33,130:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:30:33,130:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-07-11 09:30:33,130:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-07-11 09:30:33,145:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 09:30:33,177:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 09:30:33,208:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 09:30:33,208:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:30:33,208:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:30:33,223:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-07-11 09:30:33,223:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 09:30:33,255:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 09:30:33,306:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 09:30:33,306:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:30:33,308:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:30:33,308:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-07-11 09:30:33,314:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 09:30:33,345:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 09:30:33,377:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 09:30:33,377:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:30:33,377:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:30:33,392:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 09:30:33,440:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 09:30:33,471:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 09:30:33,471:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:30:33,471:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:30:33,471:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-07-11 09:30:33,527:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 09:30:33,547:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 09:30:33,547:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:30:33,547:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:30:33,594:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 09:30:33,626:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 09:30:33,626:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:30:33,626:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:30:33,626:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-07-11 09:30:33,691:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 09:30:33,734:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:30:33,735:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:30:33,779:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 09:30:33,811:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:30:33,811:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:30:33,811:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-07-11 09:30:33,897:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:30:33,897:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:30:33,980:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:30:33,980:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:30:33,980:INFO:Preparing preprocessing pipeline...
2023-07-11 09:30:33,980:INFO:Set up simple imputation.
2023-07-11 09:30:33,995:INFO:Set up encoding of categorical features.
2023-07-11 09:30:34,091:INFO:Finished creating preprocessing pipeline.
2023-07-11 09:30:34,109:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\didit\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['salary', 'salary_in_usd',
                                             'remote_ratio'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['experience_level',
                                             'employment_type', 'job_title',
                                             'salary_currency',
                                             'employee_residence',
                                             'compa...
                                    transformer=OneHotEncoder(cols=['experience_level',
                                                                    'employment_type',
                                                                    'salary_currency',
                                                                    'company_size'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('rest_encoding',
                 TransformerWrapper(include=['job_title', 'employee_residence',
                                             'company_location'],
                                    transformer=TargetEncoder(cols=['job_title',
                                                                    'employee_residence',
                                                                    'company_location'],
                                                              handle_missing='return_nan')))])
2023-07-11 09:30:34,109:INFO:Creating final display dataframe.
2023-07-11 09:30:34,410:INFO:Setup _display_container:                     Description             Value
0                    Session id              6256
1                        Target         work_year
2                   Target type        Regression
3           Original data shape        (3755, 11)
4        Transformed data shape       (3755, 178)
5   Transformed train set shape       (2628, 178)
6    Transformed test set shape       (1127, 178)
7              Numeric features                 3
8          Categorical features                 7
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13     Maximum one-hot encoding                25
14              Encoding method              None
15               Fold Generator             KFold
16                  Fold Number                10
17                     CPU Jobs                -1
18                      Use GPU             False
19               Log Experiment             False
20              Experiment Name  reg-default-name
21                          USI              28d7
2023-07-11 09:30:34,511:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:30:34,513:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:30:34,592:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:30:34,592:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:30:34,592:INFO:setup() successfully completed in 1.88s...............
2023-07-11 09:30:34,613:INFO:Initializing compare_models()
2023-07-11 09:30:34,615:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D78BA0A0>, include=None, fold=None, round=4, cross_validation=True, sort=R2, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x000001E1D78BA0A0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2023-07-11 09:30:34,615:INFO:Checking exceptions
2023-07-11 09:30:34,616:INFO:Preparing display monitor
2023-07-11 09:30:34,618:INFO:Initializing Linear Regression
2023-07-11 09:30:34,618:INFO:Total runtime is 0.0 minutes
2023-07-11 09:30:34,618:INFO:SubProcess create_model() called ==================================
2023-07-11 09:30:34,618:INFO:Initializing create_model()
2023-07-11 09:30:34,618:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D78BA0A0>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D6CF9190>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:30:34,618:INFO:Checking exceptions
2023-07-11 09:30:34,618:INFO:Importing libraries
2023-07-11 09:30:34,618:INFO:Copying training dataset
2023-07-11 09:30:34,620:INFO:Defining folds
2023-07-11 09:30:34,620:INFO:Declaring metric variables
2023-07-11 09:30:34,621:INFO:Importing untrained model
2023-07-11 09:30:34,621:INFO:Linear Regression Imported successfully
2023-07-11 09:30:34,621:INFO:Starting cross validation
2023-07-11 09:30:34,630:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:30:41,579:INFO:Calculating mean and std
2023-07-11 09:30:41,579:INFO:Creating metrics dataframe
2023-07-11 09:30:42,131:INFO:Uploading results into container
2023-07-11 09:30:42,131:INFO:Uploading model into container now
2023-07-11 09:30:42,131:INFO:_master_model_container: 1
2023-07-11 09:30:42,131:INFO:_display_container: 2
2023-07-11 09:30:42,131:INFO:LinearRegression(n_jobs=-1)
2023-07-11 09:30:42,131:INFO:create_model() successfully completed......................................
2023-07-11 09:30:42,257:INFO:SubProcess create_model() end ==================================
2023-07-11 09:30:42,257:INFO:Creating metrics dataframe
2023-07-11 09:30:42,257:INFO:Initializing Lasso Regression
2023-07-11 09:30:42,257:INFO:Total runtime is 0.12731906970342 minutes
2023-07-11 09:30:42,257:INFO:SubProcess create_model() called ==================================
2023-07-11 09:30:42,257:INFO:Initializing create_model()
2023-07-11 09:30:42,257:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D78BA0A0>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D6CF9190>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:30:42,257:INFO:Checking exceptions
2023-07-11 09:30:42,257:INFO:Importing libraries
2023-07-11 09:30:42,257:INFO:Copying training dataset
2023-07-11 09:30:42,272:INFO:Defining folds
2023-07-11 09:30:42,272:INFO:Declaring metric variables
2023-07-11 09:30:42,272:INFO:Importing untrained model
2023-07-11 09:30:42,272:INFO:Lasso Regression Imported successfully
2023-07-11 09:30:42,272:INFO:Starting cross validation
2023-07-11 09:30:42,272:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:30:46,761:INFO:Calculating mean and std
2023-07-11 09:30:46,761:INFO:Creating metrics dataframe
2023-07-11 09:30:47,346:INFO:Uploading results into container
2023-07-11 09:30:47,346:INFO:Uploading model into container now
2023-07-11 09:30:47,346:INFO:_master_model_container: 2
2023-07-11 09:30:47,346:INFO:_display_container: 2
2023-07-11 09:30:47,347:INFO:Lasso(random_state=6256)
2023-07-11 09:30:47,347:INFO:create_model() successfully completed......................................
2023-07-11 09:30:47,458:INFO:SubProcess create_model() end ==================================
2023-07-11 09:30:47,458:INFO:Creating metrics dataframe
2023-07-11 09:30:47,458:INFO:Initializing Ridge Regression
2023-07-11 09:30:47,458:INFO:Total runtime is 0.2140097737312317 minutes
2023-07-11 09:30:47,458:INFO:SubProcess create_model() called ==================================
2023-07-11 09:30:47,458:INFO:Initializing create_model()
2023-07-11 09:30:47,458:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D78BA0A0>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D6CF9190>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:30:47,458:INFO:Checking exceptions
2023-07-11 09:30:47,458:INFO:Importing libraries
2023-07-11 09:30:47,458:INFO:Copying training dataset
2023-07-11 09:30:47,458:INFO:Defining folds
2023-07-11 09:30:47,458:INFO:Declaring metric variables
2023-07-11 09:30:47,458:INFO:Importing untrained model
2023-07-11 09:30:47,458:INFO:Ridge Regression Imported successfully
2023-07-11 09:30:47,458:INFO:Starting cross validation
2023-07-11 09:30:47,458:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:30:50,274:INFO:Calculating mean and std
2023-07-11 09:30:50,274:INFO:Creating metrics dataframe
2023-07-11 09:30:50,732:INFO:Uploading results into container
2023-07-11 09:30:50,747:INFO:Uploading model into container now
2023-07-11 09:30:50,747:INFO:_master_model_container: 3
2023-07-11 09:30:50,747:INFO:_display_container: 2
2023-07-11 09:30:50,747:INFO:Ridge(random_state=6256)
2023-07-11 09:30:50,747:INFO:create_model() successfully completed......................................
2023-07-11 09:30:50,873:INFO:SubProcess create_model() end ==================================
2023-07-11 09:30:50,873:INFO:Creating metrics dataframe
2023-07-11 09:30:50,883:INFO:Initializing Elastic Net
2023-07-11 09:30:50,883:INFO:Total runtime is 0.27109018961588544 minutes
2023-07-11 09:30:50,883:INFO:SubProcess create_model() called ==================================
2023-07-11 09:30:50,884:INFO:Initializing create_model()
2023-07-11 09:30:50,884:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D78BA0A0>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D6CF9190>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:30:50,884:INFO:Checking exceptions
2023-07-11 09:30:50,884:INFO:Importing libraries
2023-07-11 09:30:50,884:INFO:Copying training dataset
2023-07-11 09:30:50,888:INFO:Defining folds
2023-07-11 09:30:50,888:INFO:Declaring metric variables
2023-07-11 09:30:50,888:INFO:Importing untrained model
2023-07-11 09:30:50,889:INFO:Elastic Net Imported successfully
2023-07-11 09:30:50,889:INFO:Starting cross validation
2023-07-11 09:30:50,890:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:30:53,658:INFO:Calculating mean and std
2023-07-11 09:30:53,658:INFO:Creating metrics dataframe
2023-07-11 09:30:54,194:INFO:Uploading results into container
2023-07-11 09:30:54,194:INFO:Uploading model into container now
2023-07-11 09:30:54,194:INFO:_master_model_container: 4
2023-07-11 09:30:54,194:INFO:_display_container: 2
2023-07-11 09:30:54,194:INFO:ElasticNet(random_state=6256)
2023-07-11 09:30:54,194:INFO:create_model() successfully completed......................................
2023-07-11 09:30:54,314:INFO:SubProcess create_model() end ==================================
2023-07-11 09:30:54,314:INFO:Creating metrics dataframe
2023-07-11 09:30:54,330:INFO:Initializing Least Angle Regression
2023-07-11 09:30:54,330:INFO:Total runtime is 0.3285360376040141 minutes
2023-07-11 09:30:54,330:INFO:SubProcess create_model() called ==================================
2023-07-11 09:30:54,330:INFO:Initializing create_model()
2023-07-11 09:30:54,330:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D78BA0A0>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D6CF9190>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:30:54,330:INFO:Checking exceptions
2023-07-11 09:30:54,330:INFO:Importing libraries
2023-07-11 09:30:54,330:INFO:Copying training dataset
2023-07-11 09:30:54,330:INFO:Defining folds
2023-07-11 09:30:54,330:INFO:Declaring metric variables
2023-07-11 09:30:54,330:INFO:Importing untrained model
2023-07-11 09:30:54,330:INFO:Least Angle Regression Imported successfully
2023-07-11 09:30:54,330:INFO:Starting cross validation
2023-07-11 09:30:54,339:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:30:54,576:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=4.635e-02, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,576:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=2.317e-02, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,591:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=3.861e-02, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,591:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=3.560e-02, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,591:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=4.549e-02, with an active set of 45 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,591:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 56 iterations, i.e. alpha=4.534e-02, with an active set of 47 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,591:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 64 iterations, i.e. alpha=4.559e-02, with an active set of 53 regressors, and the smallest cholesky pivot element being 2.788e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,591:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=4.559e-02, with an active set of 57 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,591:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=2.176e-02, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,591:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=2.220e-02, with an active set of 45 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,591:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.212e-03, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,591:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 55 iterations, i.e. alpha=2.219e-02, with an active set of 46 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,607:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 55 iterations, i.e. alpha=2.218e-02, with an active set of 46 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,607:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=8.184e-04, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,607:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=8.184e-04, with an active set of 17 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,607:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 59 iterations, i.e. alpha=2.218e-02, with an active set of 49 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,607:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=6.752e-04, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,607:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 64 iterations, i.e. alpha=2.221e-02, with an active set of 54 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,607:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 64 iterations, i.e. alpha=2.220e-02, with an active set of 54 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,607:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=6.894e-04, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,607:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=4.860e-04, with an active set of 23 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,607:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 69 iterations, i.e. alpha=2.212e-02, with an active set of 58 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,607:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=4.860e-04, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,607:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 72 iterations, i.e. alpha=2.212e-02, with an active set of 60 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,607:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=4.860e-04, with an active set of 23 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,607:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 73 iterations, i.e. alpha=2.212e-02, with an active set of 61 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,607:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=4.366e-04, with an active set of 25 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,607:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=4.064e-04, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,607:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=4.064e-04, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,607:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=4.064e-04, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,607:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=4.064e-04, with an active set of 26 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,607:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=4.064e-04, with an active set of 26 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,607:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=3.021e-04, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,607:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=2.498e-04, with an active set of 29 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,607:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=2.494e-04, with an active set of 34 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,607:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=2.252e-04, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,607:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=2.250e-04, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,607:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.158e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,607:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=1.907e-04, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,623:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=1.900e-04, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,623:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=1.884e-04, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,623:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=6.951e-03, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,623:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=1.883e-04, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,623:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=5.687e-03, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,623:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=1.883e-04, with an active set of 35 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,623:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=1.882e-04, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,623:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=4.436e-03, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,623:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=1.748e-04, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,623:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=1.676e-04, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,623:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=3.670e-03, with an active set of 18 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,623:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=1.545e-04, with an active set of 38 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,623:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=1.542e-04, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,623:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=2.924e-03, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,623:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=2.645e-03, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,623:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=2.640e-03, with an active set of 20 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,623:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=2.474e-03, with an active set of 20 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,623:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=1.613e-04, with an active set of 45 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,623:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=3.672e-03, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,623:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=1.613e-04, with an active set of 45 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,623:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=9.926e-03, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,623:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 55 iterations, i.e. alpha=1.457e-04, with an active set of 48 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,623:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=1.468e-02, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,623:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=1.468e-02, with an active set of 30 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,623:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 64 iterations, i.e. alpha=4.926e-04, with an active set of 57 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,623:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=1.454e-02, with an active set of 31 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,623:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 64 iterations, i.e. alpha=4.669e-04, with an active set of 57 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,623:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=1.451e-02, with an active set of 32 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,623:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=1.438e-02, with an active set of 32 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,623:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=7.848e-03, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,623:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=4.405e-03, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,623:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=9.446e-03, with an active set of 34 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,623:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=3.746e-03, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,623:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=9.436e-03, with an active set of 34 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,623:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=2.335e-03, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,623:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=1.866e-03, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,623:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=8.743e-03, with an active set of 35 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,623:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=1.284e-03, with an active set of 18 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,623:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=1.060e-03, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,638:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=8.621e-03, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,638:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=1.060e-03, with an active set of 20 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,638:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=8.612e-03, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,638:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=8.599e-03, with an active set of 38 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,638:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=9.330e-04, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,638:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=8.586e-03, with an active set of 38 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,638:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=8.279e-04, with an active set of 24 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,638:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=8.279e-04, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,640:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=8.429e-03, with an active set of 38 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,640:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=8.429e-03, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,640:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=7.991e-04, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,640:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=8.259e-03, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,640:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=8.173e-03, with an active set of 38 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,640:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=7.409e-04, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,640:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=7.848e-03, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,640:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=7.408e-04, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,640:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=7.642e-03, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,640:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=7.394e-04, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,640:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=7.355e-04, with an active set of 27 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,640:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=6.465e-03, with an active set of 40 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,640:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=6.456e-03, with an active set of 40 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,640:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=7.088e-04, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,640:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=6.449e-03, with an active set of 40 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,640:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=6.157e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,640:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=6.157e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,640:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=6.049e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,640:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=6.197e-03, with an active set of 42 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,640:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=4.896e-04, with an active set of 32 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,640:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=4.896e-04, with an active set of 32 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,640:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=6.196e-03, with an active set of 42 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,640:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=4.408e-04, with an active set of 32 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,640:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=4.152e-04, with an active set of 32 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,640:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=4.066e-04, with an active set of 32 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,640:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=6.056e-03, with an active set of 43 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,640:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=4.063e-04, with an active set of 33 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,640:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=4.051e-04, with an active set of 33 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,640:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=4.023e-04, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,640:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=4.022e-04, with an active set of 35 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,640:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=6.072e-03, with an active set of 45 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,640:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=4.006e-04, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,640:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=3.851e-04, with an active set of 35 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,640:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=3.728e-04, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,640:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=2.800e-04, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,640:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=2.799e-04, with an active set of 37 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,640:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=2.684e-04, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,640:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=2.594e-04, with an active set of 40 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,640:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=2.593e-04, with an active set of 40 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,640:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=2.416e-04, with an active set of 40 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,640:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 63 iterations, i.e. alpha=7.959e+00, with an active set of 54 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,640:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=2.197e-04, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,640:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=2.194e-04, with an active set of 42 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,640:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=2.157e-04, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,640:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=2.070e-04, with an active set of 42 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,640:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 51 iterations, i.e. alpha=2.153e-04, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,640:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=2.617e-03, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,654:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=2.617e-03, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,654:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=7.942e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,654:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=4.670e-03, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,654:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.229e-03, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,654:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=3.812e-03, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,654:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=2.581e-03, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,654:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=1.657e-03, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,654:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=9.476e-04, with an active set of 16 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,654:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.829e-03, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,654:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.486e-03, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,654:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=7.056e-04, with an active set of 18 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,654:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=1.271e-03, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,654:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=1.001e-03, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,654:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=6.428e-04, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,654:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=8.907e-04, with an active set of 20 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,654:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=5.746e-04, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,654:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=8.905e-04, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,654:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=5.746e-04, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,654:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=8.641e-04, with an active set of 20 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,654:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=5.715e-04, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,654:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=8.494e-04, with an active set of 20 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,654:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=8.494e-04, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,654:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=5.708e-04, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,654:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=8.462e-04, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,654:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=5.689e-04, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,654:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=5.689e-04, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,654:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=6.589e-04, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,654:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=4.880e-04, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,654:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=6.588e-04, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,654:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=4.816e-04, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,654:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=6.478e-04, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,654:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=4.739e-04, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,654:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=6.478e-04, with an active set of 25 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,654:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=6.258e-04, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,654:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=4.677e-04, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,654:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=6.222e-04, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,654:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=4.152e-04, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,654:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=5.951e-04, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,654:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=4.150e-04, with an active set of 28 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,654:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=5.949e-04, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,654:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=3.869e-04, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,654:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=4.997e-04, with an active set of 29 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,670:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=4.861e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,670:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=3.135e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,670:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=4.861e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,670:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=3.023e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,670:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=3.005e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,670:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=4.556e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,670:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=2.982e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,672:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=4.309e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,672:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=2.981e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,672:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=2.538e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,672:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=2.230e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,672:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=4.309e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,672:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.951e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,672:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=4.023e-04, with an active set of 32 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,672:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.799e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,672:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.791e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,672:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=4.020e-04, with an active set of 32 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,673:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.674e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,673:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=3.730e-04, with an active set of 32 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,673:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.980e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,673:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.662e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,674:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=3.755e-04, with an active set of 33 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,674:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=3.637e-04, with an active set of 34 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,674:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.043e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,674:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=1.516e-04, with an active set of 32 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,674:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=9.372e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,675:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=3.206e-04, with an active set of 34 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,675:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=2.997e-04, with an active set of 34 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,675:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=1.429e-04, with an active set of 33 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,676:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=4.847e-03, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,676:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=5.395e-04, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,676:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=4.362e-03, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,676:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=1.410e-04, with an active set of 34 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,676:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=1.410e-04, with an active set of 34 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,676:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=3.362e-04, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,677:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=5.589e-03, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,677:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=3.140e-04, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,677:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=3.012e-04, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,677:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=1.227e-04, with an active set of 34 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,678:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=1.203e-04, with an active set of 34 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,678:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=2.259e-03, with an active set of 15 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,678:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=2.749e-04, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,678:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=1.200e-04, with an active set of 34 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,678:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=1.200e-04, with an active set of 34 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,679:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=2.181e-03, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,679:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=9.219e-04, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=1.198e-04, with an active set of 34 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=2.576e-04, with an active set of 39 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=2.575e-04, with an active set of 39 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=1.197e-04, with an active set of 34 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=8.675e-04, with an active set of 14 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=2.445e-04, with an active set of 39 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=8.675e-04, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=2.283e-04, with an active set of 39 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=1.193e-04, with an active set of 35 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=2.263e-04, with an active set of 39 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=1.126e-04, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=2.262e-04, with an active set of 39 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=2.261e-04, with an active set of 39 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.881e-02, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=8.673e-05, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.873e-02, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=8.344e-05, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.854e-02, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=7.628e-04, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=2.096e-04, with an active set of 39 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=8.331e-05, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.785e-02, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=2.056e-04, with an active set of 39 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=8.148e-05, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=7.097e-04, with an active set of 19 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=7.985e-05, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=1.143e-02, with an active set of 23 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=7.985e-05, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=1.114e-02, with an active set of 23 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=7.243e-05, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=5.743e-04, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=5.743e-04, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=5.630e-05, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=1.531e-04, with an active set of 40 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=5.109e-05, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=9.774e-03, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=1.527e-04, with an active set of 40 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=4.697e-05, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=1.510e-04, with an active set of 40 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=5.617e-04, with an active set of 22 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=1.310e-04, with an active set of 40 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=5.617e-04, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=4.234e-05, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=1.309e-04, with an active set of 40 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=1.367e-02, with an active set of 29 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=1.280e-04, with an active set of 41 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=4.140e-05, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=5.456e-04, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=4.078e-05, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=5.422e-04, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=3.360e-02, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=5.422e-04, with an active set of 22 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=4.041e-05, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=4.032e-05, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=4.015e-05, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=1.343e-02, with an active set of 32 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 53 iterations, i.e. alpha=1.237e-04, with an active set of 43 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=3.986e-05, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 53 iterations, i.e. alpha=1.236e-04, with an active set of 43 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=3.914e-05, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=5.336e-04, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=3.808e-05, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 53 iterations, i.e. alpha=1.227e-04, with an active set of 43 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=3.464e-05, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.296e-02, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=2.572e-05, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=2.562e-05, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=4.871e-04, with an active set of 24 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=2.328e-05, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 53 iterations, i.e. alpha=1.106e-04, with an active set of 43 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=1.645e-05, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=1.591e-05, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.295e-02, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=1.544e-05, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 53 iterations, i.e. alpha=9.981e-05, with an active set of 43 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=1.467e-05, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.289e-02, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 53 iterations, i.e. alpha=8.668e-05, with an active set of 43 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=1.466e-05, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 53 iterations, i.e. alpha=8.541e-05, with an active set of 43 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=1.125e-02, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 53 iterations, i.e. alpha=8.120e-05, with an active set of 43 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=1.460e-05, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 53 iterations, i.e. alpha=8.097e-05, with an active set of 43 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 53 iterations, i.e. alpha=7.514e-05, with an active set of 43 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=1.072e-02, with an active set of 36 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=1.452e-05, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 53 iterations, i.e. alpha=6.844e-05, with an active set of 43 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 53 iterations, i.e. alpha=6.513e-05, with an active set of 43 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=1.216e-05, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=8.794e-06, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=8.663e-06, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=5.712e-04, with an active set of 32 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=1.033e-02, with an active set of 39 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=5.804e-05, with an active set of 44 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=6.787e-06, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=5.709e-04, with an active set of 32 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=5.479e-05, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=5.178e-05, with an active set of 44 regressors, and the smallest cholesky pivot element being 7.300e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=5.482e-04, with an active set of 33 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=1.007e-02, with an active set of 39 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=5.481e-04, with an active set of 33 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 58 iterations, i.e. alpha=2.024e-04, with an active set of 48 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=5.296e-04, with an active set of 33 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=9.852e-03, with an active set of 41 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=5.045e-04, with an active set of 33 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=5.022e-04, with an active set of 33 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=9.789e-03, with an active set of 41 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=9.735e-03, with an active set of 41 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 59 iterations, i.e. alpha=1.757e-04, with an active set of 49 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=4.843e-04, with an active set of 34 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,696:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 59 iterations, i.e. alpha=1.731e-04, with an active set of 49 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,696:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 51 iterations, i.e. alpha=9.028e-03, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,696:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 51 iterations, i.e. alpha=9.006e-03, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,696:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 51 iterations, i.e. alpha=9.004e-03, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,696:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=3.934e-04, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,696:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 51 iterations, i.e. alpha=8.558e-03, with an active set of 42 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,697:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=3.907e-04, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,697:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=8.474e-03, with an active set of 43 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,698:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=4.097e-04, with an active set of 39 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,698:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.212e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,698:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=8.473e-03, with an active set of 45 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,698:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=4.086e-04, with an active set of 39 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,699:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=8.468e-03, with an active set of 45 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,699:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=3.580e-04, with an active set of 39 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,699:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=3.564e-04, with an active set of 39 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,699:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=8.398e-03, with an active set of 45 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,699:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=3.542e-04, with an active set of 39 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,699:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=6.853e-03, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,700:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=5.269e-03, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,700:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 56 iterations, i.e. alpha=8.247e-03, with an active set of 46 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,700:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=3.482e-04, with an active set of 39 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,700:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 56 iterations, i.e. alpha=8.241e-03, with an active set of 46 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,701:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=2.603e-03, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,701:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=2.523e-03, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,702:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=4.367e-03, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,702:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=2.114e-03, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,703:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=1.923e-03, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,703:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=2.082e+01, with an active set of 55 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,703:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 61 iterations, i.e. alpha=4.397e-04, with an active set of 51 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,704:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 61 iterations, i.e. alpha=3.576e-04, with an active set of 51 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,704:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 69 iterations, i.e. alpha=1.922e+01, with an active set of 56 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,704:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 69 iterations, i.e. alpha=1.848e+01, with an active set of 56 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,705:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 70 iterations, i.e. alpha=1.251e+01, with an active set of 57 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,705:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 70 iterations, i.e. alpha=1.218e+01, with an active set of 57 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,706:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 65 iterations, i.e. alpha=3.243e-04, with an active set of 55 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,706:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=2.079e-03, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,706:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 65 iterations, i.e. alpha=3.174e-04, with an active set of 55 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,706:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 65 iterations, i.e. alpha=3.167e-04, with an active set of 55 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,706:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=1.936e-03, with an active set of 26 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,707:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=1.683e-03, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,707:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 76 iterations, i.e. alpha=2.534e+01, with an active set of 63 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,707:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=1.655e-03, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,707:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 76 iterations, i.e. alpha=2.500e+01, with an active set of 63 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,708:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=1.649e-03, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,709:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.400e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,710:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.398e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,710:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 84 iterations, i.e. alpha=1.258e+01, with an active set of 71 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,710:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 84 iterations, i.e. alpha=1.060e+01, with an active set of 71 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,711:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=1.635e-03, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,711:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 86 iterations, i.e. alpha=1.056e+01, with an active set of 73 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,714:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 94 iterations, i.e. alpha=5.520e+00, with an active set of 81 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,714:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 51 iterations, i.e. alpha=1.492e-03, with an active set of 46 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,715:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 53 iterations, i.e. alpha=1.492e-03, with an active set of 48 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,715:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 98 iterations, i.e. alpha=5.521e+00, with an active set of 85 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,715:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 53 iterations, i.e. alpha=1.486e-03, with an active set of 48 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,716:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 100 iterations, i.e. alpha=5.230e+00, with an active set of 87 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,717:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 60 iterations, i.e. alpha=2.288e-03, with an active set of 55 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:54,720:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 70 iterations, i.e. alpha=1.405e+00, with an active set of 64 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:30:57,178:INFO:Calculating mean and std
2023-07-11 09:30:57,194:WARNING:C:\Users\didit\anaconda3\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)

2023-07-11 09:30:57,194:INFO:Creating metrics dataframe
2023-07-11 09:30:57,679:INFO:Uploading results into container
2023-07-11 09:30:57,680:INFO:Uploading model into container now
2023-07-11 09:30:57,680:INFO:_master_model_container: 5
2023-07-11 09:30:57,680:INFO:_display_container: 2
2023-07-11 09:30:57,680:INFO:Lars(random_state=6256)
2023-07-11 09:30:57,680:INFO:create_model() successfully completed......................................
2023-07-11 09:30:57,792:INFO:SubProcess create_model() end ==================================
2023-07-11 09:30:57,792:INFO:Creating metrics dataframe
2023-07-11 09:30:57,808:INFO:Initializing Lasso Least Angle Regression
2023-07-11 09:30:57,808:INFO:Total runtime is 0.38650699853897097 minutes
2023-07-11 09:30:57,808:INFO:SubProcess create_model() called ==================================
2023-07-11 09:30:57,808:INFO:Initializing create_model()
2023-07-11 09:30:57,808:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D78BA0A0>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D6CF9190>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:30:57,808:INFO:Checking exceptions
2023-07-11 09:30:57,808:INFO:Importing libraries
2023-07-11 09:30:57,808:INFO:Copying training dataset
2023-07-11 09:30:57,808:INFO:Defining folds
2023-07-11 09:30:57,808:INFO:Declaring metric variables
2023-07-11 09:30:57,808:INFO:Importing untrained model
2023-07-11 09:30:57,808:INFO:Lasso Least Angle Regression Imported successfully
2023-07-11 09:30:57,808:INFO:Starting cross validation
2023-07-11 09:30:57,808:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:31:00,595:INFO:Calculating mean and std
2023-07-11 09:31:00,595:INFO:Creating metrics dataframe
2023-07-11 09:31:01,164:INFO:Uploading results into container
2023-07-11 09:31:01,164:INFO:Uploading model into container now
2023-07-11 09:31:01,164:INFO:_master_model_container: 6
2023-07-11 09:31:01,164:INFO:_display_container: 2
2023-07-11 09:31:01,164:INFO:LassoLars(random_state=6256)
2023-07-11 09:31:01,164:INFO:create_model() successfully completed......................................
2023-07-11 09:31:01,288:INFO:SubProcess create_model() end ==================================
2023-07-11 09:31:01,288:INFO:Creating metrics dataframe
2023-07-11 09:31:01,288:INFO:Initializing Orthogonal Matching Pursuit
2023-07-11 09:31:01,288:INFO:Total runtime is 0.4445161024729411 minutes
2023-07-11 09:31:01,288:INFO:SubProcess create_model() called ==================================
2023-07-11 09:31:01,288:INFO:Initializing create_model()
2023-07-11 09:31:01,288:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D78BA0A0>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D6CF9190>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:31:01,288:INFO:Checking exceptions
2023-07-11 09:31:01,288:INFO:Importing libraries
2023-07-11 09:31:01,288:INFO:Copying training dataset
2023-07-11 09:31:01,298:INFO:Defining folds
2023-07-11 09:31:01,298:INFO:Declaring metric variables
2023-07-11 09:31:01,298:INFO:Importing untrained model
2023-07-11 09:31:01,298:INFO:Orthogonal Matching Pursuit Imported successfully
2023-07-11 09:31:01,298:INFO:Starting cross validation
2023-07-11 09:31:01,299:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:31:04,057:INFO:Calculating mean and std
2023-07-11 09:31:04,057:INFO:Creating metrics dataframe
2023-07-11 09:31:04,514:INFO:Uploading results into container
2023-07-11 09:31:04,515:INFO:Uploading model into container now
2023-07-11 09:31:04,515:INFO:_master_model_container: 7
2023-07-11 09:31:04,515:INFO:_display_container: 2
2023-07-11 09:31:04,515:INFO:OrthogonalMatchingPursuit()
2023-07-11 09:31:04,515:INFO:create_model() successfully completed......................................
2023-07-11 09:31:04,626:INFO:SubProcess create_model() end ==================================
2023-07-11 09:31:04,626:INFO:Creating metrics dataframe
2023-07-11 09:31:04,626:INFO:Initializing Bayesian Ridge
2023-07-11 09:31:04,626:INFO:Total runtime is 0.50013374487559 minutes
2023-07-11 09:31:04,626:INFO:SubProcess create_model() called ==================================
2023-07-11 09:31:04,626:INFO:Initializing create_model()
2023-07-11 09:31:04,626:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D78BA0A0>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D6CF9190>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:31:04,626:INFO:Checking exceptions
2023-07-11 09:31:04,626:INFO:Importing libraries
2023-07-11 09:31:04,626:INFO:Copying training dataset
2023-07-11 09:31:04,626:INFO:Defining folds
2023-07-11 09:31:04,626:INFO:Declaring metric variables
2023-07-11 09:31:04,626:INFO:Importing untrained model
2023-07-11 09:31:04,626:INFO:Bayesian Ridge Imported successfully
2023-07-11 09:31:04,626:INFO:Starting cross validation
2023-07-11 09:31:04,626:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:31:07,579:INFO:Calculating mean and std
2023-07-11 09:31:07,579:INFO:Creating metrics dataframe
2023-07-11 09:31:08,058:INFO:Uploading results into container
2023-07-11 09:31:08,058:INFO:Uploading model into container now
2023-07-11 09:31:08,058:INFO:_master_model_container: 8
2023-07-11 09:31:08,058:INFO:_display_container: 2
2023-07-11 09:31:08,058:INFO:BayesianRidge()
2023-07-11 09:31:08,058:INFO:create_model() successfully completed......................................
2023-07-11 09:31:08,166:INFO:SubProcess create_model() end ==================================
2023-07-11 09:31:08,166:INFO:Creating metrics dataframe
2023-07-11 09:31:08,178:INFO:Initializing Passive Aggressive Regressor
2023-07-11 09:31:08,178:INFO:Total runtime is 0.5593464970588684 minutes
2023-07-11 09:31:08,178:INFO:SubProcess create_model() called ==================================
2023-07-11 09:31:08,178:INFO:Initializing create_model()
2023-07-11 09:31:08,178:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D78BA0A0>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D6CF9190>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:31:08,178:INFO:Checking exceptions
2023-07-11 09:31:08,178:INFO:Importing libraries
2023-07-11 09:31:08,178:INFO:Copying training dataset
2023-07-11 09:31:08,178:INFO:Defining folds
2023-07-11 09:31:08,178:INFO:Declaring metric variables
2023-07-11 09:31:08,178:INFO:Importing untrained model
2023-07-11 09:31:08,178:INFO:Passive Aggressive Regressor Imported successfully
2023-07-11 09:31:08,178:INFO:Starting cross validation
2023-07-11 09:31:08,178:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:31:11,009:INFO:Calculating mean and std
2023-07-11 09:31:11,009:INFO:Creating metrics dataframe
2023-07-11 09:31:11,547:INFO:Uploading results into container
2023-07-11 09:31:11,547:INFO:Uploading model into container now
2023-07-11 09:31:11,547:INFO:_master_model_container: 9
2023-07-11 09:31:11,547:INFO:_display_container: 2
2023-07-11 09:31:11,547:INFO:PassiveAggressiveRegressor(random_state=6256)
2023-07-11 09:31:11,547:INFO:create_model() successfully completed......................................
2023-07-11 09:31:11,674:INFO:SubProcess create_model() end ==================================
2023-07-11 09:31:11,674:INFO:Creating metrics dataframe
2023-07-11 09:31:11,674:INFO:Initializing Huber Regressor
2023-07-11 09:31:11,674:INFO:Total runtime is 0.6176029721895854 minutes
2023-07-11 09:31:11,674:INFO:SubProcess create_model() called ==================================
2023-07-11 09:31:11,674:INFO:Initializing create_model()
2023-07-11 09:31:11,674:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D78BA0A0>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D6CF9190>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:31:11,674:INFO:Checking exceptions
2023-07-11 09:31:11,674:INFO:Importing libraries
2023-07-11 09:31:11,674:INFO:Copying training dataset
2023-07-11 09:31:11,689:INFO:Defining folds
2023-07-11 09:31:11,689:INFO:Declaring metric variables
2023-07-11 09:31:11,689:INFO:Importing untrained model
2023-07-11 09:31:11,689:INFO:Huber Regressor Imported successfully
2023-07-11 09:31:11,689:INFO:Starting cross validation
2023-07-11 09:31:11,689:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:31:15,781:INFO:Calculating mean and std
2023-07-11 09:31:15,781:INFO:Creating metrics dataframe
2023-07-11 09:31:16,290:INFO:Uploading results into container
2023-07-11 09:31:16,290:INFO:Uploading model into container now
2023-07-11 09:31:16,290:INFO:_master_model_container: 10
2023-07-11 09:31:16,290:INFO:_display_container: 2
2023-07-11 09:31:16,290:INFO:HuberRegressor()
2023-07-11 09:31:16,290:INFO:create_model() successfully completed......................................
2023-07-11 09:31:16,412:INFO:SubProcess create_model() end ==================================
2023-07-11 09:31:16,412:INFO:Creating metrics dataframe
2023-07-11 09:31:16,412:INFO:Initializing K Neighbors Regressor
2023-07-11 09:31:16,412:INFO:Total runtime is 0.6965794444084168 minutes
2023-07-11 09:31:16,412:INFO:SubProcess create_model() called ==================================
2023-07-11 09:31:16,412:INFO:Initializing create_model()
2023-07-11 09:31:16,412:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D78BA0A0>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D6CF9190>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:31:16,412:INFO:Checking exceptions
2023-07-11 09:31:16,412:INFO:Importing libraries
2023-07-11 09:31:16,412:INFO:Copying training dataset
2023-07-11 09:31:16,412:INFO:Defining folds
2023-07-11 09:31:16,412:INFO:Declaring metric variables
2023-07-11 09:31:16,412:INFO:Importing untrained model
2023-07-11 09:31:16,412:INFO:K Neighbors Regressor Imported successfully
2023-07-11 09:31:16,412:INFO:Starting cross validation
2023-07-11 09:31:16,412:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:31:19,318:INFO:Calculating mean and std
2023-07-11 09:31:19,318:INFO:Creating metrics dataframe
2023-07-11 09:31:19,857:INFO:Uploading results into container
2023-07-11 09:31:19,857:INFO:Uploading model into container now
2023-07-11 09:31:19,857:INFO:_master_model_container: 11
2023-07-11 09:31:19,857:INFO:_display_container: 2
2023-07-11 09:31:19,857:INFO:KNeighborsRegressor(n_jobs=-1)
2023-07-11 09:31:19,857:INFO:create_model() successfully completed......................................
2023-07-11 09:31:19,989:INFO:SubProcess create_model() end ==================================
2023-07-11 09:31:19,989:INFO:Creating metrics dataframe
2023-07-11 09:31:19,989:INFO:Initializing Decision Tree Regressor
2023-07-11 09:31:19,989:INFO:Total runtime is 0.7561937173207601 minutes
2023-07-11 09:31:19,989:INFO:SubProcess create_model() called ==================================
2023-07-11 09:31:19,989:INFO:Initializing create_model()
2023-07-11 09:31:19,995:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D78BA0A0>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D6CF9190>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:31:19,995:INFO:Checking exceptions
2023-07-11 09:31:19,995:INFO:Importing libraries
2023-07-11 09:31:19,995:INFO:Copying training dataset
2023-07-11 09:31:19,995:INFO:Defining folds
2023-07-11 09:31:19,995:INFO:Declaring metric variables
2023-07-11 09:31:19,995:INFO:Importing untrained model
2023-07-11 09:31:19,995:INFO:Decision Tree Regressor Imported successfully
2023-07-11 09:31:19,995:INFO:Starting cross validation
2023-07-11 09:31:19,995:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:31:22,812:INFO:Calculating mean and std
2023-07-11 09:31:22,812:INFO:Creating metrics dataframe
2023-07-11 09:31:23,363:INFO:Uploading results into container
2023-07-11 09:31:23,363:INFO:Uploading model into container now
2023-07-11 09:31:23,363:INFO:_master_model_container: 12
2023-07-11 09:31:23,363:INFO:_display_container: 2
2023-07-11 09:31:23,363:INFO:DecisionTreeRegressor(random_state=6256)
2023-07-11 09:31:23,363:INFO:create_model() successfully completed......................................
2023-07-11 09:31:23,490:INFO:SubProcess create_model() end ==================================
2023-07-11 09:31:23,490:INFO:Creating metrics dataframe
2023-07-11 09:31:23,490:INFO:Initializing Random Forest Regressor
2023-07-11 09:31:23,490:INFO:Total runtime is 0.8145366668701173 minutes
2023-07-11 09:31:23,490:INFO:SubProcess create_model() called ==================================
2023-07-11 09:31:23,490:INFO:Initializing create_model()
2023-07-11 09:31:23,490:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D78BA0A0>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D6CF9190>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:31:23,490:INFO:Checking exceptions
2023-07-11 09:31:23,490:INFO:Importing libraries
2023-07-11 09:31:23,490:INFO:Copying training dataset
2023-07-11 09:31:23,497:INFO:Defining folds
2023-07-11 09:31:23,497:INFO:Declaring metric variables
2023-07-11 09:31:23,497:INFO:Importing untrained model
2023-07-11 09:31:23,498:INFO:Random Forest Regressor Imported successfully
2023-07-11 09:31:23,499:INFO:Starting cross validation
2023-07-11 09:31:23,502:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:31:27,830:INFO:Calculating mean and std
2023-07-11 09:31:27,830:INFO:Creating metrics dataframe
2023-07-11 09:31:28,278:INFO:Uploading results into container
2023-07-11 09:31:28,278:INFO:Uploading model into container now
2023-07-11 09:31:28,279:INFO:_master_model_container: 13
2023-07-11 09:31:28,279:INFO:_display_container: 2
2023-07-11 09:31:28,279:INFO:RandomForestRegressor(n_jobs=-1, random_state=6256)
2023-07-11 09:31:28,279:INFO:create_model() successfully completed......................................
2023-07-11 09:31:28,395:INFO:SubProcess create_model() end ==================================
2023-07-11 09:31:28,395:INFO:Creating metrics dataframe
2023-07-11 09:31:28,402:INFO:Initializing Extra Trees Regressor
2023-07-11 09:31:28,402:INFO:Total runtime is 0.8964152455329896 minutes
2023-07-11 09:31:28,402:INFO:SubProcess create_model() called ==================================
2023-07-11 09:31:28,412:INFO:Initializing create_model()
2023-07-11 09:31:28,412:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D78BA0A0>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D6CF9190>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:31:28,412:INFO:Checking exceptions
2023-07-11 09:31:28,412:INFO:Importing libraries
2023-07-11 09:31:28,412:INFO:Copying training dataset
2023-07-11 09:31:28,412:INFO:Defining folds
2023-07-11 09:31:28,412:INFO:Declaring metric variables
2023-07-11 09:31:28,412:INFO:Importing untrained model
2023-07-11 09:31:28,412:INFO:Extra Trees Regressor Imported successfully
2023-07-11 09:31:28,412:INFO:Starting cross validation
2023-07-11 09:31:28,412:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:31:32,504:INFO:Calculating mean and std
2023-07-11 09:31:32,505:INFO:Creating metrics dataframe
2023-07-11 09:31:32,979:INFO:Uploading results into container
2023-07-11 09:31:32,979:INFO:Uploading model into container now
2023-07-11 09:31:32,979:INFO:_master_model_container: 14
2023-07-11 09:31:32,979:INFO:_display_container: 2
2023-07-11 09:31:32,979:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=6256)
2023-07-11 09:31:32,979:INFO:create_model() successfully completed......................................
2023-07-11 09:31:33,101:INFO:SubProcess create_model() end ==================================
2023-07-11 09:31:33,101:INFO:Creating metrics dataframe
2023-07-11 09:31:33,104:INFO:Initializing AdaBoost Regressor
2023-07-11 09:31:33,104:INFO:Total runtime is 0.97476939757665 minutes
2023-07-11 09:31:33,104:INFO:SubProcess create_model() called ==================================
2023-07-11 09:31:33,104:INFO:Initializing create_model()
2023-07-11 09:31:33,104:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D78BA0A0>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D6CF9190>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:31:33,104:INFO:Checking exceptions
2023-07-11 09:31:33,104:INFO:Importing libraries
2023-07-11 09:31:33,104:INFO:Copying training dataset
2023-07-11 09:31:33,107:INFO:Defining folds
2023-07-11 09:31:33,107:INFO:Declaring metric variables
2023-07-11 09:31:33,107:INFO:Importing untrained model
2023-07-11 09:31:33,108:INFO:AdaBoost Regressor Imported successfully
2023-07-11 09:31:33,108:INFO:Starting cross validation
2023-07-11 09:31:33,110:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:31:36,615:INFO:Calculating mean and std
2023-07-11 09:31:36,615:INFO:Creating metrics dataframe
2023-07-11 09:31:37,180:INFO:Uploading results into container
2023-07-11 09:31:37,180:INFO:Uploading model into container now
2023-07-11 09:31:37,180:INFO:_master_model_container: 15
2023-07-11 09:31:37,180:INFO:_display_container: 2
2023-07-11 09:31:37,180:INFO:AdaBoostRegressor(random_state=6256)
2023-07-11 09:31:37,180:INFO:create_model() successfully completed......................................
2023-07-11 09:31:37,323:INFO:SubProcess create_model() end ==================================
2023-07-11 09:31:37,324:INFO:Creating metrics dataframe
2023-07-11 09:31:37,331:INFO:Initializing Gradient Boosting Regressor
2023-07-11 09:31:37,331:INFO:Total runtime is 1.0452325105667115 minutes
2023-07-11 09:31:37,331:INFO:SubProcess create_model() called ==================================
2023-07-11 09:31:37,331:INFO:Initializing create_model()
2023-07-11 09:31:37,331:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D78BA0A0>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D6CF9190>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:31:37,331:INFO:Checking exceptions
2023-07-11 09:31:37,331:INFO:Importing libraries
2023-07-11 09:31:37,331:INFO:Copying training dataset
2023-07-11 09:31:37,331:INFO:Defining folds
2023-07-11 09:31:37,331:INFO:Declaring metric variables
2023-07-11 09:31:37,331:INFO:Importing untrained model
2023-07-11 09:31:37,331:INFO:Gradient Boosting Regressor Imported successfully
2023-07-11 09:31:37,331:INFO:Starting cross validation
2023-07-11 09:31:37,331:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:31:41,312:INFO:Calculating mean and std
2023-07-11 09:31:41,313:INFO:Creating metrics dataframe
2023-07-11 09:31:41,827:INFO:Uploading results into container
2023-07-11 09:31:41,827:INFO:Uploading model into container now
2023-07-11 09:31:41,827:INFO:_master_model_container: 16
2023-07-11 09:31:41,827:INFO:_display_container: 2
2023-07-11 09:31:41,827:INFO:GradientBoostingRegressor(random_state=6256)
2023-07-11 09:31:41,827:INFO:create_model() successfully completed......................................
2023-07-11 09:31:41,964:INFO:SubProcess create_model() end ==================================
2023-07-11 09:31:41,964:INFO:Creating metrics dataframe
2023-07-11 09:31:41,964:INFO:Initializing Extreme Gradient Boosting
2023-07-11 09:31:41,964:INFO:Total runtime is 1.1224430004755657 minutes
2023-07-11 09:31:41,964:INFO:SubProcess create_model() called ==================================
2023-07-11 09:31:41,980:INFO:Initializing create_model()
2023-07-11 09:31:41,980:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D78BA0A0>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D6CF9190>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:31:41,980:INFO:Checking exceptions
2023-07-11 09:31:41,980:INFO:Importing libraries
2023-07-11 09:31:41,980:INFO:Copying training dataset
2023-07-11 09:31:41,980:INFO:Defining folds
2023-07-11 09:31:41,980:INFO:Declaring metric variables
2023-07-11 09:31:41,980:INFO:Importing untrained model
2023-07-11 09:31:41,980:INFO:Extreme Gradient Boosting Imported successfully
2023-07-11 09:31:41,980:INFO:Starting cross validation
2023-07-11 09:31:41,980:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:31:46,415:WARNING:create_model() for xgboost raised an exception or returned all 0.0, trying without fit_kwargs:
2023-07-11 09:31:46,415:WARNING:Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 792, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\core.py", line 620, in inner_f
    return func(**kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\sklearn.py", line 988, in fit
    train_dmatrix, evals = _wrap_evaluation_matrices(
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\sklearn.py", line 448, in _wrap_evaluation_matrices
    train_dmatrix = create_dmatrix(
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\sklearn.py", line 908, in _create_dmatrix
    return DMatrix(**kwargs, nthread=self.n_jobs)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\core.py", line 620, in inner_f
    return func(**kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\core.py", line 766, in __init__
    self.feature_names = feature_names
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\core.py", line 1155, in feature_names
    raise ValueError('feature_names must be unique')
ValueError: feature_names must be unique


2023-07-11 09:31:46,415:INFO:Initializing create_model()
2023-07-11 09:31:46,415:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D78BA0A0>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D6CF9190>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:31:46,415:INFO:Checking exceptions
2023-07-11 09:31:46,415:INFO:Importing libraries
2023-07-11 09:31:46,415:INFO:Copying training dataset
2023-07-11 09:31:46,415:INFO:Defining folds
2023-07-11 09:31:46,415:INFO:Declaring metric variables
2023-07-11 09:31:46,415:INFO:Importing untrained model
2023-07-11 09:31:46,431:INFO:Extreme Gradient Boosting Imported successfully
2023-07-11 09:31:46,431:INFO:Starting cross validation
2023-07-11 09:31:46,431:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:31:50,200:ERROR:create_model() for xgboost raised an exception or returned all 0.0:
2023-07-11 09:31:50,200:ERROR:Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 792, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\core.py", line 620, in inner_f
    return func(**kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\sklearn.py", line 988, in fit
    train_dmatrix, evals = _wrap_evaluation_matrices(
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\sklearn.py", line 448, in _wrap_evaluation_matrices
    train_dmatrix = create_dmatrix(
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\sklearn.py", line 908, in _create_dmatrix
    return DMatrix(**kwargs, nthread=self.n_jobs)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\core.py", line 620, in inner_f
    return func(**kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\core.py", line 766, in __init__
    self.feature_names = feature_names
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\core.py", line 1155, in feature_names
    raise ValueError('feature_names must be unique')
ValueError: feature_names must be unique


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 808, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\core.py", line 620, in inner_f
    return func(**kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\sklearn.py", line 988, in fit
    train_dmatrix, evals = _wrap_evaluation_matrices(
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\sklearn.py", line 448, in _wrap_evaluation_matrices
    train_dmatrix = create_dmatrix(
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\sklearn.py", line 908, in _create_dmatrix
    return DMatrix(**kwargs, nthread=self.n_jobs)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\core.py", line 620, in inner_f
    return func(**kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\core.py", line 766, in __init__
    self.feature_names = feature_names
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\core.py", line 1155, in feature_names
    raise ValueError('feature_names must be unique')
ValueError: feature_names must be unique


2023-07-11 09:31:50,965:INFO:Initializing Light Gradient Boosting Machine
2023-07-11 09:31:50,965:INFO:Total runtime is 1.2724631547927858 minutes
2023-07-11 09:31:50,965:INFO:SubProcess create_model() called ==================================
2023-07-11 09:31:50,965:INFO:Initializing create_model()
2023-07-11 09:31:50,965:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D78BA0A0>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D6CF9190>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:31:50,965:INFO:Checking exceptions
2023-07-11 09:31:50,965:INFO:Importing libraries
2023-07-11 09:31:50,965:INFO:Copying training dataset
2023-07-11 09:31:50,965:INFO:Defining folds
2023-07-11 09:31:50,965:INFO:Declaring metric variables
2023-07-11 09:31:50,965:INFO:Importing untrained model
2023-07-11 09:31:50,965:INFO:Light Gradient Boosting Machine Imported successfully
2023-07-11 09:31:50,965:INFO:Starting cross validation
2023-07-11 09:31:50,965:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:31:55,411:WARNING:create_model() for lightgbm raised an exception or returned all 0.0, trying without fit_kwargs:
2023-07-11 09:31:55,411:WARNING:Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 792, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\sklearn.py", line 895, in fit
    super().fit(X, y, sample_weight=sample_weight, init_score=init_score,
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\sklearn.py", line 748, in fit
    self._Booster = train(
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\engine.py", line 271, in train
    booster = Booster(params=params, train_set=train_set)
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 2605, in __init__
    train_set.construct()
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 1815, in construct
    self._lazy_init(self.data, label=self.label,
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 1573, in _lazy_init
    return self.set_feature_name(feature_name)
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 2142, in set_feature_name
    _safe_call(_LIB.LGBM_DatasetSetFeatureNames(
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 125, in _safe_call
    raise LightGBMError(_LIB.LGBM_GetLastError().decode('utf-8'))
lightgbm.basic.LightGBMError: Feature (salary_currency_USD) appears more than one time.


2023-07-11 09:31:55,411:INFO:Initializing create_model()
2023-07-11 09:31:55,411:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D78BA0A0>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D6CF9190>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:31:55,411:INFO:Checking exceptions
2023-07-11 09:31:55,411:INFO:Importing libraries
2023-07-11 09:31:55,411:INFO:Copying training dataset
2023-07-11 09:31:55,426:INFO:Defining folds
2023-07-11 09:31:55,426:INFO:Declaring metric variables
2023-07-11 09:31:55,426:INFO:Importing untrained model
2023-07-11 09:31:55,426:INFO:Light Gradient Boosting Machine Imported successfully
2023-07-11 09:31:55,426:INFO:Starting cross validation
2023-07-11 09:31:55,426:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:31:59,276:ERROR:create_model() for lightgbm raised an exception or returned all 0.0:
2023-07-11 09:31:59,276:ERROR:Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 792, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\sklearn.py", line 895, in fit
    super().fit(X, y, sample_weight=sample_weight, init_score=init_score,
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\sklearn.py", line 748, in fit
    self._Booster = train(
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\engine.py", line 271, in train
    booster = Booster(params=params, train_set=train_set)
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 2605, in __init__
    train_set.construct()
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 1815, in construct
    self._lazy_init(self.data, label=self.label,
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 1573, in _lazy_init
    return self.set_feature_name(feature_name)
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 2142, in set_feature_name
    _safe_call(_LIB.LGBM_DatasetSetFeatureNames(
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 125, in _safe_call
    raise LightGBMError(_LIB.LGBM_GetLastError().decode('utf-8'))
lightgbm.basic.LightGBMError: Feature (salary_currency_USD) appears more than one time.


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 808, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\sklearn.py", line 895, in fit
    super().fit(X, y, sample_weight=sample_weight, init_score=init_score,
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\sklearn.py", line 748, in fit
    self._Booster = train(
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\engine.py", line 271, in train
    booster = Booster(params=params, train_set=train_set)
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 2605, in __init__
    train_set.construct()
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 1815, in construct
    self._lazy_init(self.data, label=self.label,
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 1573, in _lazy_init
    return self.set_feature_name(feature_name)
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 2142, in set_feature_name
    _safe_call(_LIB.LGBM_DatasetSetFeatureNames(
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 125, in _safe_call
    raise LightGBMError(_LIB.LGBM_GetLastError().decode('utf-8'))
lightgbm.basic.LightGBMError: Feature (salary_currency_USD) appears more than one time.


2023-07-11 09:32:00,165:INFO:Initializing Dummy Regressor
2023-07-11 09:32:00,165:INFO:Total runtime is 1.4257867217063906 minutes
2023-07-11 09:32:00,165:INFO:SubProcess create_model() called ==================================
2023-07-11 09:32:00,165:INFO:Initializing create_model()
2023-07-11 09:32:00,165:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D78BA0A0>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D6CF9190>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:32:00,165:INFO:Checking exceptions
2023-07-11 09:32:00,165:INFO:Importing libraries
2023-07-11 09:32:00,165:INFO:Copying training dataset
2023-07-11 09:32:00,165:INFO:Defining folds
2023-07-11 09:32:00,165:INFO:Declaring metric variables
2023-07-11 09:32:00,165:INFO:Importing untrained model
2023-07-11 09:32:00,165:INFO:Dummy Regressor Imported successfully
2023-07-11 09:32:00,165:INFO:Starting cross validation
2023-07-11 09:32:00,165:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:32:03,218:INFO:Calculating mean and std
2023-07-11 09:32:03,219:INFO:Creating metrics dataframe
2023-07-11 09:32:03,715:INFO:Uploading results into container
2023-07-11 09:32:03,715:INFO:Uploading model into container now
2023-07-11 09:32:03,715:INFO:_master_model_container: 17
2023-07-11 09:32:03,715:INFO:_display_container: 2
2023-07-11 09:32:03,715:INFO:DummyRegressor()
2023-07-11 09:32:03,715:INFO:create_model() successfully completed......................................
2023-07-11 09:32:03,863:INFO:SubProcess create_model() end ==================================
2023-07-11 09:32:03,864:INFO:Creating metrics dataframe
2023-07-11 09:32:03,868:INFO:Initializing create_model()
2023-07-11 09:32:03,868:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D78BA0A0>, estimator=GradientBoostingRegressor(random_state=6256), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:32:03,868:INFO:Checking exceptions
2023-07-11 09:32:03,868:INFO:Importing libraries
2023-07-11 09:32:03,868:INFO:Copying training dataset
2023-07-11 09:32:03,871:INFO:Defining folds
2023-07-11 09:32:03,871:INFO:Declaring metric variables
2023-07-11 09:32:03,871:INFO:Importing untrained model
2023-07-11 09:32:03,871:INFO:Declaring custom model
2023-07-11 09:32:03,871:INFO:Gradient Boosting Regressor Imported successfully
2023-07-11 09:32:03,872:INFO:Cross validation set to False
2023-07-11 09:32:03,872:INFO:Fitting Model
2023-07-11 09:32:04,849:INFO:GradientBoostingRegressor(random_state=6256)
2023-07-11 09:32:04,849:INFO:create_model() successfully completed......................................
2023-07-11 09:32:04,996:INFO:_master_model_container: 17
2023-07-11 09:32:04,996:INFO:_display_container: 2
2023-07-11 09:32:04,996:INFO:GradientBoostingRegressor(random_state=6256)
2023-07-11 09:32:04,996:INFO:compare_models() successfully completed......................................
2023-07-11 09:32:05,067:INFO:Initializing save_model()
2023-07-11 09:32:05,067:INFO:save_model(model=GradientBoostingRegressor(random_state=6256), model_name=best_model.pkl, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\didit\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['salary', 'salary_in_usd',
                                             'remote_ratio'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['experience_level',
                                             'employment_type', 'job_title',
                                             'salary_currency',
                                             'employee_residence',
                                             'compa...
                                    transformer=OneHotEncoder(cols=['experience_level',
                                                                    'employment_type',
                                                                    'salary_currency',
                                                                    'company_size'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('rest_encoding',
                 TransformerWrapper(include=['job_title', 'employee_residence',
                                             'company_location'],
                                    transformer=TargetEncoder(cols=['job_title',
                                                                    'employee_residence',
                                                                    'company_location'],
                                                              handle_missing='return_nan')))]), verbose=True, use_case=MLUsecase.REGRESSION, kwargs={})
2023-07-11 09:32:05,067:INFO:Adding model into prep_pipe
2023-07-11 09:32:05,080:INFO:best_model.pkl.pkl saved in current working directory
2023-07-11 09:32:05,087:INFO:Pipeline(memory=FastMemory(location=C:\Users\didit\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['salary', 'salary_in_usd',
                                             'remote_ratio'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['experience_level',
                                             'employment_type', 'job_title',
                                             'salary_currency',
                                             'employee_residence',
                                             'compa...
                                                                    'salary_currency',
                                                                    'company_size'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('rest_encoding',
                 TransformerWrapper(include=['job_title', 'employee_residence',
                                             'company_location'],
                                    transformer=TargetEncoder(cols=['job_title',
                                                                    'employee_residence',
                                                                    'company_location'],
                                                              handle_missing='return_nan'))),
                ('trained_model',
                 GradientBoostingRegressor(random_state=6256))])
2023-07-11 09:32:05,087:INFO:save_model() successfully completed......................................
2023-07-11 09:32:37,595:INFO:PyCaret RegressionExperiment
2023-07-11 09:32:37,595:INFO:Logging name: reg-default-name
2023-07-11 09:32:37,595:INFO:ML Usecase: MLUsecase.REGRESSION
2023-07-11 09:32:37,595:INFO:version 3.0.2
2023-07-11 09:32:37,595:INFO:Initializing setup()
2023-07-11 09:32:37,595:INFO:self.USI: 1411
2023-07-11 09:32:37,595:INFO:self._variable_keys: {'_ml_usecase', 'log_plots_param', 'USI', 'X_train', 'pipeline', 'gpu_param', 'exp_name_log', 'y', 'html_param', 'X', 'data', 'exp_id', 'y_train', 'target_param', 'X_test', 'fold_groups_param', 'n_jobs_param', 'idx', 'fold_generator', 'logging_param', 'memory', 'gpu_n_jobs_param', 'fold_shuffle_param', '_available_plots', 'seed', 'y_test', 'transform_target_param'}
2023-07-11 09:32:37,595:INFO:Checking environment
2023-07-11 09:32:37,595:INFO:python_version: 3.9.13
2023-07-11 09:32:37,595:INFO:python_build: ('main', 'Oct 13 2022 21:23:06')
2023-07-11 09:32:37,595:INFO:machine: AMD64
2023-07-11 09:32:37,595:INFO:platform: Windows-10-10.0.22621-SP0
2023-07-11 09:32:37,595:INFO:Memory: svmem(total=16893358080, available=1587941376, percent=90.6, used=15305416704, free=1587941376)
2023-07-11 09:32:37,595:INFO:Physical Core: 8
2023-07-11 09:32:37,595:INFO:Logical Core: 16
2023-07-11 09:32:37,595:INFO:Checking libraries
2023-07-11 09:32:37,595:INFO:System:
2023-07-11 09:32:37,595:INFO:    python: 3.9.13 (main, Oct 13 2022, 21:23:06) [MSC v.1916 64 bit (AMD64)]
2023-07-11 09:32:37,595:INFO:executable: C:\Users\didit\anaconda3\python.exe
2023-07-11 09:32:37,595:INFO:   machine: Windows-10-10.0.22621-SP0
2023-07-11 09:32:37,595:INFO:PyCaret required dependencies:
2023-07-11 09:32:37,595:INFO:                 pip: 23.0.1
2023-07-11 09:32:37,595:INFO:          setuptools: 67.8.0
2023-07-11 09:32:37,595:INFO:             pycaret: 3.0.2
2023-07-11 09:32:37,595:INFO:             IPython: 8.12.0
2023-07-11 09:32:37,595:INFO:          ipywidgets: 8.0.4
2023-07-11 09:32:37,595:INFO:                tqdm: 4.65.0
2023-07-11 09:32:37,595:INFO:               numpy: 1.21.5
2023-07-11 09:32:37,595:INFO:              pandas: 1.5.3
2023-07-11 09:32:37,595:INFO:              jinja2: 3.1.2
2023-07-11 09:32:37,595:INFO:               scipy: 1.10.1
2023-07-11 09:32:37,595:INFO:              joblib: 1.2.0
2023-07-11 09:32:37,595:INFO:             sklearn: 1.2.2
2023-07-11 09:32:37,595:INFO:                pyod: 1.0.9
2023-07-11 09:32:37,595:INFO:            imblearn: 0.10.1
2023-07-11 09:32:37,595:INFO:   category_encoders: 2.6.1
2023-07-11 09:32:37,595:INFO:            lightgbm: 3.3.5
2023-07-11 09:32:37,595:INFO:               numba: 0.57.0
2023-07-11 09:32:37,595:INFO:            requests: 2.29.0
2023-07-11 09:32:37,595:INFO:          matplotlib: 3.7.1
2023-07-11 09:32:37,595:INFO:          scikitplot: 0.3.7
2023-07-11 09:32:37,595:INFO:         yellowbrick: 1.5
2023-07-11 09:32:37,595:INFO:              plotly: 5.9.0
2023-07-11 09:32:37,595:INFO:             kaleido: 0.2.1
2023-07-11 09:32:37,595:INFO:         statsmodels: 0.13.5
2023-07-11 09:32:37,595:INFO:              sktime: 0.17.0
2023-07-11 09:32:37,595:INFO:               tbats: 1.1.3
2023-07-11 09:32:37,595:INFO:            pmdarima: 2.0.3
2023-07-11 09:32:37,595:INFO:              psutil: 5.9.0
2023-07-11 09:32:37,595:INFO:PyCaret optional dependencies:
2023-07-11 09:32:37,595:INFO:                shap: 0.41.0
2023-07-11 09:32:37,595:INFO:           interpret: Not installed
2023-07-11 09:32:37,595:INFO:                umap: Not installed
2023-07-11 09:32:37,595:INFO:    pandas_profiling: 4.3.1
2023-07-11 09:32:37,595:INFO:  explainerdashboard: Not installed
2023-07-11 09:32:37,595:INFO:             autoviz: Not installed
2023-07-11 09:32:37,595:INFO:           fairlearn: Not installed
2023-07-11 09:32:37,595:INFO:             xgboost: 1.7.6
2023-07-11 09:32:37,595:INFO:            catboost: Not installed
2023-07-11 09:32:37,595:INFO:              kmodes: Not installed
2023-07-11 09:32:37,595:INFO:             mlxtend: Not installed
2023-07-11 09:32:37,595:INFO:       statsforecast: Not installed
2023-07-11 09:32:37,595:INFO:        tune_sklearn: Not installed
2023-07-11 09:32:37,595:INFO:                 ray: Not installed
2023-07-11 09:32:37,595:INFO:            hyperopt: Not installed
2023-07-11 09:32:37,595:INFO:              optuna: Not installed
2023-07-11 09:32:37,595:INFO:               skopt: Not installed
2023-07-11 09:32:37,595:INFO:              mlflow: 2.4.2
2023-07-11 09:32:37,595:INFO:              gradio: Not installed
2023-07-11 09:32:37,595:INFO:             fastapi: 0.95.2
2023-07-11 09:32:37,595:INFO:             uvicorn: 0.22.0
2023-07-11 09:32:37,595:INFO:              m2cgen: Not installed
2023-07-11 09:32:37,595:INFO:           evidently: Not installed
2023-07-11 09:32:37,595:INFO:               fugue: Not installed
2023-07-11 09:32:37,595:INFO:           streamlit: 1.23.1
2023-07-11 09:32:37,595:INFO:             prophet: Not installed
2023-07-11 09:32:37,595:INFO:None
2023-07-11 09:32:37,595:INFO:Set up data.
2023-07-11 09:32:37,611:INFO:Set up train/test split.
2023-07-11 09:32:37,611:INFO:Set up index.
2023-07-11 09:32:37,611:INFO:Set up folding strategy.
2023-07-11 09:32:37,611:INFO:Assigning column types.
2023-07-11 09:32:37,622:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-07-11 09:32:37,622:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-07-11 09:32:37,622:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-07-11 09:32:37,627:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 09:32:37,668:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 09:32:37,700:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 09:32:37,701:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:32:37,703:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:32:37,703:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-07-11 09:32:37,706:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-07-11 09:32:37,709:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 09:32:37,747:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 09:32:37,778:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 09:32:37,778:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:32:37,780:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:32:37,780:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-07-11 09:32:37,783:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-07-11 09:32:37,786:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 09:32:37,830:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 09:32:37,862:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 09:32:37,862:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:32:37,862:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:32:37,862:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-07-11 09:32:37,862:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 09:32:37,931:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 09:32:37,962:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 09:32:37,962:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:32:37,962:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:32:37,962:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-07-11 09:32:37,978:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 09:32:38,009:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 09:32:38,057:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 09:32:38,057:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:32:38,057:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:32:38,057:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 09:32:38,111:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 09:32:38,134:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 09:32:38,134:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:32:38,149:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:32:38,149:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-07-11 09:32:38,196:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 09:32:38,228:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 09:32:38,228:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:32:38,228:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:32:38,278:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 09:32:38,310:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 09:32:38,311:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:32:38,313:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:32:38,313:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-07-11 09:32:38,364:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 09:32:38,395:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:32:38,395:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:32:38,443:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 09:32:38,475:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:32:38,475:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:32:38,475:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-07-11 09:32:38,548:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:32:38,548:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:32:38,627:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:32:38,642:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:32:38,642:INFO:Preparing preprocessing pipeline...
2023-07-11 09:32:38,642:INFO:Set up simple imputation.
2023-07-11 09:32:38,642:INFO:Set up encoding of categorical features.
2023-07-11 09:32:38,733:INFO:Finished creating preprocessing pipeline.
2023-07-11 09:32:38,749:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\didit\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['work_year', 'salary',
                                             'remote_ratio'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['experience_level',
                                             'employment_type', 'job_title',
                                             'salary_currency',
                                             'employee_residence',
                                             'company_l...
                                    transformer=OneHotEncoder(cols=['experience_level',
                                                                    'employment_type',
                                                                    'salary_currency',
                                                                    'company_size'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('rest_encoding',
                 TransformerWrapper(include=['job_title', 'employee_residence',
                                             'company_location'],
                                    transformer=TargetEncoder(cols=['job_title',
                                                                    'employee_residence',
                                                                    'company_location'],
                                                              handle_missing='return_nan')))])
2023-07-11 09:32:38,749:INFO:Creating final display dataframe.
2023-07-11 09:32:38,995:INFO:Setup _display_container:                     Description             Value
0                    Session id              7453
1                        Target     salary_in_usd
2                   Target type        Regression
3           Original data shape        (3755, 11)
4        Transformed data shape       (3755, 170)
5   Transformed train set shape       (2628, 170)
6    Transformed test set shape       (1127, 170)
7              Numeric features                 3
8          Categorical features                 7
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13     Maximum one-hot encoding                25
14              Encoding method              None
15               Fold Generator             KFold
16                  Fold Number                10
17                     CPU Jobs                -1
18                      Use GPU             False
19               Log Experiment             False
20              Experiment Name  reg-default-name
21                          USI              1411
2023-07-11 09:32:39,079:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:32:39,081:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:32:39,148:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:32:39,164:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:32:39,164:INFO:setup() successfully completed in 1.8s...............
2023-07-11 09:32:39,170:INFO:Initializing compare_models()
2023-07-11 09:32:39,170:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D8D82580>, include=None, fold=None, round=4, cross_validation=True, sort=R2, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x000001E1D8D82580>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2023-07-11 09:32:39,170:INFO:Checking exceptions
2023-07-11 09:32:39,173:INFO:Preparing display monitor
2023-07-11 09:32:39,175:INFO:Initializing Linear Regression
2023-07-11 09:32:39,175:INFO:Total runtime is 0.0 minutes
2023-07-11 09:32:39,175:INFO:SubProcess create_model() called ==================================
2023-07-11 09:32:39,175:INFO:Initializing create_model()
2023-07-11 09:32:39,175:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D8D82580>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D6FC0BE0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:32:39,175:INFO:Checking exceptions
2023-07-11 09:32:39,175:INFO:Importing libraries
2023-07-11 09:32:39,175:INFO:Copying training dataset
2023-07-11 09:32:39,177:INFO:Defining folds
2023-07-11 09:32:39,177:INFO:Declaring metric variables
2023-07-11 09:32:39,178:INFO:Importing untrained model
2023-07-11 09:32:39,178:INFO:Linear Regression Imported successfully
2023-07-11 09:32:39,178:INFO:Starting cross validation
2023-07-11 09:32:39,179:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:32:42,101:INFO:Calculating mean and std
2023-07-11 09:32:42,101:INFO:Creating metrics dataframe
2023-07-11 09:32:42,662:INFO:Uploading results into container
2023-07-11 09:32:42,662:INFO:Uploading model into container now
2023-07-11 09:32:42,663:INFO:_master_model_container: 1
2023-07-11 09:32:42,663:INFO:_display_container: 2
2023-07-11 09:32:42,663:INFO:LinearRegression(n_jobs=-1)
2023-07-11 09:32:42,663:INFO:create_model() successfully completed......................................
2023-07-11 09:32:42,785:INFO:SubProcess create_model() end ==================================
2023-07-11 09:32:42,785:INFO:Creating metrics dataframe
2023-07-11 09:32:42,788:INFO:Initializing Lasso Regression
2023-07-11 09:32:42,788:INFO:Total runtime is 0.0602217157681783 minutes
2023-07-11 09:32:42,788:INFO:SubProcess create_model() called ==================================
2023-07-11 09:32:42,788:INFO:Initializing create_model()
2023-07-11 09:32:42,788:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D8D82580>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D6FC0BE0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:32:42,788:INFO:Checking exceptions
2023-07-11 09:32:42,788:INFO:Importing libraries
2023-07-11 09:32:42,788:INFO:Copying training dataset
2023-07-11 09:32:42,792:INFO:Defining folds
2023-07-11 09:32:42,792:INFO:Declaring metric variables
2023-07-11 09:32:42,792:INFO:Importing untrained model
2023-07-11 09:32:42,792:INFO:Lasso Regression Imported successfully
2023-07-11 09:32:42,793:INFO:Starting cross validation
2023-07-11 09:32:42,794:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:32:43,393:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.243e+12, tolerance: 9.186e+08
  model = cd_fast.enet_coordinate_descent(

2023-07-11 09:32:43,478:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.484e+11, tolerance: 9.484e+08
  model = cd_fast.enet_coordinate_descent(

2023-07-11 09:32:43,572:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.485e+11, tolerance: 9.540e+08
  model = cd_fast.enet_coordinate_descent(

2023-07-11 09:32:43,586:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.291e+11, tolerance: 9.197e+08
  model = cd_fast.enet_coordinate_descent(

2023-07-11 09:32:43,703:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.582e+11, tolerance: 9.391e+08
  model = cd_fast.enet_coordinate_descent(

2023-07-11 09:32:43,706:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.483e+11, tolerance: 9.458e+08
  model = cd_fast.enet_coordinate_descent(

2023-07-11 09:32:43,722:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.479e+11, tolerance: 9.594e+08
  model = cd_fast.enet_coordinate_descent(

2023-07-11 09:32:46,310:INFO:Calculating mean and std
2023-07-11 09:32:46,311:INFO:Creating metrics dataframe
2023-07-11 09:32:46,782:INFO:Uploading results into container
2023-07-11 09:32:46,783:INFO:Uploading model into container now
2023-07-11 09:32:46,783:INFO:_master_model_container: 2
2023-07-11 09:32:46,783:INFO:_display_container: 2
2023-07-11 09:32:46,783:INFO:Lasso(random_state=7453)
2023-07-11 09:32:46,783:INFO:create_model() successfully completed......................................
2023-07-11 09:32:46,950:INFO:SubProcess create_model() end ==================================
2023-07-11 09:32:46,950:INFO:Creating metrics dataframe
2023-07-11 09:32:46,950:INFO:Initializing Ridge Regression
2023-07-11 09:32:46,950:INFO:Total runtime is 0.12959559361139933 minutes
2023-07-11 09:32:46,950:INFO:SubProcess create_model() called ==================================
2023-07-11 09:32:46,950:INFO:Initializing create_model()
2023-07-11 09:32:46,950:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D8D82580>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D6FC0BE0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:32:46,950:INFO:Checking exceptions
2023-07-11 09:32:46,950:INFO:Importing libraries
2023-07-11 09:32:46,950:INFO:Copying training dataset
2023-07-11 09:32:46,950:INFO:Defining folds
2023-07-11 09:32:46,950:INFO:Declaring metric variables
2023-07-11 09:32:46,950:INFO:Importing untrained model
2023-07-11 09:32:46,950:INFO:Ridge Regression Imported successfully
2023-07-11 09:32:46,950:INFO:Starting cross validation
2023-07-11 09:32:46,950:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:32:49,811:INFO:Calculating mean and std
2023-07-11 09:32:49,826:INFO:Creating metrics dataframe
2023-07-11 09:32:50,514:INFO:Uploading results into container
2023-07-11 09:32:50,515:INFO:Uploading model into container now
2023-07-11 09:32:50,515:INFO:_master_model_container: 3
2023-07-11 09:32:50,515:INFO:_display_container: 2
2023-07-11 09:32:50,516:INFO:Ridge(random_state=7453)
2023-07-11 09:32:50,516:INFO:create_model() successfully completed......................................
2023-07-11 09:32:50,628:INFO:SubProcess create_model() end ==================================
2023-07-11 09:32:50,628:INFO:Creating metrics dataframe
2023-07-11 09:32:50,628:INFO:Initializing Elastic Net
2023-07-11 09:32:50,628:INFO:Total runtime is 0.1908818006515503 minutes
2023-07-11 09:32:50,628:INFO:SubProcess create_model() called ==================================
2023-07-11 09:32:50,628:INFO:Initializing create_model()
2023-07-11 09:32:50,628:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D8D82580>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D6FC0BE0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:32:50,628:INFO:Checking exceptions
2023-07-11 09:32:50,628:INFO:Importing libraries
2023-07-11 09:32:50,628:INFO:Copying training dataset
2023-07-11 09:32:50,628:INFO:Defining folds
2023-07-11 09:32:50,628:INFO:Declaring metric variables
2023-07-11 09:32:50,628:INFO:Importing untrained model
2023-07-11 09:32:50,628:INFO:Elastic Net Imported successfully
2023-07-11 09:32:50,628:INFO:Starting cross validation
2023-07-11 09:32:50,628:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:32:53,658:INFO:Calculating mean and std
2023-07-11 09:32:53,658:INFO:Creating metrics dataframe
2023-07-11 09:32:54,249:INFO:Uploading results into container
2023-07-11 09:32:54,249:INFO:Uploading model into container now
2023-07-11 09:32:54,249:INFO:_master_model_container: 4
2023-07-11 09:32:54,249:INFO:_display_container: 2
2023-07-11 09:32:54,249:INFO:ElasticNet(random_state=7453)
2023-07-11 09:32:54,249:INFO:create_model() successfully completed......................................
2023-07-11 09:32:54,364:INFO:SubProcess create_model() end ==================================
2023-07-11 09:32:54,364:INFO:Creating metrics dataframe
2023-07-11 09:32:54,364:INFO:Initializing Least Angle Regression
2023-07-11 09:32:54,364:INFO:Total runtime is 0.2531596620877584 minutes
2023-07-11 09:32:54,364:INFO:SubProcess create_model() called ==================================
2023-07-11 09:32:54,364:INFO:Initializing create_model()
2023-07-11 09:32:54,364:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D8D82580>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D6FC0BE0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:32:54,364:INFO:Checking exceptions
2023-07-11 09:32:54,364:INFO:Importing libraries
2023-07-11 09:32:54,364:INFO:Copying training dataset
2023-07-11 09:32:54,364:INFO:Defining folds
2023-07-11 09:32:54,364:INFO:Declaring metric variables
2023-07-11 09:32:54,364:INFO:Importing untrained model
2023-07-11 09:32:54,364:INFO:Least Angle Regression Imported successfully
2023-07-11 09:32:54,364:INFO:Starting cross validation
2023-07-11 09:32:54,364:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:32:54,565:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=7.970e+02, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,581:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=1.789e+02, with an active set of 18 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,581:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=1.467e+02, with an active set of 18 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,581:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=9.719e+01, with an active set of 19 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,581:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=8.108e+01, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,581:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=7.875e+01, with an active set of 22 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,581:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=7.307e+01, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,581:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=7.017e+01, with an active set of 26 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,581:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=6.618e+01, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,581:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=5.609e+01, with an active set of 31 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,581:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=5.609e+01, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,581:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=5.548e+01, with an active set of 32 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,581:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=5.677e+01, with an active set of 34 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,581:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=5.507e+01, with an active set of 35 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,581:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=5.141e+01, with an active set of 35 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,581:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=5.612e+01, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,581:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 57 iterations, i.e. alpha=1.725e+02, with an active set of 41 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,581:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 57 iterations, i.e. alpha=1.724e+02, with an active set of 41 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,581:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 57 iterations, i.e. alpha=1.594e+02, with an active set of 41 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,597:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 70 iterations, i.e. alpha=1.234e+02, with an active set of 54 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,597:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 70 iterations, i.e. alpha=1.234e+02, with an active set of 54 regressors, and the smallest cholesky pivot element being 5.576e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,597:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 70 iterations, i.e. alpha=1.196e+02, with an active set of 54 regressors, and the smallest cholesky pivot element being 4.593e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,597:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 70 iterations, i.e. alpha=1.184e+02, with an active set of 54 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,597:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 70 iterations, i.e. alpha=1.170e+02, with an active set of 54 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,597:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 76 iterations, i.e. alpha=4.188e+02, with an active set of 57 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,597:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=7.129e+02, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,597:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=3.217e+03, with an active set of 19 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,597:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=3.160e+03, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,597:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=3.154e+03, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,597:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=3.128e+03, with an active set of 22 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,597:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=2.371e+03, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,597:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=2.311e+03, with an active set of 28 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,597:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=2.306e+03, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,628:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=8.231e+02, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,628:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=4.205e+02, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,628:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=4.205e+02, with an active set of 15 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,643:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=4.409e+02, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,643:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=2.823e+02, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,643:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=3.282e+02, with an active set of 21 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,643:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=2.103e+02, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,643:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=3.275e+02, with an active set of 34 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,643:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=2.220e+02, with an active set of 34 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,643:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=1.940e+02, with an active set of 34 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,643:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=7.997e+02, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,643:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=1.558e+02, with an active set of 35 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,643:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=4.592e+02, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,643:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=3.900e+02, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,650:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=3.404e+02, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,650:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=1.719e+02, with an active set of 18 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,650:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=1.564e+02, with an active set of 39 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,650:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=1.719e+02, with an active set of 18 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,650:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=1.693e+02, with an active set of 18 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,650:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.564e+02, with an active set of 20 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,650:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.563e+02, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,650:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=7.248e+02, with an active set of 12 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,650:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=1.071e+02, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,650:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=4.430e+02, with an active set of 16 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,650:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=1.071e+02, with an active set of 22 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,650:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=5.125e+02, with an active set of 19 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,650:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=4.004e+02, with an active set of 19 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,650:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.378e+03, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,650:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=3.168e+02, with an active set of 19 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,650:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=3.054e+02, with an active set of 19 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,650:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=7.615e+01, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,650:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=7.614e+01, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,650:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=7.540e+01, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,650:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=7.462e+01, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,650:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=7.507e+02, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,650:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=6.375e+01, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,650:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=1.402e+03, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,650:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=7.129e+02, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,650:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=1.278e+03, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,650:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 75 iterations, i.e. alpha=2.768e+02, with an active set of 65 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,650:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=8.210e+02, with an active set of 12 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,650:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=5.237e+01, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,650:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=7.822e+02, with an active set of 29 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,650:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=1.339e+03, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,650:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=5.236e+01, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,650:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=6.653e+02, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,650:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=4.250e+02, with an active set of 14 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,650:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=7.768e+02, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,650:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=4.803e+01, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,650:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=6.907e+02, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,650:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=4.400e+01, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,650:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=4.360e+01, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,659:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=5.032e+02, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,659:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=3.973e+01, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,659:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=4.892e+02, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,659:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=7.622e+02, with an active set of 18 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,659:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=3.676e+01, with an active set of 26 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,659:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 88 iterations, i.e. alpha=1.166e+06, with an active set of 75 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,659:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=3.577e+01, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,659:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=7.592e+02, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,659:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=3.876e+02, with an active set of 39 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,659:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=3.467e+01, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,659:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=4.789e+02, with an active set of 41 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,659:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=3.788e+02, with an active set of 41 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,659:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=7.587e+02, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,659:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=2.855e+02, with an active set of 41 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,659:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=3.424e+01, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,659:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=2.956e+02, with an active set of 45 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,659:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.292e+03, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,659:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=2.911e+01, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,659:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=7.568e+02, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,659:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.017e+03, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,659:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 97 iterations, i.e. alpha=2.779e+06, with an active set of 83 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,659:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 59 iterations, i.e. alpha=7.607e+02, with an active set of 49 regressors, and the smallest cholesky pivot element being 9.186e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,659:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=2.131e+01, with an active set of 28 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,659:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=1.801e+01, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,659:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=7.567e+02, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,659:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 97 iterations, i.e. alpha=1.770e+06, with an active set of 83 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,659:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=1.714e+01, with an active set of 29 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,659:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 63 iterations, i.e. alpha=9.756e+02, with an active set of 53 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,659:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=9.567e+02, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,659:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=7.567e+02, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,659:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=9.366e+02, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,659:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=1.702e+01, with an active set of 29 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,659:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=7.562e+02, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,659:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 99 iterations, i.e. alpha=1.570e+06, with an active set of 85 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,659:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.654e+01, with an active set of 30 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,659:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=9.322e+02, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,659:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.653e+01, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,659:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 99 iterations, i.e. alpha=1.191e+06, with an active set of 85 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,659:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 99 iterations, i.e. alpha=9.328e+05, with an active set of 85 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,659:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=5.536e+02, with an active set of 28 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,659:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=7.829e+02, with an active set of 39 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,659:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 99 iterations, i.e. alpha=9.325e+05, with an active set of 85 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,659:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 84 iterations, i.e. alpha=7.090e+04, with an active set of 69 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,659:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=5.535e+02, with an active set of 28 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,659:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 84 iterations, i.e. alpha=7.050e+04, with an active set of 69 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,659:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 84 iterations, i.e. alpha=7.049e+04, with an active set of 69 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,659:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=7.520e+02, with an active set of 39 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,659:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 101 iterations, i.e. alpha=8.664e+05, with an active set of 87 regressors, and the smallest cholesky pivot element being 4.829e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,659:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=1.214e+01, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,659:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=5.531e+02, with an active set of 28 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,659:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 86 iterations, i.e. alpha=5.344e+04, with an active set of 71 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,659:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 86 iterations, i.e. alpha=5.315e+04, with an active set of 71 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,659:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=1.213e+01, with an active set of 35 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,659:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 86 iterations, i.e. alpha=3.918e+04, with an active set of 71 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,659:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=5.343e+02, with an active set of 28 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,659:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=7.629e+02, with an active set of 43 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,659:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=5.029e+02, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,659:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 86 iterations, i.e. alpha=3.813e+04, with an active set of 71 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,659:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=7.560e+02, with an active set of 43 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,659:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=4.993e+02, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,659:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 86 iterations, i.e. alpha=3.812e+04, with an active set of 71 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,659:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=1.210e+01, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,659:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 86 iterations, i.e. alpha=3.812e+04, with an active set of 71 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,659:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=7.429e+02, with an active set of 43 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,659:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=1.209e+01, with an active set of 35 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,659:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=4.559e+02, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,659:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=4.559e+02, with an active set of 30 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,659:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=1.083e+01, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,659:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=1.082e+01, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,659:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=4.521e+02, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,659:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 55 iterations, i.e. alpha=6.507e+02, with an active set of 47 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,659:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.048e+01, with an active set of 37 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,659:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 55 iterations, i.e. alpha=6.492e+02, with an active set of 47 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,659:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.047e+01, with an active set of 37 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,659:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 55 iterations, i.e. alpha=6.381e+02, with an active set of 47 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,659:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=7.807e+00, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,659:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 58 iterations, i.e. alpha=6.376e+02, with an active set of 49 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,659:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=7.687e+00, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,659:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=7.059e+00, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,659:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 60 iterations, i.e. alpha=6.386e+02, with an active set of 51 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,659:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=6.060e+00, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,659:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=2.235e+12, with an active set of 43 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,659:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 60 iterations, i.e. alpha=6.376e+02, with an active set of 51 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,659:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 60 iterations, i.e. alpha=6.374e+02, with an active set of 51 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,659:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=5.506e+00, with an active set of 37 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,659:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=1.832e+12, with an active set of 43 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,659:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=5.502e+00, with an active set of 37 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,659:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=5.141e+00, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,659:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 63 iterations, i.e. alpha=6.108e+02, with an active set of 53 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,659:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=1.076e+12, with an active set of 45 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,659:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 63 iterations, i.e. alpha=6.108e+02, with an active set of 53 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,659:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=5.015e+00, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,659:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=4.518e+02, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,659:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=4.518e+02, with an active set of 15 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,659:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=1.591e+12, with an active set of 47 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,659:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=5.015e+00, with an active set of 38 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,675:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=4.998e+00, with an active set of 38 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,676:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=4.808e+00, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,676:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=4.673e+00, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,676:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=4.654e+00, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,676:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 72 iterations, i.e. alpha=5.062e+02, with an active set of 62 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,677:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=4.561e+00, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,677:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=5.307e+02, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,677:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 62 iterations, i.e. alpha=1.565e+12, with an active set of 55 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,677:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=3.635e+00, with an active set of 38 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,677:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 62 iterations, i.e. alpha=1.524e+12, with an active set of 55 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,677:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=3.395e+00, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,677:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=3.324e+00, with an active set of 38 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,677:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=3.319e+00, with an active set of 38 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,677:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=3.179e+00, with an active set of 38 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,678:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=2.836e+00, with an active set of 38 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,678:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=3.558e+02, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,678:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=2.509e+00, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,678:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=1.140e+12, with an active set of 59 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,678:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=2.001e+00, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,678:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=3.137e+02, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,679:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=1.653e+00, with an active set of 39 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,679:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=2.562e+02, with an active set of 17 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,679:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 70 iterations, i.e. alpha=9.831e+11, with an active set of 63 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,679:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=5.734e-01, with an active set of 39 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=4.732e-01, with an active set of 39 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=4.612e-01, with an active set of 39 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.919e+02, with an active set of 18 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.693e+02, with an active set of 18 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,681:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=3.464e-01, with an active set of 39 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,681:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=9.214e+02, with an active set of 32 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,681:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 96 iterations, i.e. alpha=5.654e+09, with an active set of 81 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,681:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=1.339e+02, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,681:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=2.977e-01, with an active set of 39 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,682:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=1.336e+02, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,682:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=2.784e-01, with an active set of 39 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,682:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=1.332e+02, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,683:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=2.735e-01, with an active set of 39 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,683:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=4.188e+03, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,683:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=1.230e+02, with an active set of 20 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,683:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=2.627e-01, with an active set of 39 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,683:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=2.908e+03, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,683:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.161e+02, with an active set of 21 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,683:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=2.261e-01, with an active set of 39 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,683:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=2.261e-01, with an active set of 39 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,683:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.161e+02, with an active set of 21 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,683:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=1.969e-01, with an active set of 39 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,683:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.012e+02, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,683:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=1.960e-01, with an active set of 39 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,683:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=1.911e-01, with an active set of 39 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,683:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 60 iterations, i.e. alpha=3.904e+03, with an active set of 46 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,683:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=1.798e-01, with an active set of 39 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,683:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=8.970e+01, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,683:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=1.764e-01, with an active set of 39 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,683:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=7.530e+01, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,683:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 62 iterations, i.e. alpha=3.904e+03, with an active set of 48 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,683:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 62 iterations, i.e. alpha=3.667e+03, with an active set of 48 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,683:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=1.308e-01, with an active set of 39 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,683:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=9.518e-02, with an active set of 39 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,683:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=7.208e-02, with an active set of 39 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,683:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=5.352e+01, with an active set of 24 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,683:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=7.115e-02, with an active set of 39 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,683:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 65 iterations, i.e. alpha=3.601e+03, with an active set of 50 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,683:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=5.352e+01, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,683:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 65 iterations, i.e. alpha=3.601e+03, with an active set of 50 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,683:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=6.746e-02, with an active set of 39 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,683:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=6.284e-02, with an active set of 39 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,683:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=5.914e-02, with an active set of 39 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,683:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=4.964e+01, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,683:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=4.061e+01, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,683:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=4.048e+01, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,683:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=5.544e-02, with an active set of 39 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,683:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=3.839e+01, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,683:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=4.435e-02, with an active set of 39 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,683:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=2.957e-02, with an active set of 39 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,683:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=3.410e+01, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,683:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 76 iterations, i.e. alpha=3.579e+03, with an active set of 59 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,683:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=3.388e+01, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,683:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=3.373e+01, with an active set of 28 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,683:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=2.718e+01, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,683:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 82 iterations, i.e. alpha=4.616e+03, with an active set of 63 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,683:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=2.565e+01, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,683:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=2.564e+01, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,683:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=2.369e+01, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,683:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=2.369e+01, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,683:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=2.369e+01, with an active set of 30 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,683:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=2.356e+01, with an active set of 30 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,683:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=2.165e+01, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,683:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=2.165e+01, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,683:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=2.116e+01, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,683:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=2.077e+01, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,683:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=1.899e+01, with an active set of 32 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,683:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=1.897e+01, with an active set of 32 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,683:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=1.832e+01, with an active set of 32 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,699:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=1.820e+01, with an active set of 32 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,699:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=1.808e+01, with an active set of 32 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,699:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=1.537e+01, with an active set of 32 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,699:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=1.430e+01, with an active set of 32 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,700:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=1.394e+01, with an active set of 33 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,701:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=1.394e+01, with an active set of 33 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,701:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=1.303e+01, with an active set of 34 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,702:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=1.302e+01, with an active set of 34 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,703:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.187e+01, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,704:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.184e+01, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,704:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=1.659e+05, with an active set of 32 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,704:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.177e+01, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,704:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.102e+01, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,704:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.058e+01, with an active set of 36 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,705:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=8.951e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,705:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=1.669e+05, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,705:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=6.466e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,706:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=5.460e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,706:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=5.391e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,706:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=5.391e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,707:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=5.273e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,707:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 59 iterations, i.e. alpha=3.472e+05, with an active set of 46 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,707:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=4.939e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,707:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=4.914e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,708:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=4.889e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,708:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=3.474e+05, with an active set of 55 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,708:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=4.843e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,708:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=4.840e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,709:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=4.795e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,709:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=4.091e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,709:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=3.784e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,710:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=3.005e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,710:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=3.004e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,710:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=2.983e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,711:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=2.978e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,711:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=2.954e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,711:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=2.250e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,711:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.923e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,711:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.700e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,712:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.679e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,712:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.665e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,713:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.664e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,713:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.365e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,713:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:731: RuntimeWarning: overflow encountered in true_divide
  g1 = arrayfuncs.min_pos((C - Cov) / (AA - corr_eq_dir + tiny32))

2023-07-11 09:32:54,713:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:735: RuntimeWarning: overflow encountered in true_divide
  g2 = arrayfuncs.min_pos((C + Cov) / (AA + corr_eq_dir + tiny32))

2023-07-11 09:32:54,713:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.330e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,713:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:736: RuntimeWarning: overflow encountered in double_scalars
  gamma_ = min(g1, g2, C / AA)

2023-07-11 09:32:54,714:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.328e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,714:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:772: RuntimeWarning: overflow encountered in multiply
  coef[active] = prev_coef[active] + gamma_ * least_squares

2023-07-11 09:32:54,714:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:772: RuntimeWarning: overflow encountered in add
  coef[active] = prev_coef[active] + gamma_ * least_squares

2023-07-11 09:32:54,716:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:731: RuntimeWarning: overflow encountered in subtract
  g1 = arrayfuncs.min_pos((C - Cov) / (AA - corr_eq_dir + tiny32))

2023-07-11 09:32:54,717:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:775: RuntimeWarning: overflow encountered in multiply
  Cov -= gamma_ * corr_eq_dir

2023-07-11 09:32:54,717:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 56 iterations, i.e. alpha=1.179e+09, with an active set of 45 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,717:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:735: RuntimeWarning: invalid value encountered in add
  g2 = arrayfuncs.min_pos((C + Cov) / (AA + corr_eq_dir + tiny32))

2023-07-11 09:32:54,717:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:731: RuntimeWarning: invalid value encountered in subtract
  g1 = arrayfuncs.min_pos((C - Cov) / (AA - corr_eq_dir + tiny32))

2023-07-11 09:32:54,717:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:775: RuntimeWarning: invalid value encountered in subtract
  Cov -= gamma_ * corr_eq_dir

2023-07-11 09:32:54,717:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 57 iterations, i.e. alpha=8.135e+08, with an active set of 46 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,718:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 57 iterations, i.e. alpha=7.320e+08, with an active set of 46 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:54,750:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:772: RuntimeWarning: overflow encountered in multiply
  coef[active] = prev_coef[active] + gamma_ * least_squares

2023-07-11 09:32:54,750:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:736: RuntimeWarning: overflow encountered in double_scalars
  gamma_ = min(g1, g2, C / AA)

2023-07-11 09:32:54,750:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:772: RuntimeWarning: overflow encountered in add
  coef[active] = prev_coef[active] + gamma_ * least_squares

2023-07-11 09:32:54,750:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:731: RuntimeWarning: overflow encountered in true_divide
  g1 = arrayfuncs.min_pos((C - Cov) / (AA - corr_eq_dir + tiny32))

2023-07-11 09:32:54,750:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:735: RuntimeWarning: overflow encountered in true_divide
  g2 = arrayfuncs.min_pos((C + Cov) / (AA + corr_eq_dir + tiny32))

2023-07-11 09:32:54,897:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\extmath.py:189: RuntimeWarning: invalid value encountered in matmul
  ret = a @ b

2023-07-11 09:32:54,907:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py:192: FitFailedWarning: Metric 'make_scorer(root_mean_squared_log_error, greater_is_better=False)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 184, in _score
    return super()._score(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 282, in _score
    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\containers\metrics\regression.py", line 209, in root_mean_squared_log_error
    metrics.mean_squared_log_error(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_regression.py", line 519, in mean_squared_log_error
    y_type, y_true, y_pred, multioutput = _check_reg_targets(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_regression.py", line 102, in _check_reg_targets
    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2023-07-11 09:32:54,908:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py:192: FitFailedWarning: Metric 'make_scorer(mean_absolute_percentage_error, greater_is_better=False)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 184, in _score
    return super()._score(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 282, in _score
    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\containers\metrics\regression.py", line 233, in mean_absolute_percentage_error
    y_type, y_true, y_pred, multioutput = _check_reg_targets(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_regression.py", line 102, in _check_reg_targets
    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2023-07-11 09:32:54,908:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 282, in _score
    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_regression.py", line 196, in mean_absolute_error
    y_type, y_true, y_pred, multioutput = _check_reg_targets(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_regression.py", line 102, in _check_reg_targets
    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2023-07-11 09:32:54,908:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 282, in _score
    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_regression.py", line 442, in mean_squared_error
    y_type, y_true, y_pred, multioutput = _check_reg_targets(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_regression.py", line 102, in _check_reg_targets
    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2023-07-11 09:32:54,908:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 282, in _score
    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_regression.py", line 911, in r2_score
    y_type, y_true, y_pred, multioutput = _check_reg_targets(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_regression.py", line 102, in _check_reg_targets
    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2023-07-11 09:32:57,517:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py:378: FitFailedWarning: 
1 fits failed out of a total of 10.
The score on these train-test partitions for these parameters will be set to 0.
If these failures are not expected, you can try to debug them by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
1 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py", line 1144, in fit
    self._fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py", line 1046, in _fit
    alphas, active, coef_path, n_iter_ = lars_path(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py", line 170, in lars_path
    return _lars_path_solver(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py", line 603, in _lars_path_solver
    sign_active[n_active] = np.sign(C_)
ValueError: cannot convert float NaN to integer

  warnings.warn(some_fits_failed_message, FitFailedWarning)

2023-07-11 09:32:57,517:INFO:Calculating mean and std
2023-07-11 09:32:57,517:WARNING:C:\Users\didit\anaconda3\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)

2023-07-11 09:32:57,518:INFO:Creating metrics dataframe
2023-07-11 09:32:58,055:INFO:Uploading results into container
2023-07-11 09:32:58,055:INFO:Uploading model into container now
2023-07-11 09:32:58,055:INFO:_master_model_container: 5
2023-07-11 09:32:58,055:INFO:_display_container: 2
2023-07-11 09:32:58,055:INFO:Lars(random_state=7453)
2023-07-11 09:32:58,055:INFO:create_model() successfully completed......................................
2023-07-11 09:32:58,196:INFO:SubProcess create_model() end ==================================
2023-07-11 09:32:58,205:INFO:Creating metrics dataframe
2023-07-11 09:32:58,211:INFO:Initializing Lasso Least Angle Regression
2023-07-11 09:32:58,211:INFO:Total runtime is 0.3172644058863322 minutes
2023-07-11 09:32:58,211:INFO:SubProcess create_model() called ==================================
2023-07-11 09:32:58,211:INFO:Initializing create_model()
2023-07-11 09:32:58,211:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D8D82580>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D6FC0BE0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:32:58,211:INFO:Checking exceptions
2023-07-11 09:32:58,211:INFO:Importing libraries
2023-07-11 09:32:58,211:INFO:Copying training dataset
2023-07-11 09:32:58,214:INFO:Defining folds
2023-07-11 09:32:58,214:INFO:Declaring metric variables
2023-07-11 09:32:58,214:INFO:Importing untrained model
2023-07-11 09:32:58,214:INFO:Lasso Least Angle Regression Imported successfully
2023-07-11 09:32:58,214:INFO:Starting cross validation
2023-07-11 09:32:58,215:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:32:58,412:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=7.428e+02, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:58,412:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=3.714e+02, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:58,412:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=3.131e+02, with an active set of 18 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:58,412:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=2.493e+02, with an active set of 19 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:58,412:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=1.857e+02, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:58,412:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=1.302e+02, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:58,412:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=9.967e+01, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:58,412:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=9.860e+01, with an active set of 27 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:58,412:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=9.854e+01, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:58,412:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 38 iterations, alpha=1.256e+02, previous alpha=6.786e+01, with an active set of 29 regressors.
  warnings.warn(

2023-07-11 09:32:58,455:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=8.982e+02, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:58,455:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 19 iterations, alpha=5.211e+02, previous alpha=4.491e+02, with an active set of 14 regressors.
  warnings.warn(

2023-07-11 09:32:58,460:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=7.970e+02, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:58,460:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 19 iterations, alpha=3.457e+02, previous alpha=3.457e+02, with an active set of 14 regressors.
  warnings.warn(

2023-07-11 09:32:58,460:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.018e+03, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:58,460:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=7.482e+02, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:58,460:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=5.089e+02, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:58,460:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=4.243e+02, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:58,460:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=3.976e+02, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:58,460:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 23 iterations, alpha=4.871e+02, previous alpha=3.779e+02, with an active set of 18 regressors.
  warnings.warn(

2023-07-11 09:32:58,505:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=2.475e+02, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:58,506:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=1.238e+02, with an active set of 18 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:58,507:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=6.206e+01, with an active set of 20 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:58,507:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=6.206e+01, with an active set of 20 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:58,509:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 25 iterations, alpha=6.206e+01, previous alpha=6.206e+01, with an active set of 22 regressors.
  warnings.warn(

2023-07-11 09:32:58,514:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=5.217e+02, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:58,515:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 25 iterations, alpha=2.608e+02, previous alpha=2.608e+02, with an active set of 18 regressors.
  warnings.warn(

2023-07-11 09:32:58,531:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=9.817e+02, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:58,532:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=6.464e+02, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:58,532:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 18 iterations, alpha=5.909e+02, previous alpha=5.206e+02, with an active set of 15 regressors.
  warnings.warn(

2023-07-11 09:32:58,532:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=8.163e+01, with an active set of 18 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:58,532:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=8.163e+01, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:58,532:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=8.163e+01, with an active set of 20 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:58,532:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=4.525e+01, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:58,532:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=4.628e+01, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:58,532:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=4.433e+01, with an active set of 25 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:58,532:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=4.263e+01, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:58,532:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=4.259e+01, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:58,532:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=4.259e+01, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:58,532:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=4.255e+01, with an active set of 27 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:58,548:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=3.743e+01, with an active set of 29 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:58,548:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=3.740e+01, with an active set of 29 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:58,548:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=3.668e+01, with an active set of 29 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:58,548:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=2.930e+01, with an active set of 29 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:58,548:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=2.734e+01, with an active set of 32 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:58,548:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=2.540e+01, with an active set of 32 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:58,548:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=2.530e+01, with an active set of 32 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:32:58,548:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 51 iterations, alpha=2.706e+01, previous alpha=2.505e+01, with an active set of 32 regressors.
  warnings.warn(

2023-07-11 09:32:58,555:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 25 iterations, alpha=8.439e+01, previous alpha=8.003e+01, with an active set of 22 regressors.
  warnings.warn(

2023-07-11 09:32:58,582:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 25 iterations, alpha=2.458e+02, previous alpha=1.312e+02, with an active set of 20 regressors.
  warnings.warn(

2023-07-11 09:33:01,165:INFO:Calculating mean and std
2023-07-11 09:33:01,165:INFO:Creating metrics dataframe
2023-07-11 09:33:01,906:INFO:Uploading results into container
2023-07-11 09:33:01,907:INFO:Uploading model into container now
2023-07-11 09:33:01,907:INFO:_master_model_container: 6
2023-07-11 09:33:01,908:INFO:_display_container: 2
2023-07-11 09:33:01,908:INFO:LassoLars(random_state=7453)
2023-07-11 09:33:01,908:INFO:create_model() successfully completed......................................
2023-07-11 09:33:02,028:INFO:SubProcess create_model() end ==================================
2023-07-11 09:33:02,028:INFO:Creating metrics dataframe
2023-07-11 09:33:02,031:INFO:Initializing Orthogonal Matching Pursuit
2023-07-11 09:33:02,031:INFO:Total runtime is 0.38093825976053874 minutes
2023-07-11 09:33:02,032:INFO:SubProcess create_model() called ==================================
2023-07-11 09:33:02,032:INFO:Initializing create_model()
2023-07-11 09:33:02,032:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D8D82580>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D6FC0BE0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:33:02,032:INFO:Checking exceptions
2023-07-11 09:33:02,032:INFO:Importing libraries
2023-07-11 09:33:02,032:INFO:Copying training dataset
2023-07-11 09:33:02,036:INFO:Defining folds
2023-07-11 09:33:02,036:INFO:Declaring metric variables
2023-07-11 09:33:02,036:INFO:Importing untrained model
2023-07-11 09:33:02,036:INFO:Orthogonal Matching Pursuit Imported successfully
2023-07-11 09:33:02,036:INFO:Starting cross validation
2023-07-11 09:33:02,037:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:33:05,305:INFO:Calculating mean and std
2023-07-11 09:33:05,321:INFO:Creating metrics dataframe
2023-07-11 09:33:05,949:INFO:Uploading results into container
2023-07-11 09:33:05,949:INFO:Uploading model into container now
2023-07-11 09:33:05,949:INFO:_master_model_container: 7
2023-07-11 09:33:05,949:INFO:_display_container: 2
2023-07-11 09:33:05,949:INFO:OrthogonalMatchingPursuit()
2023-07-11 09:33:05,949:INFO:create_model() successfully completed......................................
2023-07-11 09:33:06,106:INFO:SubProcess create_model() end ==================================
2023-07-11 09:33:06,106:INFO:Creating metrics dataframe
2023-07-11 09:33:06,106:INFO:Initializing Bayesian Ridge
2023-07-11 09:33:06,106:INFO:Total runtime is 0.44886377652486165 minutes
2023-07-11 09:33:06,106:INFO:SubProcess create_model() called ==================================
2023-07-11 09:33:06,106:INFO:Initializing create_model()
2023-07-11 09:33:06,106:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D8D82580>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D6FC0BE0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:33:06,106:INFO:Checking exceptions
2023-07-11 09:33:06,106:INFO:Importing libraries
2023-07-11 09:33:06,106:INFO:Copying training dataset
2023-07-11 09:33:06,106:INFO:Defining folds
2023-07-11 09:33:06,106:INFO:Declaring metric variables
2023-07-11 09:33:06,106:INFO:Importing untrained model
2023-07-11 09:33:06,106:INFO:Bayesian Ridge Imported successfully
2023-07-11 09:33:06,106:INFO:Starting cross validation
2023-07-11 09:33:06,106:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:33:09,366:INFO:Calculating mean and std
2023-07-11 09:33:09,366:INFO:Creating metrics dataframe
2023-07-11 09:33:10,093:INFO:Uploading results into container
2023-07-11 09:33:10,093:INFO:Uploading model into container now
2023-07-11 09:33:10,093:INFO:_master_model_container: 8
2023-07-11 09:33:10,093:INFO:_display_container: 2
2023-07-11 09:33:10,093:INFO:BayesianRidge()
2023-07-11 09:33:10,093:INFO:create_model() successfully completed......................................
2023-07-11 09:33:10,218:INFO:SubProcess create_model() end ==================================
2023-07-11 09:33:10,218:INFO:Creating metrics dataframe
2023-07-11 09:33:10,218:INFO:Initializing Passive Aggressive Regressor
2023-07-11 09:33:10,218:INFO:Total runtime is 0.5173953811327616 minutes
2023-07-11 09:33:10,218:INFO:SubProcess create_model() called ==================================
2023-07-11 09:33:10,218:INFO:Initializing create_model()
2023-07-11 09:33:10,218:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D8D82580>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D6FC0BE0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:33:10,218:INFO:Checking exceptions
2023-07-11 09:33:10,218:INFO:Importing libraries
2023-07-11 09:33:10,218:INFO:Copying training dataset
2023-07-11 09:33:10,234:INFO:Defining folds
2023-07-11 09:33:10,234:INFO:Declaring metric variables
2023-07-11 09:33:10,234:INFO:Importing untrained model
2023-07-11 09:33:10,234:INFO:Passive Aggressive Regressor Imported successfully
2023-07-11 09:33:10,234:INFO:Starting cross validation
2023-07-11 09:33:10,234:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:33:13,193:INFO:Calculating mean and std
2023-07-11 09:33:13,193:INFO:Creating metrics dataframe
2023-07-11 09:33:13,753:INFO:Uploading results into container
2023-07-11 09:33:13,753:INFO:Uploading model into container now
2023-07-11 09:33:13,753:INFO:_master_model_container: 9
2023-07-11 09:33:13,753:INFO:_display_container: 2
2023-07-11 09:33:13,753:INFO:PassiveAggressiveRegressor(random_state=7453)
2023-07-11 09:33:13,753:INFO:create_model() successfully completed......................................
2023-07-11 09:33:13,886:INFO:SubProcess create_model() end ==================================
2023-07-11 09:33:13,886:INFO:Creating metrics dataframe
2023-07-11 09:33:13,886:INFO:Initializing Huber Regressor
2023-07-11 09:33:13,886:INFO:Total runtime is 0.5785253286361695 minutes
2023-07-11 09:33:13,886:INFO:SubProcess create_model() called ==================================
2023-07-11 09:33:13,886:INFO:Initializing create_model()
2023-07-11 09:33:13,886:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D8D82580>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D6FC0BE0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:33:13,886:INFO:Checking exceptions
2023-07-11 09:33:13,886:INFO:Importing libraries
2023-07-11 09:33:13,886:INFO:Copying training dataset
2023-07-11 09:33:13,886:INFO:Defining folds
2023-07-11 09:33:13,886:INFO:Declaring metric variables
2023-07-11 09:33:13,886:INFO:Importing untrained model
2023-07-11 09:33:13,886:INFO:Huber Regressor Imported successfully
2023-07-11 09:33:13,886:INFO:Starting cross validation
2023-07-11 09:33:13,886:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:33:14,855:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-07-11 09:33:18,399:INFO:Calculating mean and std
2023-07-11 09:33:18,400:INFO:Creating metrics dataframe
2023-07-11 09:33:18,973:INFO:Uploading results into container
2023-07-11 09:33:18,974:INFO:Uploading model into container now
2023-07-11 09:33:18,974:INFO:_master_model_container: 10
2023-07-11 09:33:18,974:INFO:_display_container: 2
2023-07-11 09:33:18,974:INFO:HuberRegressor()
2023-07-11 09:33:18,975:INFO:create_model() successfully completed......................................
2023-07-11 09:33:19,100:INFO:SubProcess create_model() end ==================================
2023-07-11 09:33:19,100:INFO:Creating metrics dataframe
2023-07-11 09:33:19,100:INFO:Initializing K Neighbors Regressor
2023-07-11 09:33:19,100:INFO:Total runtime is 0.6654228210449219 minutes
2023-07-11 09:33:19,100:INFO:SubProcess create_model() called ==================================
2023-07-11 09:33:19,100:INFO:Initializing create_model()
2023-07-11 09:33:19,100:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D8D82580>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D6FC0BE0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:33:19,100:INFO:Checking exceptions
2023-07-11 09:33:19,100:INFO:Importing libraries
2023-07-11 09:33:19,100:INFO:Copying training dataset
2023-07-11 09:33:19,100:INFO:Defining folds
2023-07-11 09:33:19,100:INFO:Declaring metric variables
2023-07-11 09:33:19,116:INFO:Importing untrained model
2023-07-11 09:33:19,116:INFO:K Neighbors Regressor Imported successfully
2023-07-11 09:33:19,116:INFO:Starting cross validation
2023-07-11 09:33:19,116:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:33:22,185:INFO:Calculating mean and std
2023-07-11 09:33:22,185:INFO:Creating metrics dataframe
2023-07-11 09:33:22,709:INFO:Uploading results into container
2023-07-11 09:33:22,709:INFO:Uploading model into container now
2023-07-11 09:33:22,709:INFO:_master_model_container: 11
2023-07-11 09:33:22,709:INFO:_display_container: 2
2023-07-11 09:33:22,709:INFO:KNeighborsRegressor(n_jobs=-1)
2023-07-11 09:33:22,709:INFO:create_model() successfully completed......................................
2023-07-11 09:33:22,835:INFO:SubProcess create_model() end ==================================
2023-07-11 09:33:22,835:INFO:Creating metrics dataframe
2023-07-11 09:33:22,835:INFO:Initializing Decision Tree Regressor
2023-07-11 09:33:22,835:INFO:Total runtime is 0.7276672760645548 minutes
2023-07-11 09:33:22,835:INFO:SubProcess create_model() called ==================================
2023-07-11 09:33:22,835:INFO:Initializing create_model()
2023-07-11 09:33:22,835:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D8D82580>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D6FC0BE0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:33:22,835:INFO:Checking exceptions
2023-07-11 09:33:22,835:INFO:Importing libraries
2023-07-11 09:33:22,835:INFO:Copying training dataset
2023-07-11 09:33:22,835:INFO:Defining folds
2023-07-11 09:33:22,835:INFO:Declaring metric variables
2023-07-11 09:33:22,835:INFO:Importing untrained model
2023-07-11 09:33:22,835:INFO:Decision Tree Regressor Imported successfully
2023-07-11 09:33:22,835:INFO:Starting cross validation
2023-07-11 09:33:22,835:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:33:25,846:INFO:Calculating mean and std
2023-07-11 09:33:25,846:INFO:Creating metrics dataframe
2023-07-11 09:33:26,531:INFO:Uploading results into container
2023-07-11 09:33:26,531:INFO:Uploading model into container now
2023-07-11 09:33:26,531:INFO:_master_model_container: 12
2023-07-11 09:33:26,531:INFO:_display_container: 2
2023-07-11 09:33:26,531:INFO:DecisionTreeRegressor(random_state=7453)
2023-07-11 09:33:26,531:INFO:create_model() successfully completed......................................
2023-07-11 09:33:26,656:INFO:SubProcess create_model() end ==================================
2023-07-11 09:33:26,656:INFO:Creating metrics dataframe
2023-07-11 09:33:26,656:INFO:Initializing Random Forest Regressor
2023-07-11 09:33:26,656:INFO:Total runtime is 0.7913596232732136 minutes
2023-07-11 09:33:26,656:INFO:SubProcess create_model() called ==================================
2023-07-11 09:33:26,656:INFO:Initializing create_model()
2023-07-11 09:33:26,656:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D8D82580>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D6FC0BE0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:33:26,656:INFO:Checking exceptions
2023-07-11 09:33:26,656:INFO:Importing libraries
2023-07-11 09:33:26,656:INFO:Copying training dataset
2023-07-11 09:33:26,656:INFO:Defining folds
2023-07-11 09:33:26,656:INFO:Declaring metric variables
2023-07-11 09:33:26,656:INFO:Importing untrained model
2023-07-11 09:33:26,656:INFO:Random Forest Regressor Imported successfully
2023-07-11 09:33:26,656:INFO:Starting cross validation
2023-07-11 09:33:26,656:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:33:31,098:INFO:Calculating mean and std
2023-07-11 09:33:31,098:INFO:Creating metrics dataframe
2023-07-11 09:33:31,632:INFO:Uploading results into container
2023-07-11 09:33:31,632:INFO:Uploading model into container now
2023-07-11 09:33:31,632:INFO:_master_model_container: 13
2023-07-11 09:33:31,632:INFO:_display_container: 2
2023-07-11 09:33:31,632:INFO:RandomForestRegressor(n_jobs=-1, random_state=7453)
2023-07-11 09:33:31,632:INFO:create_model() successfully completed......................................
2023-07-11 09:33:31,773:INFO:SubProcess create_model() end ==================================
2023-07-11 09:33:31,773:INFO:Creating metrics dataframe
2023-07-11 09:33:31,773:INFO:Initializing Extra Trees Regressor
2023-07-11 09:33:31,773:INFO:Total runtime is 0.8766352494557698 minutes
2023-07-11 09:33:31,773:INFO:SubProcess create_model() called ==================================
2023-07-11 09:33:31,773:INFO:Initializing create_model()
2023-07-11 09:33:31,773:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D8D82580>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D6FC0BE0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:33:31,773:INFO:Checking exceptions
2023-07-11 09:33:31,773:INFO:Importing libraries
2023-07-11 09:33:31,773:INFO:Copying training dataset
2023-07-11 09:33:31,773:INFO:Defining folds
2023-07-11 09:33:31,773:INFO:Declaring metric variables
2023-07-11 09:33:31,773:INFO:Importing untrained model
2023-07-11 09:33:31,773:INFO:Extra Trees Regressor Imported successfully
2023-07-11 09:33:31,788:INFO:Starting cross validation
2023-07-11 09:33:31,788:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:33:36,039:INFO:Calculating mean and std
2023-07-11 09:33:36,040:INFO:Creating metrics dataframe
2023-07-11 09:33:36,764:INFO:Uploading results into container
2023-07-11 09:33:36,780:INFO:Uploading model into container now
2023-07-11 09:33:36,780:INFO:_master_model_container: 14
2023-07-11 09:33:36,780:INFO:_display_container: 2
2023-07-11 09:33:36,780:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=7453)
2023-07-11 09:33:36,780:INFO:create_model() successfully completed......................................
2023-07-11 09:33:36,917:INFO:SubProcess create_model() end ==================================
2023-07-11 09:33:36,917:INFO:Creating metrics dataframe
2023-07-11 09:33:36,917:INFO:Initializing AdaBoost Regressor
2023-07-11 09:33:36,917:INFO:Total runtime is 0.962368873755137 minutes
2023-07-11 09:33:36,917:INFO:SubProcess create_model() called ==================================
2023-07-11 09:33:36,917:INFO:Initializing create_model()
2023-07-11 09:33:36,917:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D8D82580>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D6FC0BE0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:33:36,917:INFO:Checking exceptions
2023-07-11 09:33:36,917:INFO:Importing libraries
2023-07-11 09:33:36,917:INFO:Copying training dataset
2023-07-11 09:33:36,922:INFO:Defining folds
2023-07-11 09:33:36,922:INFO:Declaring metric variables
2023-07-11 09:33:36,922:INFO:Importing untrained model
2023-07-11 09:33:36,922:INFO:AdaBoost Regressor Imported successfully
2023-07-11 09:33:36,922:INFO:Starting cross validation
2023-07-11 09:33:36,922:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:33:41,248:INFO:Calculating mean and std
2023-07-11 09:33:41,249:INFO:Creating metrics dataframe
2023-07-11 09:33:42,014:INFO:Uploading results into container
2023-07-11 09:33:42,015:INFO:Uploading model into container now
2023-07-11 09:33:42,015:INFO:_master_model_container: 15
2023-07-11 09:33:42,015:INFO:_display_container: 2
2023-07-11 09:33:42,015:INFO:AdaBoostRegressor(random_state=7453)
2023-07-11 09:33:42,015:INFO:create_model() successfully completed......................................
2023-07-11 09:33:42,171:INFO:SubProcess create_model() end ==================================
2023-07-11 09:33:42,171:INFO:Creating metrics dataframe
2023-07-11 09:33:42,181:INFO:Initializing Gradient Boosting Regressor
2023-07-11 09:33:42,181:INFO:Total runtime is 1.0501072645187377 minutes
2023-07-11 09:33:42,181:INFO:SubProcess create_model() called ==================================
2023-07-11 09:33:42,181:INFO:Initializing create_model()
2023-07-11 09:33:42,181:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D8D82580>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D6FC0BE0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:33:42,181:INFO:Checking exceptions
2023-07-11 09:33:42,181:INFO:Importing libraries
2023-07-11 09:33:42,181:INFO:Copying training dataset
2023-07-11 09:33:42,185:INFO:Defining folds
2023-07-11 09:33:42,185:INFO:Declaring metric variables
2023-07-11 09:33:42,185:INFO:Importing untrained model
2023-07-11 09:33:42,185:INFO:Gradient Boosting Regressor Imported successfully
2023-07-11 09:33:42,185:INFO:Starting cross validation
2023-07-11 09:33:42,185:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:33:46,279:INFO:Calculating mean and std
2023-07-11 09:33:46,279:INFO:Creating metrics dataframe
2023-07-11 09:33:47,088:INFO:Uploading results into container
2023-07-11 09:33:47,088:INFO:Uploading model into container now
2023-07-11 09:33:47,089:INFO:_master_model_container: 16
2023-07-11 09:33:47,089:INFO:_display_container: 2
2023-07-11 09:33:47,089:INFO:GradientBoostingRegressor(random_state=7453)
2023-07-11 09:33:47,089:INFO:create_model() successfully completed......................................
2023-07-11 09:33:47,217:INFO:SubProcess create_model() end ==================================
2023-07-11 09:33:47,217:INFO:Creating metrics dataframe
2023-07-11 09:33:47,220:INFO:Initializing Extreme Gradient Boosting
2023-07-11 09:33:47,220:INFO:Total runtime is 1.1340880672136942 minutes
2023-07-11 09:33:47,221:INFO:SubProcess create_model() called ==================================
2023-07-11 09:33:47,221:INFO:Initializing create_model()
2023-07-11 09:33:47,221:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D8D82580>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D6FC0BE0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:33:47,221:INFO:Checking exceptions
2023-07-11 09:33:47,221:INFO:Importing libraries
2023-07-11 09:33:47,221:INFO:Copying training dataset
2023-07-11 09:33:47,223:INFO:Defining folds
2023-07-11 09:33:47,224:INFO:Declaring metric variables
2023-07-11 09:33:47,224:INFO:Importing untrained model
2023-07-11 09:33:47,224:INFO:Extreme Gradient Boosting Imported successfully
2023-07-11 09:33:47,224:INFO:Starting cross validation
2023-07-11 09:33:47,225:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:33:50,992:WARNING:create_model() for xgboost raised an exception or returned all 0.0, trying without fit_kwargs:
2023-07-11 09:33:50,992:WARNING:Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 792, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\core.py", line 620, in inner_f
    return func(**kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\sklearn.py", line 988, in fit
    train_dmatrix, evals = _wrap_evaluation_matrices(
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\sklearn.py", line 448, in _wrap_evaluation_matrices
    train_dmatrix = create_dmatrix(
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\sklearn.py", line 908, in _create_dmatrix
    return DMatrix(**kwargs, nthread=self.n_jobs)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\core.py", line 620, in inner_f
    return func(**kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\core.py", line 766, in __init__
    self.feature_names = feature_names
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\core.py", line 1155, in feature_names
    raise ValueError('feature_names must be unique')
ValueError: feature_names must be unique


2023-07-11 09:33:50,992:INFO:Initializing create_model()
2023-07-11 09:33:50,992:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D8D82580>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D6FC0BE0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:33:50,992:INFO:Checking exceptions
2023-07-11 09:33:50,992:INFO:Importing libraries
2023-07-11 09:33:50,993:INFO:Copying training dataset
2023-07-11 09:33:50,997:INFO:Defining folds
2023-07-11 09:33:50,997:INFO:Declaring metric variables
2023-07-11 09:33:50,997:INFO:Importing untrained model
2023-07-11 09:33:50,998:INFO:Extreme Gradient Boosting Imported successfully
2023-07-11 09:33:50,998:INFO:Starting cross validation
2023-07-11 09:33:50,999:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:33:54,817:ERROR:create_model() for xgboost raised an exception or returned all 0.0:
2023-07-11 09:33:54,817:ERROR:Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 792, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\core.py", line 620, in inner_f
    return func(**kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\sklearn.py", line 988, in fit
    train_dmatrix, evals = _wrap_evaluation_matrices(
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\sklearn.py", line 448, in _wrap_evaluation_matrices
    train_dmatrix = create_dmatrix(
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\sklearn.py", line 908, in _create_dmatrix
    return DMatrix(**kwargs, nthread=self.n_jobs)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\core.py", line 620, in inner_f
    return func(**kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\core.py", line 766, in __init__
    self.feature_names = feature_names
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\core.py", line 1155, in feature_names
    raise ValueError('feature_names must be unique')
ValueError: feature_names must be unique


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 808, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\core.py", line 620, in inner_f
    return func(**kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\sklearn.py", line 988, in fit
    train_dmatrix, evals = _wrap_evaluation_matrices(
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\sklearn.py", line 448, in _wrap_evaluation_matrices
    train_dmatrix = create_dmatrix(
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\sklearn.py", line 908, in _create_dmatrix
    return DMatrix(**kwargs, nthread=self.n_jobs)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\core.py", line 620, in inner_f
    return func(**kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\core.py", line 766, in __init__
    self.feature_names = feature_names
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\core.py", line 1155, in feature_names
    raise ValueError('feature_names must be unique')
ValueError: feature_names must be unique


2023-07-11 09:33:55,893:INFO:Initializing Light Gradient Boosting Machine
2023-07-11 09:33:55,893:INFO:Total runtime is 1.2786348462104797 minutes
2023-07-11 09:33:55,893:INFO:SubProcess create_model() called ==================================
2023-07-11 09:33:55,893:INFO:Initializing create_model()
2023-07-11 09:33:55,893:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D8D82580>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D6FC0BE0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:33:55,893:INFO:Checking exceptions
2023-07-11 09:33:55,893:INFO:Importing libraries
2023-07-11 09:33:55,893:INFO:Copying training dataset
2023-07-11 09:33:55,893:INFO:Defining folds
2023-07-11 09:33:55,893:INFO:Declaring metric variables
2023-07-11 09:33:55,893:INFO:Importing untrained model
2023-07-11 09:33:55,893:INFO:Light Gradient Boosting Machine Imported successfully
2023-07-11 09:33:55,893:INFO:Starting cross validation
2023-07-11 09:33:55,893:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:33:59,501:WARNING:create_model() for lightgbm raised an exception or returned all 0.0, trying without fit_kwargs:
2023-07-11 09:33:59,501:WARNING:Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 792, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
1 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\sklearn.py", line 895, in fit
    super().fit(X, y, sample_weight=sample_weight, init_score=init_score,
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\sklearn.py", line 748, in fit
    self._Booster = train(
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\engine.py", line 271, in train
    booster = Booster(params=params, train_set=train_set)
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 2605, in __init__
    train_set.construct()
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 1815, in construct
    self._lazy_init(self.data, label=self.label,
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 1573, in _lazy_init
    return self.set_feature_name(feature_name)
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 2142, in set_feature_name
    _safe_call(_LIB.LGBM_DatasetSetFeatureNames(
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 125, in _safe_call
    raise LightGBMError(_LIB.LGBM_GetLastError().decode('utf-8'))
lightgbm.basic.LightGBMError: Feature (salary_currency_USD) appears more than one time.

--------------------------------------------------------------------------------
9 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\sklearn.py", line 895, in fit
    super().fit(X, y, sample_weight=sample_weight, init_score=init_score,
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\sklearn.py", line 748, in fit
    self._Booster = train(
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\engine.py", line 271, in train
    booster = Booster(params=params, train_set=train_set)
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 2605, in __init__
    train_set.construct()
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 1815, in construct
    self._lazy_init(self.data, label=self.label,
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 1573, in _lazy_init
    return self.set_feature_name(feature_name)
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 2142, in set_feature_name
    _safe_call(_LIB.LGBM_DatasetSetFeatureNames(
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 125, in _safe_call
    raise LightGBMError(_LIB.LGBM_GetLastError().decode('utf-8'))
lightgbm.basic.LightGBMError: Feature (salary_currency_EUR) appears more than one time.


2023-07-11 09:33:59,501:INFO:Initializing create_model()
2023-07-11 09:33:59,501:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D8D82580>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D6FC0BE0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:33:59,501:INFO:Checking exceptions
2023-07-11 09:33:59,501:INFO:Importing libraries
2023-07-11 09:33:59,501:INFO:Copying training dataset
2023-07-11 09:33:59,501:INFO:Defining folds
2023-07-11 09:33:59,501:INFO:Declaring metric variables
2023-07-11 09:33:59,501:INFO:Importing untrained model
2023-07-11 09:33:59,501:INFO:Light Gradient Boosting Machine Imported successfully
2023-07-11 09:33:59,501:INFO:Starting cross validation
2023-07-11 09:33:59,516:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:34:03,311:ERROR:create_model() for lightgbm raised an exception or returned all 0.0:
2023-07-11 09:34:03,327:ERROR:Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 792, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
1 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\sklearn.py", line 895, in fit
    super().fit(X, y, sample_weight=sample_weight, init_score=init_score,
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\sklearn.py", line 748, in fit
    self._Booster = train(
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\engine.py", line 271, in train
    booster = Booster(params=params, train_set=train_set)
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 2605, in __init__
    train_set.construct()
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 1815, in construct
    self._lazy_init(self.data, label=self.label,
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 1573, in _lazy_init
    return self.set_feature_name(feature_name)
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 2142, in set_feature_name
    _safe_call(_LIB.LGBM_DatasetSetFeatureNames(
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 125, in _safe_call
    raise LightGBMError(_LIB.LGBM_GetLastError().decode('utf-8'))
lightgbm.basic.LightGBMError: Feature (salary_currency_USD) appears more than one time.

--------------------------------------------------------------------------------
9 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\sklearn.py", line 895, in fit
    super().fit(X, y, sample_weight=sample_weight, init_score=init_score,
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\sklearn.py", line 748, in fit
    self._Booster = train(
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\engine.py", line 271, in train
    booster = Booster(params=params, train_set=train_set)
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 2605, in __init__
    train_set.construct()
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 1815, in construct
    self._lazy_init(self.data, label=self.label,
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 1573, in _lazy_init
    return self.set_feature_name(feature_name)
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 2142, in set_feature_name
    _safe_call(_LIB.LGBM_DatasetSetFeatureNames(
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 125, in _safe_call
    raise LightGBMError(_LIB.LGBM_GetLastError().decode('utf-8'))
lightgbm.basic.LightGBMError: Feature (salary_currency_EUR) appears more than one time.


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 808, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
1 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\sklearn.py", line 895, in fit
    super().fit(X, y, sample_weight=sample_weight, init_score=init_score,
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\sklearn.py", line 748, in fit
    self._Booster = train(
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\engine.py", line 271, in train
    booster = Booster(params=params, train_set=train_set)
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 2605, in __init__
    train_set.construct()
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 1815, in construct
    self._lazy_init(self.data, label=self.label,
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 1573, in _lazy_init
    return self.set_feature_name(feature_name)
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 2142, in set_feature_name
    _safe_call(_LIB.LGBM_DatasetSetFeatureNames(
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 125, in _safe_call
    raise LightGBMError(_LIB.LGBM_GetLastError().decode('utf-8'))
lightgbm.basic.LightGBMError: Feature (salary_currency_USD) appears more than one time.

--------------------------------------------------------------------------------
9 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\sklearn.py", line 895, in fit
    super().fit(X, y, sample_weight=sample_weight, init_score=init_score,
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\sklearn.py", line 748, in fit
    self._Booster = train(
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\engine.py", line 271, in train
    booster = Booster(params=params, train_set=train_set)
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 2605, in __init__
    train_set.construct()
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 1815, in construct
    self._lazy_init(self.data, label=self.label,
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 1573, in _lazy_init
    return self.set_feature_name(feature_name)
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 2142, in set_feature_name
    _safe_call(_LIB.LGBM_DatasetSetFeatureNames(
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 125, in _safe_call
    raise LightGBMError(_LIB.LGBM_GetLastError().decode('utf-8'))
lightgbm.basic.LightGBMError: Feature (salary_currency_EUR) appears more than one time.


2023-07-11 09:34:04,350:INFO:Initializing Dummy Regressor
2023-07-11 09:34:04,350:INFO:Total runtime is 1.419596791267395 minutes
2023-07-11 09:34:04,350:INFO:SubProcess create_model() called ==================================
2023-07-11 09:34:04,350:INFO:Initializing create_model()
2023-07-11 09:34:04,350:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D8D82580>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D6FC0BE0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:34:04,350:INFO:Checking exceptions
2023-07-11 09:34:04,350:INFO:Importing libraries
2023-07-11 09:34:04,350:INFO:Copying training dataset
2023-07-11 09:34:04,350:INFO:Defining folds
2023-07-11 09:34:04,350:INFO:Declaring metric variables
2023-07-11 09:34:04,350:INFO:Importing untrained model
2023-07-11 09:34:04,350:INFO:Dummy Regressor Imported successfully
2023-07-11 09:34:04,350:INFO:Starting cross validation
2023-07-11 09:34:04,350:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:34:07,597:INFO:Calculating mean and std
2023-07-11 09:34:07,597:INFO:Creating metrics dataframe
2023-07-11 09:34:08,229:INFO:Uploading results into container
2023-07-11 09:34:08,229:INFO:Uploading model into container now
2023-07-11 09:34:08,229:INFO:_master_model_container: 17
2023-07-11 09:34:08,229:INFO:_display_container: 2
2023-07-11 09:34:08,229:INFO:DummyRegressor()
2023-07-11 09:34:08,229:INFO:create_model() successfully completed......................................
2023-07-11 09:34:08,366:INFO:SubProcess create_model() end ==================================
2023-07-11 09:34:08,366:INFO:Creating metrics dataframe
2023-07-11 09:34:08,374:INFO:Initializing create_model()
2023-07-11 09:34:08,374:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D8D82580>, estimator=ExtraTreesRegressor(n_jobs=-1, random_state=7453), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:34:08,374:INFO:Checking exceptions
2023-07-11 09:34:08,374:INFO:Importing libraries
2023-07-11 09:34:08,374:INFO:Copying training dataset
2023-07-11 09:34:08,374:INFO:Defining folds
2023-07-11 09:34:08,374:INFO:Declaring metric variables
2023-07-11 09:34:08,374:INFO:Importing untrained model
2023-07-11 09:34:08,374:INFO:Declaring custom model
2023-07-11 09:34:08,374:INFO:Extra Trees Regressor Imported successfully
2023-07-11 09:34:08,374:INFO:Cross validation set to False
2023-07-11 09:34:08,374:INFO:Fitting Model
2023-07-11 09:34:09,013:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=7453)
2023-07-11 09:34:09,013:INFO:create_model() successfully completed......................................
2023-07-11 09:34:09,164:INFO:_master_model_container: 17
2023-07-11 09:34:09,164:INFO:_display_container: 2
2023-07-11 09:34:09,164:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=7453)
2023-07-11 09:34:09,164:INFO:compare_models() successfully completed......................................
2023-07-11 09:34:09,207:INFO:Initializing save_model()
2023-07-11 09:34:09,207:INFO:save_model(model=ExtraTreesRegressor(n_jobs=-1, random_state=7453), model_name=best_model.pkl, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\didit\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['work_year', 'salary',
                                             'remote_ratio'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['experience_level',
                                             'employment_type', 'job_title',
                                             'salary_currency',
                                             'employee_residence',
                                             'company_l...
                                    transformer=OneHotEncoder(cols=['experience_level',
                                                                    'employment_type',
                                                                    'salary_currency',
                                                                    'company_size'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('rest_encoding',
                 TransformerWrapper(include=['job_title', 'employee_residence',
                                             'company_location'],
                                    transformer=TargetEncoder(cols=['job_title',
                                                                    'employee_residence',
                                                                    'company_location'],
                                                              handle_missing='return_nan')))]), verbose=True, use_case=MLUsecase.REGRESSION, kwargs={})
2023-07-11 09:34:09,207:INFO:Adding model into prep_pipe
2023-07-11 09:34:09,246:INFO:best_model.pkl.pkl saved in current working directory
2023-07-11 09:34:09,256:INFO:Pipeline(memory=FastMemory(location=C:\Users\didit\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['work_year', 'salary',
                                             'remote_ratio'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['experience_level',
                                             'employment_type', 'job_title',
                                             'salary_currency',
                                             'employee_residence',
                                             'company_l...
                                                                    'salary_currency',
                                                                    'company_size'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('rest_encoding',
                 TransformerWrapper(include=['job_title', 'employee_residence',
                                             'company_location'],
                                    transformer=TargetEncoder(cols=['job_title',
                                                                    'employee_residence',
                                                                    'company_location'],
                                                              handle_missing='return_nan'))),
                ('trained_model',
                 ExtraTreesRegressor(n_jobs=-1, random_state=7453))])
2023-07-11 09:34:09,256:INFO:save_model() successfully completed......................................
2023-07-11 09:37:34,128:INFO:PyCaret RegressionExperiment
2023-07-11 09:37:34,128:INFO:Logging name: reg-default-name
2023-07-11 09:37:34,128:INFO:ML Usecase: MLUsecase.REGRESSION
2023-07-11 09:37:34,128:INFO:version 3.0.2
2023-07-11 09:37:34,128:INFO:Initializing setup()
2023-07-11 09:37:34,128:INFO:self.USI: 62b9
2023-07-11 09:37:34,128:INFO:self._variable_keys: {'_ml_usecase', 'log_plots_param', 'USI', 'X_train', 'pipeline', 'gpu_param', 'exp_name_log', 'y', 'html_param', 'X', 'data', 'exp_id', 'y_train', 'target_param', 'X_test', 'fold_groups_param', 'n_jobs_param', 'idx', 'fold_generator', 'logging_param', 'memory', 'gpu_n_jobs_param', 'fold_shuffle_param', '_available_plots', 'seed', 'y_test', 'transform_target_param'}
2023-07-11 09:37:34,128:INFO:Checking environment
2023-07-11 09:37:34,128:INFO:python_version: 3.9.13
2023-07-11 09:37:34,128:INFO:python_build: ('main', 'Oct 13 2022 21:23:06')
2023-07-11 09:37:34,128:INFO:machine: AMD64
2023-07-11 09:37:34,128:INFO:platform: Windows-10-10.0.22621-SP0
2023-07-11 09:37:34,128:INFO:Memory: svmem(total=16893358080, available=2695450624, percent=84.0, used=14197907456, free=2695450624)
2023-07-11 09:37:34,128:INFO:Physical Core: 8
2023-07-11 09:37:34,128:INFO:Logical Core: 16
2023-07-11 09:37:34,128:INFO:Checking libraries
2023-07-11 09:37:34,128:INFO:System:
2023-07-11 09:37:34,128:INFO:    python: 3.9.13 (main, Oct 13 2022, 21:23:06) [MSC v.1916 64 bit (AMD64)]
2023-07-11 09:37:34,128:INFO:executable: C:\Users\didit\anaconda3\python.exe
2023-07-11 09:37:34,129:INFO:   machine: Windows-10-10.0.22621-SP0
2023-07-11 09:37:34,129:INFO:PyCaret required dependencies:
2023-07-11 09:37:34,129:INFO:                 pip: 23.0.1
2023-07-11 09:37:34,129:INFO:          setuptools: 67.8.0
2023-07-11 09:37:34,129:INFO:             pycaret: 3.0.2
2023-07-11 09:37:34,129:INFO:             IPython: 8.12.0
2023-07-11 09:37:34,129:INFO:          ipywidgets: 8.0.4
2023-07-11 09:37:34,129:INFO:                tqdm: 4.65.0
2023-07-11 09:37:34,129:INFO:               numpy: 1.21.5
2023-07-11 09:37:34,129:INFO:              pandas: 1.5.3
2023-07-11 09:37:34,129:INFO:              jinja2: 3.1.2
2023-07-11 09:37:34,129:INFO:               scipy: 1.10.1
2023-07-11 09:37:34,129:INFO:              joblib: 1.2.0
2023-07-11 09:37:34,129:INFO:             sklearn: 1.2.2
2023-07-11 09:37:34,129:INFO:                pyod: 1.0.9
2023-07-11 09:37:34,129:INFO:            imblearn: 0.10.1
2023-07-11 09:37:34,129:INFO:   category_encoders: 2.6.1
2023-07-11 09:37:34,129:INFO:            lightgbm: 3.3.5
2023-07-11 09:37:34,129:INFO:               numba: 0.57.0
2023-07-11 09:37:34,129:INFO:            requests: 2.29.0
2023-07-11 09:37:34,129:INFO:          matplotlib: 3.7.1
2023-07-11 09:37:34,129:INFO:          scikitplot: 0.3.7
2023-07-11 09:37:34,129:INFO:         yellowbrick: 1.5
2023-07-11 09:37:34,129:INFO:              plotly: 5.9.0
2023-07-11 09:37:34,129:INFO:             kaleido: 0.2.1
2023-07-11 09:37:34,129:INFO:         statsmodels: 0.13.5
2023-07-11 09:37:34,129:INFO:              sktime: 0.17.0
2023-07-11 09:37:34,129:INFO:               tbats: 1.1.3
2023-07-11 09:37:34,129:INFO:            pmdarima: 2.0.3
2023-07-11 09:37:34,129:INFO:              psutil: 5.9.0
2023-07-11 09:37:34,129:INFO:PyCaret optional dependencies:
2023-07-11 09:37:34,129:INFO:                shap: 0.41.0
2023-07-11 09:37:34,129:INFO:           interpret: Not installed
2023-07-11 09:37:34,129:INFO:                umap: Not installed
2023-07-11 09:37:34,129:INFO:    pandas_profiling: 4.3.1
2023-07-11 09:37:34,129:INFO:  explainerdashboard: Not installed
2023-07-11 09:37:34,129:INFO:             autoviz: Not installed
2023-07-11 09:37:34,129:INFO:           fairlearn: Not installed
2023-07-11 09:37:34,129:INFO:             xgboost: 1.7.6
2023-07-11 09:37:34,130:INFO:            catboost: Not installed
2023-07-11 09:37:34,130:INFO:              kmodes: Not installed
2023-07-11 09:37:34,130:INFO:             mlxtend: Not installed
2023-07-11 09:37:34,130:INFO:       statsforecast: Not installed
2023-07-11 09:37:34,130:INFO:        tune_sklearn: Not installed
2023-07-11 09:37:34,130:INFO:                 ray: Not installed
2023-07-11 09:37:34,130:INFO:            hyperopt: Not installed
2023-07-11 09:37:34,130:INFO:              optuna: Not installed
2023-07-11 09:37:34,130:INFO:               skopt: Not installed
2023-07-11 09:37:34,130:INFO:              mlflow: 2.4.2
2023-07-11 09:37:34,130:INFO:              gradio: Not installed
2023-07-11 09:37:34,130:INFO:             fastapi: 0.95.2
2023-07-11 09:37:34,130:INFO:             uvicorn: 0.22.0
2023-07-11 09:37:34,130:INFO:              m2cgen: Not installed
2023-07-11 09:37:34,130:INFO:           evidently: Not installed
2023-07-11 09:37:34,130:INFO:               fugue: Not installed
2023-07-11 09:37:34,130:INFO:           streamlit: 1.23.1
2023-07-11 09:37:34,130:INFO:             prophet: Not installed
2023-07-11 09:37:34,130:INFO:None
2023-07-11 09:37:34,130:INFO:Set up data.
2023-07-11 09:37:34,137:INFO:Set up train/test split.
2023-07-11 09:37:34,150:INFO:Set up index.
2023-07-11 09:37:34,150:INFO:Set up folding strategy.
2023-07-11 09:37:34,150:INFO:Assigning column types.
2023-07-11 09:37:34,154:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-07-11 09:37:34,155:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-07-11 09:37:34,159:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-07-11 09:37:34,161:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 09:37:34,205:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 09:37:34,242:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 09:37:34,243:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:37:34,245:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:37:34,245:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-07-11 09:37:34,249:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-07-11 09:37:34,252:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 09:37:34,311:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 09:37:34,353:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 09:37:34,353:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:37:34,355:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:37:34,355:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-07-11 09:37:34,360:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-07-11 09:37:34,363:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 09:37:34,404:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 09:37:34,437:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 09:37:34,437:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:37:34,439:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:37:34,443:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-07-11 09:37:34,447:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 09:37:34,488:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 09:37:34,522:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 09:37:34,522:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:37:34,525:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:37:34,525:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-07-11 09:37:34,533:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 09:37:34,574:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 09:37:34,608:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 09:37:34,609:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:37:34,611:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:37:34,617:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 09:37:34,663:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 09:37:34,704:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 09:37:34,705:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:37:34,707:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:37:34,707:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-07-11 09:37:34,759:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 09:37:34,799:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 09:37:34,800:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:37:34,804:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:37:34,852:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 09:37:34,896:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 09:37:34,896:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:37:34,898:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:37:34,899:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-07-11 09:37:34,949:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 09:37:34,989:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:37:34,991:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:37:35,035:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 09:37:35,067:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:37:35,069:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:37:35,069:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-07-11 09:37:35,150:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:37:35,153:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:37:35,240:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:37:35,242:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:37:35,242:INFO:Preparing preprocessing pipeline...
2023-07-11 09:37:35,242:INFO:Set up simple imputation.
2023-07-11 09:37:35,245:INFO:Set up encoding of categorical features.
2023-07-11 09:37:35,373:INFO:Finished creating preprocessing pipeline.
2023-07-11 09:37:35,380:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\didit\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['salary', 'salary_in_usd',
                                             'remote_ratio'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['experience_level',
                                             'employment_type', 'job_title',
                                             'salary_currency',
                                             'employee_residence',
                                             'compa...
                                    transformer=OneHotEncoder(cols=['experience_level',
                                                                    'employment_type',
                                                                    'salary_currency',
                                                                    'company_size'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('rest_encoding',
                 TransformerWrapper(include=['job_title', 'employee_residence',
                                             'company_location'],
                                    transformer=TargetEncoder(cols=['job_title',
                                                                    'employee_residence',
                                                                    'company_location'],
                                                              handle_missing='return_nan')))])
2023-07-11 09:37:35,380:INFO:Creating final display dataframe.
2023-07-11 09:37:35,647:INFO:Setup _display_container:                     Description             Value
0                    Session id              5049
1                        Target         work_year
2                   Target type        Regression
3           Original data shape        (3755, 11)
4        Transformed data shape       (3755, 162)
5   Transformed train set shape       (2628, 162)
6    Transformed test set shape       (1127, 162)
7              Numeric features                 3
8          Categorical features                 7
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13     Maximum one-hot encoding                25
14              Encoding method              None
15               Fold Generator             KFold
16                  Fold Number                10
17                     CPU Jobs                -1
18                      Use GPU             False
19               Log Experiment             False
20              Experiment Name  reg-default-name
21                          USI              62b9
2023-07-11 09:37:35,741:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:37:35,742:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:37:35,828:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:37:35,830:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:37:35,830:INFO:setup() successfully completed in 1.98s...............
2023-07-11 09:37:35,833:INFO:Initializing compare_models()
2023-07-11 09:37:35,833:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D786A8B0>, include=None, fold=None, round=4, cross_validation=True, sort=R2, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x000001E1D786A8B0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2023-07-11 09:37:35,833:INFO:Checking exceptions
2023-07-11 09:37:35,835:INFO:Preparing display monitor
2023-07-11 09:37:35,836:INFO:Initializing Linear Regression
2023-07-11 09:37:35,836:INFO:Total runtime is 0.0 minutes
2023-07-11 09:37:35,836:INFO:SubProcess create_model() called ==================================
2023-07-11 09:37:35,836:INFO:Initializing create_model()
2023-07-11 09:37:35,837:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D786A8B0>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D78022E0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:37:35,837:INFO:Checking exceptions
2023-07-11 09:37:35,837:INFO:Importing libraries
2023-07-11 09:37:35,837:INFO:Copying training dataset
2023-07-11 09:37:35,839:INFO:Defining folds
2023-07-11 09:37:35,839:INFO:Declaring metric variables
2023-07-11 09:37:35,839:INFO:Importing untrained model
2023-07-11 09:37:35,839:INFO:Linear Regression Imported successfully
2023-07-11 09:37:35,840:INFO:Starting cross validation
2023-07-11 09:37:35,840:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:37:39,860:INFO:Calculating mean and std
2023-07-11 09:37:39,861:INFO:Creating metrics dataframe
2023-07-11 09:37:40,340:INFO:Uploading results into container
2023-07-11 09:37:40,341:INFO:Uploading model into container now
2023-07-11 09:37:40,341:INFO:_master_model_container: 1
2023-07-11 09:37:40,341:INFO:_display_container: 2
2023-07-11 09:37:40,341:INFO:LinearRegression(n_jobs=-1)
2023-07-11 09:37:40,342:INFO:create_model() successfully completed......................................
2023-07-11 09:37:40,466:INFO:SubProcess create_model() end ==================================
2023-07-11 09:37:40,466:INFO:Creating metrics dataframe
2023-07-11 09:37:40,470:INFO:Initializing Lasso Regression
2023-07-11 09:37:40,470:INFO:Total runtime is 0.07722340027491252 minutes
2023-07-11 09:37:40,470:INFO:SubProcess create_model() called ==================================
2023-07-11 09:37:40,470:INFO:Initializing create_model()
2023-07-11 09:37:40,470:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D786A8B0>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D78022E0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:37:40,470:INFO:Checking exceptions
2023-07-11 09:37:40,471:INFO:Importing libraries
2023-07-11 09:37:40,471:INFO:Copying training dataset
2023-07-11 09:37:40,474:INFO:Defining folds
2023-07-11 09:37:40,474:INFO:Declaring metric variables
2023-07-11 09:37:40,474:INFO:Importing untrained model
2023-07-11 09:37:40,474:INFO:Lasso Regression Imported successfully
2023-07-11 09:37:40,475:INFO:Starting cross validation
2023-07-11 09:37:40,476:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:37:43,768:INFO:Calculating mean and std
2023-07-11 09:37:43,769:INFO:Creating metrics dataframe
2023-07-11 09:37:44,275:INFO:Uploading results into container
2023-07-11 09:37:44,276:INFO:Uploading model into container now
2023-07-11 09:37:44,276:INFO:_master_model_container: 2
2023-07-11 09:37:44,276:INFO:_display_container: 2
2023-07-11 09:37:44,277:INFO:Lasso(random_state=5049)
2023-07-11 09:37:44,277:INFO:create_model() successfully completed......................................
2023-07-11 09:37:44,405:INFO:SubProcess create_model() end ==================================
2023-07-11 09:37:44,405:INFO:Creating metrics dataframe
2023-07-11 09:37:44,414:INFO:Initializing Ridge Regression
2023-07-11 09:37:44,414:INFO:Total runtime is 0.1429641326268514 minutes
2023-07-11 09:37:44,414:INFO:SubProcess create_model() called ==================================
2023-07-11 09:37:44,414:INFO:Initializing create_model()
2023-07-11 09:37:44,415:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D786A8B0>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D78022E0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:37:44,415:INFO:Checking exceptions
2023-07-11 09:37:44,415:INFO:Importing libraries
2023-07-11 09:37:44,415:INFO:Copying training dataset
2023-07-11 09:37:44,420:INFO:Defining folds
2023-07-11 09:37:44,420:INFO:Declaring metric variables
2023-07-11 09:37:44,420:INFO:Importing untrained model
2023-07-11 09:37:44,422:INFO:Ridge Regression Imported successfully
2023-07-11 09:37:44,422:INFO:Starting cross validation
2023-07-11 09:37:44,423:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:37:47,666:INFO:Calculating mean and std
2023-07-11 09:37:47,667:INFO:Creating metrics dataframe
2023-07-11 09:37:48,139:INFO:Uploading results into container
2023-07-11 09:37:48,140:INFO:Uploading model into container now
2023-07-11 09:37:48,140:INFO:_master_model_container: 3
2023-07-11 09:37:48,140:INFO:_display_container: 2
2023-07-11 09:37:48,140:INFO:Ridge(random_state=5049)
2023-07-11 09:37:48,140:INFO:create_model() successfully completed......................................
2023-07-11 09:37:48,259:INFO:SubProcess create_model() end ==================================
2023-07-11 09:37:48,259:INFO:Creating metrics dataframe
2023-07-11 09:37:48,262:INFO:Initializing Elastic Net
2023-07-11 09:37:48,262:INFO:Total runtime is 0.20709580580393472 minutes
2023-07-11 09:37:48,262:INFO:SubProcess create_model() called ==================================
2023-07-11 09:37:48,263:INFO:Initializing create_model()
2023-07-11 09:37:48,263:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D786A8B0>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D78022E0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:37:48,263:INFO:Checking exceptions
2023-07-11 09:37:48,263:INFO:Importing libraries
2023-07-11 09:37:48,263:INFO:Copying training dataset
2023-07-11 09:37:48,266:INFO:Defining folds
2023-07-11 09:37:48,266:INFO:Declaring metric variables
2023-07-11 09:37:48,267:INFO:Importing untrained model
2023-07-11 09:37:48,267:INFO:Elastic Net Imported successfully
2023-07-11 09:37:48,267:INFO:Starting cross validation
2023-07-11 09:37:48,268:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:37:51,467:INFO:Calculating mean and std
2023-07-11 09:37:51,468:INFO:Creating metrics dataframe
2023-07-11 09:37:52,008:INFO:Uploading results into container
2023-07-11 09:37:52,009:INFO:Uploading model into container now
2023-07-11 09:37:52,009:INFO:_master_model_container: 4
2023-07-11 09:37:52,009:INFO:_display_container: 2
2023-07-11 09:37:52,009:INFO:ElasticNet(random_state=5049)
2023-07-11 09:37:52,009:INFO:create_model() successfully completed......................................
2023-07-11 09:37:52,128:INFO:SubProcess create_model() end ==================================
2023-07-11 09:37:52,128:INFO:Creating metrics dataframe
2023-07-11 09:37:52,131:INFO:Initializing Least Angle Regression
2023-07-11 09:37:52,131:INFO:Total runtime is 0.2715798894564311 minutes
2023-07-11 09:37:52,131:INFO:SubProcess create_model() called ==================================
2023-07-11 09:37:52,131:INFO:Initializing create_model()
2023-07-11 09:37:52,131:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D786A8B0>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D78022E0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:37:52,131:INFO:Checking exceptions
2023-07-11 09:37:52,131:INFO:Importing libraries
2023-07-11 09:37:52,131:INFO:Copying training dataset
2023-07-11 09:37:52,134:INFO:Defining folds
2023-07-11 09:37:52,134:INFO:Declaring metric variables
2023-07-11 09:37:52,134:INFO:Importing untrained model
2023-07-11 09:37:52,134:INFO:Least Angle Regression Imported successfully
2023-07-11 09:37:52,134:INFO:Starting cross validation
2023-07-11 09:37:52,135:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:37:52,361:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.296e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,362:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=6.725e-03, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,363:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=6.718e-03, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,363:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=6.697e-03, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,363:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=6.230e-03, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,364:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=3.495e-03, with an active set of 13 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,365:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=3.429e-03, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,365:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=3.264e-03, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,366:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=2.323e-03, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,367:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=2.051e-03, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,368:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=1.723e-03, with an active set of 16 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,368:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=1.723e-03, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,370:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=1.412e-03, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,371:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.344e-03, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,372:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.266e-03, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,373:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.265e-03, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,373:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.262e-03, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,373:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=1.111e-03, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,374:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=1.100e-03, with an active set of 25 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,375:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=1.100e-03, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,375:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=9.630e-04, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,375:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=9.356e-04, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,376:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=9.332e-04, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,376:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=8.982e-04, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,377:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=8.342e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,378:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=8.339e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,378:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=7.825e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,378:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=5.503e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,378:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=4.914e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,378:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=4.906e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,378:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=4.847e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,379:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=4.787e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,379:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=4.565e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,379:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=4.464e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,379:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=4.226e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,380:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=4.103e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,380:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=3.505e-04, with an active set of 32 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,380:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=3.080e-04, with an active set of 32 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,381:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=3.011e-04, with an active set of 32 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,381:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=2.969e-04, with an active set of 32 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,384:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=6.552e+03, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,384:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=5.884e+03, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,385:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=1.376e+03, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,386:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=1.352e+03, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,386:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.168e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,387:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=1.167e+03, with an active set of 48 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,387:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=6.089e-03, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,388:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=6.600e-03, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,388:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=5.838e-03, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,388:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=3.044e-03, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,389:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=3.111e-03, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,389:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=2.063e-03, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,390:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=2.031e-03, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,390:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=4.323e-03, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,391:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=1.199e-03, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,392:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=7.598e-03, with an active set of 18 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,392:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=4.434e-03, with an active set of 18 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,392:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=9.691e-04, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,392:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=5.830e-03, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,393:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=9.663e-04, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,393:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=5.241e-03, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,393:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=9.656e-04, with an active set of 24 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,394:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=9.590e-04, with an active set of 24 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,394:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=9.590e-04, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,394:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=9.357e-04, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,394:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=2.757e-03, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,394:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=8.759e-04, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,394:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 85 iterations, i.e. alpha=1.268e+05, with an active set of 79 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,395:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=3.832e-03, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,395:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=2.751e-03, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,396:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=8.205e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,396:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=8.045e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,396:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=7.455e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,396:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=6.846e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,396:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=2.074e-03, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,397:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.990e-03, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,397:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 91 iterations, i.e. alpha=1.368e+05, with an active set of 85 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,397:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=2.380e-03, with an active set of 32 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,397:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=6.800e-04, with an active set of 34 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,398:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=2.618e-03, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,398:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=2.587e-03, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,398:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=6.693e-04, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,398:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=2.071e-03, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,398:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=6.684e-04, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,399:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=1.767e-03, with an active set of 29 regressors, and the smallest cholesky pivot element being 8.689e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,399:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=6.606e-04, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,400:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=6.539e-04, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,400:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=6.431e-04, with an active set of 38 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,401:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=6.430e-04, with an active set of 38 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,402:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=5.935e-04, with an active set of 39 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,402:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=1.753e-03, with an active set of 34 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,402:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=5.935e-04, with an active set of 39 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,403:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=5.706e-04, with an active set of 40 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,403:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=5.338e-04, with an active set of 43 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,404:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=5.088e-04, with an active set of 43 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,405:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=4.711e-04, with an active set of 43 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,405:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=4.525e-04, with an active set of 43 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,405:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=4.473e-04, with an active set of 44 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,406:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=4.472e-04, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,406:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=4.299e-04, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,406:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=3.899e-04, with an active set of 45 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,407:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=3.872e-04, with an active set of 45 regressors, and the smallest cholesky pivot element being 6.322e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,407:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=3.197e-04, with an active set of 47 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,408:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 56 iterations, i.e. alpha=3.197e-04, with an active set of 53 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,408:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 72 iterations, i.e. alpha=3.966e-02, with an active set of 56 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,409:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=2.572e-03, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,409:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 63 iterations, i.e. alpha=3.995e-04, with an active set of 59 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,410:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 63 iterations, i.e. alpha=3.221e-04, with an active set of 59 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,410:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 65 iterations, i.e. alpha=3.197e-04, with an active set of 61 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,410:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 65 iterations, i.e. alpha=2.602e-04, with an active set of 61 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,410:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=1.286e-03, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,410:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 65 iterations, i.e. alpha=2.374e-04, with an active set of 61 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,411:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 80 iterations, i.e. alpha=4.044e-02, with an active set of 63 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,411:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=9.674e-04, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,412:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 80 iterations, i.e. alpha=4.002e-02, with an active set of 63 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,412:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=9.353e-04, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,412:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 82 iterations, i.e. alpha=4.096e-02, with an active set of 64 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,412:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=9.069e-04, with an active set of 18 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,414:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=6.988e-04, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,415:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=6.483e-04, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,415:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 94 iterations, i.e. alpha=1.619e+03, with an active set of 74 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,416:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 94 iterations, i.e. alpha=1.614e+03, with an active set of 74 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,416:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=6.480e-04, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,418:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=5.939e-04, with an active set of 32 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,419:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 105 iterations, i.e. alpha=4.561e+03, with an active set of 84 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,421:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=8.643e-04, with an active set of 43 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,423:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=8.091e-04, with an active set of 45 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,424:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 55 iterations, i.e. alpha=1.156e-03, with an active set of 50 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,426:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 62 iterations, i.e. alpha=7.968e-04, with an active set of 55 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,427:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 63 iterations, i.e. alpha=1.207e-03, with an active set of 56 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,429:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 69 iterations, i.e. alpha=7.894e-04, with an active set of 62 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,430:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 72 iterations, i.e. alpha=7.894e-04, with an active set of 64 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,431:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 74 iterations, i.e. alpha=7.894e-04, with an active set of 66 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,434:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 85 iterations, i.e. alpha=8.882e-04, with an active set of 77 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,434:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=9.258e-03, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,435:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=4.881e-03, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,435:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 89 iterations, i.e. alpha=1.050e-03, with an active set of 80 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,435:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 89 iterations, i.e. alpha=1.028e-03, with an active set of 80 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,435:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=4.678e-03, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,436:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=2.743e-03, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,436:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=2.743e-03, with an active set of 14 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,437:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=2.043e-03, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,437:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=1.371e-03, with an active set of 19 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,438:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=1.371e-03, with an active set of 19 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,438:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.123e-03, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,438:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 100 iterations, i.e. alpha=1.159e-03, with an active set of 91 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,438:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.123e-03, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,439:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=9.591e-04, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,439:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=6.658e-04, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,440:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=5.541e-04, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,440:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=5.063e-04, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,440:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=5.063e-04, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,440:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=5.044e-04, with an active set of 26 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,442:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=4.933e-04, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,443:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=4.932e-04, with an active set of 29 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,443:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=4.516e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,443:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=4.514e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,443:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=4.467e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,444:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=4.454e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,444:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=4.262e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,444:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=4.092e-04, with an active set of 33 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,445:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=4.092e-04, with an active set of 33 regressors, and the smallest cholesky pivot element being 7.300e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,445:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=3.897e-04, with an active set of 33 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,445:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=3.768e-04, with an active set of 33 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,445:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=2.887e-04, with an active set of 34 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,445:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=2.630e-04, with an active set of 34 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,446:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=2.449e-04, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,446:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=2.270e-04, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,447:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.976e-04, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,447:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.963e-04, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,447:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.398e-04, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,447:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.336e-04, with an active set of 37 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,448:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.319e-04, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,448:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.268e-04, with an active set of 37 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,448:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.242e-04, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,449:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=1.154e-04, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,449:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=1.154e-04, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,449:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=1.151e-04, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,450:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=1.113e-04, with an active set of 38 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,450:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=1.099e-04, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,450:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=1.094e-04, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,450:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=1.091e-04, with an active set of 38 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,450:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=1.085e-04, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,451:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=1.027e-04, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,451:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=1.014e-04, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,451:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=1.013e-04, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,451:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=1.012e-04, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,451:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=8.931e-05, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,452:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=7.264e-05, with an active set of 40 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,452:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=7.264e-05, with an active set of 40 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,452:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=7.255e-05, with an active set of 40 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,452:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=5.164e-05, with an active set of 40 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,453:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=3.812e-05, with an active set of 40 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,453:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=3.431e-05, with an active set of 41 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,453:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=3.430e-05, with an active set of 41 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,453:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=3.201e-05, with an active set of 41 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,453:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=3.023e-05, with an active set of 41 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,453:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=2.859e-05, with an active set of 41 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,454:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=2.817e-05, with an active set of 41 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,454:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=2.764e-05, with an active set of 41 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,454:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=2.763e-05, with an active set of 41 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,455:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=2.762e-05, with an active set of 41 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,455:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=2.685e-05, with an active set of 41 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,455:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=2.185e-05, with an active set of 41 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,455:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=2.156e-05, with an active set of 41 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,456:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=2.144e-05, with an active set of 41 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,456:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=2.131e-05, with an active set of 41 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,456:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=2.033e-05, with an active set of 41 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,456:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=2.032e-05, with an active set of 41 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,456:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=2.030e-05, with an active set of 41 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,457:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=2.029e-05, with an active set of 41 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,457:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=2.024e-05, with an active set of 41 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,457:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=2.024e-05, with an active set of 41 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,457:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=1.998e-05, with an active set of 41 regressors, and the smallest cholesky pivot element being 7.300e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,458:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=1.960e-05, with an active set of 41 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,458:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.462e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,458:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=1.934e-05, with an active set of 41 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,458:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=1.888e-05, with an active set of 41 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,458:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=1.887e-05, with an active set of 41 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,458:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=1.875e-05, with an active set of 41 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,458:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=1.791e-05, with an active set of 41 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,458:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=1.680e-05, with an active set of 41 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,458:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=1.405e-05, with an active set of 41 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,458:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=1.294e-05, with an active set of 41 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,459:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=1.131e-05, with an active set of 41 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,459:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=7.931e-06, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,460:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=7.728e-03, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,460:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=7.311e-03, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,461:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.069e-02, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,461:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=2.976e+14, with an active set of 49 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,461:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=3.928e-03, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,461:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=1.530e+14, with an active set of 49 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,461:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=7.580e+13, with an active set of 49 regressors, and the smallest cholesky pivot element being 9.424e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,462:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=3.656e-03, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,462:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=3.569e+13, with an active set of 49 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,462:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=3.454e+13, with an active set of 49 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,462:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=3.618e-03, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,462:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=2.869e+13, with an active set of 49 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,463:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=2.152e+13, with an active set of 49 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,463:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=1.828e-03, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,463:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=2.104e+13, with an active set of 49 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,463:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=2.034e+13, with an active set of 49 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,464:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=1.931e+13, with an active set of 49 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,464:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=1.761e-03, with an active set of 17 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,464:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=1.922e+13, with an active set of 49 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,464:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=1.702e-03, with an active set of 18 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,464:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=1.900e+13, with an active set of 49 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,465:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=1.644e-03, with an active set of 18 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,465:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=1.862e+13, with an active set of 49 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,465:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=1.676e+13, with an active set of 49 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,465:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=1.001e+13, with an active set of 49 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,466:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=8.463e-04, with an active set of 23 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,466:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=2.797e+12, with an active set of 49 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,466:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=2.797e+12, with an active set of 49 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,466:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=8.463e-04, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,467:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=8.458e-04, with an active set of 24 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,467:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=8.458e-04, with an active set of 25 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,468:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=8.458e-04, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,468:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=8.776e-04, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,469:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=8.125e-04, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,469:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=8.125e-04, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,470:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=7.828e-04, with an active set of 29 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,471:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=7.190e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,471:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=7.190e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,472:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=5.912e-04, with an active set of 33 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,473:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=5.392e-04, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,473:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=5.392e-04, with an active set of 35 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,473:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=5.392e-04, with an active set of 35 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,474:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=5.392e-04, with an active set of 35 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,476:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=3.597e-04, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,476:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=3.597e-04, with an active set of 38 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,478:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.562e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,478:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=3.592e-04, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,479:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.134e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,479:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=3.506e-04, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,479:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=3.362e-04, with an active set of 38 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,479:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=3.260e-04, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,479:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=3.260e-04, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,481:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=7.864e-03, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,481:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=7.760e-03, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,481:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=2.743e-04, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,482:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=2.743e-04, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,482:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=5.012e-03, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,482:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=2.743e-04, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,482:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=4.142e-03, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,483:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=2.736e-04, with an active set of 42 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,484:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=2.705e-04, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,484:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=2.399e-04, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,484:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=2.312e-04, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,484:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=2.106e-04, with an active set of 42 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,484:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=2.105e-04, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,485:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=1.898e-04, with an active set of 43 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,485:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=2.272e-03, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,485:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=2.188e-03, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,485:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.342e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,486:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=1.926e-03, with an active set of 14 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,486:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=1.752e-04, with an active set of 45 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,486:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.094e-02, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,487:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=1.647e-03, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,488:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=7.122e-03, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,488:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=1.086e-03, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,488:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=6.301e-03, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,488:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=1.073e-03, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,489:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=1.072e-03, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,489:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=4.643e-03, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,489:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=9.343e-04, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,490:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=3.114e-03, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,490:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=9.343e-04, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,490:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=2.373e-03, with an active set of 14 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,490:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=9.226e-04, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,491:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=2.373e-03, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,491:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=8.410e-04, with an active set of 19 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,492:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=1.634e-03, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,492:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=8.255e-04, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,492:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.223e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,492:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=8.255e-04, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,493:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=1.341e-03, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,493:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=8.254e-04, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,493:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=8.285e-03, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,493:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=1.329e-03, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,494:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=6.883e-03, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,495:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=5.583e-03, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,495:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=8.815e-04, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,495:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=8.815e-04, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,495:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=2.921e-03, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,496:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=2.759e-03, with an active set of 30 regressors, and the smallest cholesky pivot element being 5.576e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,496:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=8.430e-04, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,496:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=2.148e-03, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,496:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=2.737e-03, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,496:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=6.648e-04, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,496:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=2.614e-03, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,497:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=2.534e-03, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,497:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=1.802e-03, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,497:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=6.251e-04, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,498:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=5.851e-04, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,498:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=2.475e-03, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,498:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=1.685e-03, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,498:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.693e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,498:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=1.371e-03, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,498:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.693e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,499:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=5.277e-04, with an active set of 24 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,499:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=5.277e-04, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,499:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=1.244e-03, with an active set of 33 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,500:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=5.275e-04, with an active set of 24 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,500:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=2.304e-03, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,500:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=5.261e-04, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,500:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=2.180e-03, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,501:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=2.014e-03, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,501:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=1.232e-03, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,501:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=1.216e-03, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,501:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.803e-03, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,501:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=5.111e-04, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,501:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=1.211e-03, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,502:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=1.211e-03, with an active set of 35 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,502:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=5.099e-04, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,502:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.706e-03, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,502:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=5.099e-04, with an active set of 27 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,502:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=1.205e-03, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,502:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=1.016e-03, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,502:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=1.541e-03, with an active set of 25 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,503:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=5.098e-04, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,503:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=9.524e-04, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,503:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=1.207e-03, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,503:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=7.619e-04, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,503:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=8.818e-04, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,503:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=4.697e-04, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,504:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=7.275e-04, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,504:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=4.695e-04, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,504:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=8.633e-04, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,504:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=7.252e-04, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,504:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=5.454e-04, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,505:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=8.210e-04, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,505:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=7.727e-04, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,505:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=4.591e-04, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,505:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=7.503e-04, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,505:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=3.807e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,505:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=3.447e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,506:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=3.867e-04, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,506:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=3.851e-04, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,506:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=7.259e-04, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,506:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=2.880e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,506:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=3.645e-04, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,506:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=6.217e-04, with an active set of 28 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,506:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=2.879e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,507:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=2.878e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,507:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=3.027e-04, with an active set of 39 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,507:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=5.151e-04, with an active set of 29 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,508:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=2.986e-04, with an active set of 39 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,508:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=5.105e-04, with an active set of 29 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,508:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=2.844e-04, with an active set of 32 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,508:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=2.973e-04, with an active set of 39 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,508:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=2.828e-04, with an active set of 32 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,508:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=2.964e-04, with an active set of 39 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,508:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=2.820e-04, with an active set of 32 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,509:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=5.056e-04, with an active set of 29 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,509:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=2.654e-04, with an active set of 33 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,509:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=2.633e-04, with an active set of 33 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,509:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=2.642e-04, with an active set of 40 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,509:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=4.756e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,510:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=2.596e-04, with an active set of 40 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,510:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.909e-04, with an active set of 40 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,510:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=2.570e-04, with an active set of 33 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,510:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=2.512e-04, with an active set of 33 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,510:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=4.727e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,510:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=2.215e-04, with an active set of 33 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,511:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=1.513e-04, with an active set of 41 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,511:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=2.082e-04, with an active set of 34 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,511:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=4.245e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,511:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=1.479e-04, with an active set of 41 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,511:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=1.467e-04, with an active set of 41 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,511:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=2.080e-04, with an active set of 34 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,511:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=4.005e-04, with an active set of 32 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,511:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=3.921e-04, with an active set of 32 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,512:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=2.079e-04, with an active set of 34 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,512:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=3.929e-04, with an active set of 33 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,512:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=2.078e-04, with an active set of 34 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,512:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=1.971e-04, with an active set of 34 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,512:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=1.288e-04, with an active set of 43 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,512:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=3.837e-04, with an active set of 33 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,513:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=1.959e-04, with an active set of 34 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,513:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=3.711e-04, with an active set of 33 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,513:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=1.951e-04, with an active set of 34 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,513:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=3.493e-04, with an active set of 33 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,514:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=1.646e-04, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,514:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=4.903e-03, with an active set of 24 regressors, and the smallest cholesky pivot element being 7.300e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,514:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=3.170e-04, with an active set of 34 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,514:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=1.614e-04, with an active set of 35 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,514:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=1.163e-04, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,514:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=3.162e-04, with an active set of 34 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,514:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=1.541e-04, with an active set of 35 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,515:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=1.442e-04, with an active set of 35 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,515:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=4.718e-03, with an active set of 24 regressors, and the smallest cholesky pivot element being 7.300e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,515:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=1.409e-04, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,515:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=1.255e-04, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,515:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=1.155e-04, with an active set of 44 regressors, and the smallest cholesky pivot element being 9.424e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,515:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=4.229e-03, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,515:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=1.177e-04, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,515:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=1.123e-04, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,515:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=9.916e-05, with an active set of 35 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,515:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=2.927e-04, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,516:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=1.097e-04, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,516:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=6.854e-05, with an active set of 35 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,516:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=1.091e-04, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,516:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=3.790e-03, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,516:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=7.134e-05, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,516:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=9.201e-05, with an active set of 44 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,517:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=6.244e-05, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,517:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=8.393e-05, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,517:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=5.706e-05, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,517:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=5.166e-05, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,517:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=3.772e-03, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,517:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=5.456e-05, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,517:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=1.251e-03, with an active set of 40 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,518:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=4.439e-05, with an active set of 45 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,518:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=4.645e-05, with an active set of 37 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,518:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 51 iterations, i.e. alpha=3.783e-05, with an active set of 47 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,519:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=4.645e-05, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,519:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=3.783e-03, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,519:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=3.973e-05, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,519:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 51 iterations, i.e. alpha=3.764e-05, with an active set of 47 regressors, and the smallest cholesky pivot element being 5.576e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,519:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=3.455e-05, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,519:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 51 iterations, i.e. alpha=3.644e-05, with an active set of 47 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,520:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=2.683e-02, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,520:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=2.630e-02, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,520:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 56 iterations, i.e. alpha=1.331e-03, with an active set of 44 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,520:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=4.454e-05, with an active set of 50 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,521:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=2.396e-02, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,522:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 58 iterations, i.e. alpha=4.095e-05, with an active set of 54 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,522:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=2.397e-02, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,522:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 61 iterations, i.e. alpha=1.068e-03, with an active set of 49 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,522:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 58 iterations, i.e. alpha=3.872e-05, with an active set of 54 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,522:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 62 iterations, i.e. alpha=1.041e-03, with an active set of 50 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,522:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=2.390e-02, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,522:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 59 iterations, i.e. alpha=3.778e-05, with an active set of 55 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,523:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 62 iterations, i.e. alpha=8.506e-04, with an active set of 50 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,523:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=2.369e-02, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,523:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=6.456e-05, with an active set of 48 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,523:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 60 iterations, i.e. alpha=3.573e-05, with an active set of 56 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,523:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 63 iterations, i.e. alpha=7.699e-04, with an active set of 51 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,523:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 60 iterations, i.e. alpha=3.157e-05, with an active set of 56 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,524:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 60 iterations, i.e. alpha=2.205e-05, with an active set of 56 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,524:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 60 iterations, i.e. alpha=2.057e-05, with an active set of 56 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,525:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=3.868e-02, with an active set of 44 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,526:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=3.865e-02, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,533:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 78 iterations, i.e. alpha=1.262e+01, with an active set of 71 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,534:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 109 iterations, i.e. alpha=2.703e+08, with an active set of 84 regressors, and the smallest cholesky pivot element being 6.144e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:37:52,917:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-07-11 09:37:52,918:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-07-11 09:37:52,918:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-07-11 09:37:55,514:INFO:Calculating mean and std
2023-07-11 09:37:55,514:WARNING:C:\Users\didit\anaconda3\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)

2023-07-11 09:37:55,515:WARNING:C:\Users\didit\anaconda3\lib\site-packages\numpy\core\_methods.py:230: RuntimeWarning: invalid value encountered in subtract
  x = asanyarray(arr - arrmean)

2023-07-11 09:37:55,516:INFO:Creating metrics dataframe
2023-07-11 09:37:56,031:INFO:Uploading results into container
2023-07-11 09:37:56,032:INFO:Uploading model into container now
2023-07-11 09:37:56,032:INFO:_master_model_container: 5
2023-07-11 09:37:56,032:INFO:_display_container: 2
2023-07-11 09:37:56,032:INFO:Lars(random_state=5049)
2023-07-11 09:37:56,032:INFO:create_model() successfully completed......................................
2023-07-11 09:37:56,172:INFO:SubProcess create_model() end ==================================
2023-07-11 09:37:56,172:INFO:Creating metrics dataframe
2023-07-11 09:37:56,176:INFO:Initializing Lasso Least Angle Regression
2023-07-11 09:37:56,176:INFO:Total runtime is 0.3389849940935771 minutes
2023-07-11 09:37:56,176:INFO:SubProcess create_model() called ==================================
2023-07-11 09:37:56,176:INFO:Initializing create_model()
2023-07-11 09:37:56,176:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D786A8B0>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D78022E0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:37:56,176:INFO:Checking exceptions
2023-07-11 09:37:56,177:INFO:Importing libraries
2023-07-11 09:37:56,177:INFO:Copying training dataset
2023-07-11 09:37:56,180:INFO:Defining folds
2023-07-11 09:37:56,180:INFO:Declaring metric variables
2023-07-11 09:37:56,180:INFO:Importing untrained model
2023-07-11 09:37:56,180:INFO:Lasso Least Angle Regression Imported successfully
2023-07-11 09:37:56,180:INFO:Starting cross validation
2023-07-11 09:37:56,181:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:37:59,410:INFO:Calculating mean and std
2023-07-11 09:37:59,411:INFO:Creating metrics dataframe
2023-07-11 09:37:59,932:INFO:Uploading results into container
2023-07-11 09:37:59,933:INFO:Uploading model into container now
2023-07-11 09:37:59,933:INFO:_master_model_container: 6
2023-07-11 09:37:59,933:INFO:_display_container: 2
2023-07-11 09:37:59,934:INFO:LassoLars(random_state=5049)
2023-07-11 09:37:59,934:INFO:create_model() successfully completed......................................
2023-07-11 09:38:00,056:INFO:SubProcess create_model() end ==================================
2023-07-11 09:38:00,056:INFO:Creating metrics dataframe
2023-07-11 09:38:00,059:INFO:Initializing Orthogonal Matching Pursuit
2023-07-11 09:38:00,060:INFO:Total runtime is 0.403718364238739 minutes
2023-07-11 09:38:00,060:INFO:SubProcess create_model() called ==================================
2023-07-11 09:38:00,060:INFO:Initializing create_model()
2023-07-11 09:38:00,060:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D786A8B0>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D78022E0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:38:00,060:INFO:Checking exceptions
2023-07-11 09:38:00,060:INFO:Importing libraries
2023-07-11 09:38:00,060:INFO:Copying training dataset
2023-07-11 09:38:00,063:INFO:Defining folds
2023-07-11 09:38:00,063:INFO:Declaring metric variables
2023-07-11 09:38:00,063:INFO:Importing untrained model
2023-07-11 09:38:00,063:INFO:Orthogonal Matching Pursuit Imported successfully
2023-07-11 09:38:00,063:INFO:Starting cross validation
2023-07-11 09:38:00,064:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:38:03,284:INFO:Calculating mean and std
2023-07-11 09:38:03,285:INFO:Creating metrics dataframe
2023-07-11 09:38:03,811:INFO:Uploading results into container
2023-07-11 09:38:03,811:INFO:Uploading model into container now
2023-07-11 09:38:03,812:INFO:_master_model_container: 7
2023-07-11 09:38:03,812:INFO:_display_container: 2
2023-07-11 09:38:03,812:INFO:OrthogonalMatchingPursuit()
2023-07-11 09:38:03,812:INFO:create_model() successfully completed......................................
2023-07-11 09:38:03,942:INFO:SubProcess create_model() end ==================================
2023-07-11 09:38:03,943:INFO:Creating metrics dataframe
2023-07-11 09:38:03,946:INFO:Initializing Bayesian Ridge
2023-07-11 09:38:03,946:INFO:Total runtime is 0.4684937278429667 minutes
2023-07-11 09:38:03,946:INFO:SubProcess create_model() called ==================================
2023-07-11 09:38:03,946:INFO:Initializing create_model()
2023-07-11 09:38:03,946:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D786A8B0>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D78022E0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:38:03,946:INFO:Checking exceptions
2023-07-11 09:38:03,946:INFO:Importing libraries
2023-07-11 09:38:03,946:INFO:Copying training dataset
2023-07-11 09:38:03,950:INFO:Defining folds
2023-07-11 09:38:03,950:INFO:Declaring metric variables
2023-07-11 09:38:03,950:INFO:Importing untrained model
2023-07-11 09:38:03,950:INFO:Bayesian Ridge Imported successfully
2023-07-11 09:38:03,950:INFO:Starting cross validation
2023-07-11 09:38:03,951:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:38:07,245:INFO:Calculating mean and std
2023-07-11 09:38:07,246:INFO:Creating metrics dataframe
2023-07-11 09:38:07,741:INFO:Uploading results into container
2023-07-11 09:38:07,742:INFO:Uploading model into container now
2023-07-11 09:38:07,742:INFO:_master_model_container: 8
2023-07-11 09:38:07,742:INFO:_display_container: 2
2023-07-11 09:38:07,743:INFO:BayesianRidge()
2023-07-11 09:38:07,743:INFO:create_model() successfully completed......................................
2023-07-11 09:38:07,863:INFO:SubProcess create_model() end ==================================
2023-07-11 09:38:07,863:INFO:Creating metrics dataframe
2023-07-11 09:38:07,873:INFO:Initializing Passive Aggressive Regressor
2023-07-11 09:38:07,873:INFO:Total runtime is 0.5339474081993103 minutes
2023-07-11 09:38:07,874:INFO:SubProcess create_model() called ==================================
2023-07-11 09:38:07,874:INFO:Initializing create_model()
2023-07-11 09:38:07,874:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D786A8B0>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D78022E0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:38:07,874:INFO:Checking exceptions
2023-07-11 09:38:07,874:INFO:Importing libraries
2023-07-11 09:38:07,874:INFO:Copying training dataset
2023-07-11 09:38:07,879:INFO:Defining folds
2023-07-11 09:38:07,879:INFO:Declaring metric variables
2023-07-11 09:38:07,880:INFO:Importing untrained model
2023-07-11 09:38:07,880:INFO:Passive Aggressive Regressor Imported successfully
2023-07-11 09:38:07,880:INFO:Starting cross validation
2023-07-11 09:38:07,881:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:38:11,083:INFO:Calculating mean and std
2023-07-11 09:38:11,083:INFO:Creating metrics dataframe
2023-07-11 09:38:11,621:INFO:Uploading results into container
2023-07-11 09:38:11,621:INFO:Uploading model into container now
2023-07-11 09:38:11,622:INFO:_master_model_container: 9
2023-07-11 09:38:11,622:INFO:_display_container: 2
2023-07-11 09:38:11,622:INFO:PassiveAggressiveRegressor(random_state=5049)
2023-07-11 09:38:11,622:INFO:create_model() successfully completed......................................
2023-07-11 09:38:11,747:INFO:SubProcess create_model() end ==================================
2023-07-11 09:38:11,748:INFO:Creating metrics dataframe
2023-07-11 09:38:11,751:INFO:Initializing Huber Regressor
2023-07-11 09:38:11,751:INFO:Total runtime is 0.5985816319783529 minutes
2023-07-11 09:38:11,751:INFO:SubProcess create_model() called ==================================
2023-07-11 09:38:11,751:INFO:Initializing create_model()
2023-07-11 09:38:11,751:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D786A8B0>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D78022E0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:38:11,751:INFO:Checking exceptions
2023-07-11 09:38:11,752:INFO:Importing libraries
2023-07-11 09:38:11,752:INFO:Copying training dataset
2023-07-11 09:38:11,755:INFO:Defining folds
2023-07-11 09:38:11,755:INFO:Declaring metric variables
2023-07-11 09:38:11,755:INFO:Importing untrained model
2023-07-11 09:38:11,756:INFO:Huber Regressor Imported successfully
2023-07-11 09:38:11,756:INFO:Starting cross validation
2023-07-11 09:38:11,757:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:38:16,107:INFO:Calculating mean and std
2023-07-11 09:38:16,108:INFO:Creating metrics dataframe
2023-07-11 09:38:16,641:INFO:Uploading results into container
2023-07-11 09:38:16,642:INFO:Uploading model into container now
2023-07-11 09:38:16,642:INFO:_master_model_container: 10
2023-07-11 09:38:16,642:INFO:_display_container: 2
2023-07-11 09:38:16,642:INFO:HuberRegressor()
2023-07-11 09:38:16,642:INFO:create_model() successfully completed......................................
2023-07-11 09:38:16,763:INFO:SubProcess create_model() end ==================================
2023-07-11 09:38:16,763:INFO:Creating metrics dataframe
2023-07-11 09:38:16,766:INFO:Initializing K Neighbors Regressor
2023-07-11 09:38:16,766:INFO:Total runtime is 0.6821617881457012 minutes
2023-07-11 09:38:16,766:INFO:SubProcess create_model() called ==================================
2023-07-11 09:38:16,766:INFO:Initializing create_model()
2023-07-11 09:38:16,766:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D786A8B0>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D78022E0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:38:16,766:INFO:Checking exceptions
2023-07-11 09:38:16,766:INFO:Importing libraries
2023-07-11 09:38:16,766:INFO:Copying training dataset
2023-07-11 09:38:16,770:INFO:Defining folds
2023-07-11 09:38:16,770:INFO:Declaring metric variables
2023-07-11 09:38:16,770:INFO:Importing untrained model
2023-07-11 09:38:16,770:INFO:K Neighbors Regressor Imported successfully
2023-07-11 09:38:16,770:INFO:Starting cross validation
2023-07-11 09:38:16,771:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:38:19,976:INFO:Calculating mean and std
2023-07-11 09:38:19,977:INFO:Creating metrics dataframe
2023-07-11 09:38:20,529:INFO:Uploading results into container
2023-07-11 09:38:20,529:INFO:Uploading model into container now
2023-07-11 09:38:20,530:INFO:_master_model_container: 11
2023-07-11 09:38:20,530:INFO:_display_container: 2
2023-07-11 09:38:20,530:INFO:KNeighborsRegressor(n_jobs=-1)
2023-07-11 09:38:20,530:INFO:create_model() successfully completed......................................
2023-07-11 09:38:20,657:INFO:SubProcess create_model() end ==================================
2023-07-11 09:38:20,657:INFO:Creating metrics dataframe
2023-07-11 09:38:20,661:INFO:Initializing Decision Tree Regressor
2023-07-11 09:38:20,661:INFO:Total runtime is 0.7470675587654114 minutes
2023-07-11 09:38:20,661:INFO:SubProcess create_model() called ==================================
2023-07-11 09:38:20,661:INFO:Initializing create_model()
2023-07-11 09:38:20,661:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D786A8B0>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D78022E0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:38:20,661:INFO:Checking exceptions
2023-07-11 09:38:20,661:INFO:Importing libraries
2023-07-11 09:38:20,661:INFO:Copying training dataset
2023-07-11 09:38:20,665:INFO:Defining folds
2023-07-11 09:38:20,665:INFO:Declaring metric variables
2023-07-11 09:38:20,665:INFO:Importing untrained model
2023-07-11 09:38:20,665:INFO:Decision Tree Regressor Imported successfully
2023-07-11 09:38:20,666:INFO:Starting cross validation
2023-07-11 09:38:20,667:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:38:23,841:INFO:Calculating mean and std
2023-07-11 09:38:23,841:INFO:Creating metrics dataframe
2023-07-11 09:38:24,392:INFO:Uploading results into container
2023-07-11 09:38:24,392:INFO:Uploading model into container now
2023-07-11 09:38:24,393:INFO:_master_model_container: 12
2023-07-11 09:38:24,393:INFO:_display_container: 2
2023-07-11 09:38:24,393:INFO:DecisionTreeRegressor(random_state=5049)
2023-07-11 09:38:24,393:INFO:create_model() successfully completed......................................
2023-07-11 09:38:24,527:INFO:SubProcess create_model() end ==================================
2023-07-11 09:38:24,528:INFO:Creating metrics dataframe
2023-07-11 09:38:24,536:INFO:Initializing Random Forest Regressor
2023-07-11 09:38:24,536:INFO:Total runtime is 0.8116636912027995 minutes
2023-07-11 09:38:24,536:INFO:SubProcess create_model() called ==================================
2023-07-11 09:38:24,537:INFO:Initializing create_model()
2023-07-11 09:38:24,537:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D786A8B0>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D78022E0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:38:24,537:INFO:Checking exceptions
2023-07-11 09:38:24,537:INFO:Importing libraries
2023-07-11 09:38:24,537:INFO:Copying training dataset
2023-07-11 09:38:24,546:INFO:Defining folds
2023-07-11 09:38:24,546:INFO:Declaring metric variables
2023-07-11 09:38:24,546:INFO:Importing untrained model
2023-07-11 09:38:24,547:INFO:Random Forest Regressor Imported successfully
2023-07-11 09:38:24,547:INFO:Starting cross validation
2023-07-11 09:38:24,550:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:38:29,172:INFO:Calculating mean and std
2023-07-11 09:38:29,173:INFO:Creating metrics dataframe
2023-07-11 09:38:29,665:INFO:Uploading results into container
2023-07-11 09:38:29,665:INFO:Uploading model into container now
2023-07-11 09:38:29,666:INFO:_master_model_container: 13
2023-07-11 09:38:29,666:INFO:_display_container: 2
2023-07-11 09:38:29,666:INFO:RandomForestRegressor(n_jobs=-1, random_state=5049)
2023-07-11 09:38:29,666:INFO:create_model() successfully completed......................................
2023-07-11 09:38:29,814:INFO:SubProcess create_model() end ==================================
2023-07-11 09:38:29,814:INFO:Creating metrics dataframe
2023-07-11 09:38:29,821:INFO:Initializing Extra Trees Regressor
2023-07-11 09:38:29,822:INFO:Total runtime is 0.8997530380884806 minutes
2023-07-11 09:38:29,822:INFO:SubProcess create_model() called ==================================
2023-07-11 09:38:29,822:INFO:Initializing create_model()
2023-07-11 09:38:29,822:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D786A8B0>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D78022E0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:38:29,822:INFO:Checking exceptions
2023-07-11 09:38:29,822:INFO:Importing libraries
2023-07-11 09:38:29,822:INFO:Copying training dataset
2023-07-11 09:38:29,827:INFO:Defining folds
2023-07-11 09:38:29,827:INFO:Declaring metric variables
2023-07-11 09:38:29,827:INFO:Importing untrained model
2023-07-11 09:38:29,827:INFO:Extra Trees Regressor Imported successfully
2023-07-11 09:38:29,827:INFO:Starting cross validation
2023-07-11 09:38:29,828:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:38:34,357:INFO:Calculating mean and std
2023-07-11 09:38:34,358:INFO:Creating metrics dataframe
2023-07-11 09:38:34,913:INFO:Uploading results into container
2023-07-11 09:38:34,913:INFO:Uploading model into container now
2023-07-11 09:38:34,913:INFO:_master_model_container: 14
2023-07-11 09:38:34,913:INFO:_display_container: 2
2023-07-11 09:38:34,914:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=5049)
2023-07-11 09:38:34,914:INFO:create_model() successfully completed......................................
2023-07-11 09:38:35,056:INFO:SubProcess create_model() end ==================================
2023-07-11 09:38:35,057:INFO:Creating metrics dataframe
2023-07-11 09:38:35,067:INFO:Initializing AdaBoost Regressor
2023-07-11 09:38:35,067:INFO:Total runtime is 0.9871741851170858 minutes
2023-07-11 09:38:35,067:INFO:SubProcess create_model() called ==================================
2023-07-11 09:38:35,068:INFO:Initializing create_model()
2023-07-11 09:38:35,068:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D786A8B0>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D78022E0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:38:35,068:INFO:Checking exceptions
2023-07-11 09:38:35,068:INFO:Importing libraries
2023-07-11 09:38:35,068:INFO:Copying training dataset
2023-07-11 09:38:35,075:INFO:Defining folds
2023-07-11 09:38:35,075:INFO:Declaring metric variables
2023-07-11 09:38:35,075:INFO:Importing untrained model
2023-07-11 09:38:35,076:INFO:AdaBoost Regressor Imported successfully
2023-07-11 09:38:35,076:INFO:Starting cross validation
2023-07-11 09:38:35,077:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:38:38,985:INFO:Calculating mean and std
2023-07-11 09:38:38,985:INFO:Creating metrics dataframe
2023-07-11 09:38:39,478:INFO:Uploading results into container
2023-07-11 09:38:39,478:INFO:Uploading model into container now
2023-07-11 09:38:39,479:INFO:_master_model_container: 15
2023-07-11 09:38:39,479:INFO:_display_container: 2
2023-07-11 09:38:39,479:INFO:AdaBoostRegressor(random_state=5049)
2023-07-11 09:38:39,479:INFO:create_model() successfully completed......................................
2023-07-11 09:38:39,625:INFO:SubProcess create_model() end ==================================
2023-07-11 09:38:39,625:INFO:Creating metrics dataframe
2023-07-11 09:38:39,629:INFO:Initializing Gradient Boosting Regressor
2023-07-11 09:38:39,629:INFO:Total runtime is 1.0632063031196595 minutes
2023-07-11 09:38:39,629:INFO:SubProcess create_model() called ==================================
2023-07-11 09:38:39,629:INFO:Initializing create_model()
2023-07-11 09:38:39,629:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D786A8B0>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D78022E0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:38:39,629:INFO:Checking exceptions
2023-07-11 09:38:39,630:INFO:Importing libraries
2023-07-11 09:38:39,630:INFO:Copying training dataset
2023-07-11 09:38:39,632:INFO:Defining folds
2023-07-11 09:38:39,633:INFO:Declaring metric variables
2023-07-11 09:38:39,633:INFO:Importing untrained model
2023-07-11 09:38:39,633:INFO:Gradient Boosting Regressor Imported successfully
2023-07-11 09:38:39,633:INFO:Starting cross validation
2023-07-11 09:38:39,634:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:38:43,949:INFO:Calculating mean and std
2023-07-11 09:38:43,950:INFO:Creating metrics dataframe
2023-07-11 09:38:44,482:INFO:Uploading results into container
2023-07-11 09:38:44,482:INFO:Uploading model into container now
2023-07-11 09:38:44,483:INFO:_master_model_container: 16
2023-07-11 09:38:44,483:INFO:_display_container: 2
2023-07-11 09:38:44,483:INFO:GradientBoostingRegressor(random_state=5049)
2023-07-11 09:38:44,483:INFO:create_model() successfully completed......................................
2023-07-11 09:38:44,601:INFO:SubProcess create_model() end ==================================
2023-07-11 09:38:44,601:INFO:Creating metrics dataframe
2023-07-11 09:38:44,604:INFO:Initializing Extreme Gradient Boosting
2023-07-11 09:38:44,604:INFO:Total runtime is 1.1461261113484702 minutes
2023-07-11 09:38:44,604:INFO:SubProcess create_model() called ==================================
2023-07-11 09:38:44,605:INFO:Initializing create_model()
2023-07-11 09:38:44,605:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D786A8B0>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D78022E0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:38:44,605:INFO:Checking exceptions
2023-07-11 09:38:44,605:INFO:Importing libraries
2023-07-11 09:38:44,605:INFO:Copying training dataset
2023-07-11 09:38:44,608:INFO:Defining folds
2023-07-11 09:38:44,608:INFO:Declaring metric variables
2023-07-11 09:38:44,608:INFO:Importing untrained model
2023-07-11 09:38:44,608:INFO:Extreme Gradient Boosting Imported successfully
2023-07-11 09:38:44,608:INFO:Starting cross validation
2023-07-11 09:38:44,610:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:38:48,330:WARNING:create_model() for xgboost raised an exception or returned all 0.0, trying without fit_kwargs:
2023-07-11 09:38:48,330:WARNING:Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 792, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\core.py", line 620, in inner_f
    return func(**kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\sklearn.py", line 988, in fit
    train_dmatrix, evals = _wrap_evaluation_matrices(
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\sklearn.py", line 448, in _wrap_evaluation_matrices
    train_dmatrix = create_dmatrix(
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\sklearn.py", line 908, in _create_dmatrix
    return DMatrix(**kwargs, nthread=self.n_jobs)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\core.py", line 620, in inner_f
    return func(**kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\core.py", line 766, in __init__
    self.feature_names = feature_names
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\core.py", line 1155, in feature_names
    raise ValueError('feature_names must be unique')
ValueError: feature_names must be unique


2023-07-11 09:38:48,331:INFO:Initializing create_model()
2023-07-11 09:38:48,331:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D786A8B0>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D78022E0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:38:48,331:INFO:Checking exceptions
2023-07-11 09:38:48,331:INFO:Importing libraries
2023-07-11 09:38:48,331:INFO:Copying training dataset
2023-07-11 09:38:48,335:INFO:Defining folds
2023-07-11 09:38:48,335:INFO:Declaring metric variables
2023-07-11 09:38:48,335:INFO:Importing untrained model
2023-07-11 09:38:48,336:INFO:Extreme Gradient Boosting Imported successfully
2023-07-11 09:38:48,336:INFO:Starting cross validation
2023-07-11 09:38:48,337:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:38:52,313:ERROR:create_model() for xgboost raised an exception or returned all 0.0:
2023-07-11 09:38:52,314:ERROR:Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 792, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\core.py", line 620, in inner_f
    return func(**kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\sklearn.py", line 988, in fit
    train_dmatrix, evals = _wrap_evaluation_matrices(
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\sklearn.py", line 448, in _wrap_evaluation_matrices
    train_dmatrix = create_dmatrix(
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\sklearn.py", line 908, in _create_dmatrix
    return DMatrix(**kwargs, nthread=self.n_jobs)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\core.py", line 620, in inner_f
    return func(**kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\core.py", line 766, in __init__
    self.feature_names = feature_names
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\core.py", line 1155, in feature_names
    raise ValueError('feature_names must be unique')
ValueError: feature_names must be unique


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 808, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\core.py", line 620, in inner_f
    return func(**kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\sklearn.py", line 988, in fit
    train_dmatrix, evals = _wrap_evaluation_matrices(
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\sklearn.py", line 448, in _wrap_evaluation_matrices
    train_dmatrix = create_dmatrix(
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\sklearn.py", line 908, in _create_dmatrix
    return DMatrix(**kwargs, nthread=self.n_jobs)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\core.py", line 620, in inner_f
    return func(**kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\core.py", line 766, in __init__
    self.feature_names = feature_names
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\core.py", line 1155, in feature_names
    raise ValueError('feature_names must be unique')
ValueError: feature_names must be unique


2023-07-11 09:38:53,165:INFO:Initializing Light Gradient Boosting Machine
2023-07-11 09:38:53,165:INFO:Total runtime is 1.2888152837753297 minutes
2023-07-11 09:38:53,165:INFO:SubProcess create_model() called ==================================
2023-07-11 09:38:53,165:INFO:Initializing create_model()
2023-07-11 09:38:53,165:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D786A8B0>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D78022E0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:38:53,165:INFO:Checking exceptions
2023-07-11 09:38:53,165:INFO:Importing libraries
2023-07-11 09:38:53,165:INFO:Copying training dataset
2023-07-11 09:38:53,169:INFO:Defining folds
2023-07-11 09:38:53,169:INFO:Declaring metric variables
2023-07-11 09:38:53,169:INFO:Importing untrained model
2023-07-11 09:38:53,170:INFO:Light Gradient Boosting Machine Imported successfully
2023-07-11 09:38:53,170:INFO:Starting cross validation
2023-07-11 09:38:53,171:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:38:56,925:WARNING:create_model() for lightgbm raised an exception or returned all 0.0, trying without fit_kwargs:
2023-07-11 09:38:56,925:WARNING:Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 792, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\sklearn.py", line 895, in fit
    super().fit(X, y, sample_weight=sample_weight, init_score=init_score,
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\sklearn.py", line 748, in fit
    self._Booster = train(
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\engine.py", line 271, in train
    booster = Booster(params=params, train_set=train_set)
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 2605, in __init__
    train_set.construct()
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 1815, in construct
    self._lazy_init(self.data, label=self.label,
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 1573, in _lazy_init
    return self.set_feature_name(feature_name)
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 2142, in set_feature_name
    _safe_call(_LIB.LGBM_DatasetSetFeatureNames(
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 125, in _safe_call
    raise LightGBMError(_LIB.LGBM_GetLastError().decode('utf-8'))
lightgbm.basic.LightGBMError: Feature (salary_currency_USD) appears more than one time.


2023-07-11 09:38:56,925:INFO:Initializing create_model()
2023-07-11 09:38:56,925:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D786A8B0>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D78022E0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:38:56,926:INFO:Checking exceptions
2023-07-11 09:38:56,926:INFO:Importing libraries
2023-07-11 09:38:56,926:INFO:Copying training dataset
2023-07-11 09:38:56,931:INFO:Defining folds
2023-07-11 09:38:56,931:INFO:Declaring metric variables
2023-07-11 09:38:56,931:INFO:Importing untrained model
2023-07-11 09:38:56,932:INFO:Light Gradient Boosting Machine Imported successfully
2023-07-11 09:38:56,932:INFO:Starting cross validation
2023-07-11 09:38:56,933:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:39:00,858:ERROR:create_model() for lightgbm raised an exception or returned all 0.0:
2023-07-11 09:39:00,858:ERROR:Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 792, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\sklearn.py", line 895, in fit
    super().fit(X, y, sample_weight=sample_weight, init_score=init_score,
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\sklearn.py", line 748, in fit
    self._Booster = train(
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\engine.py", line 271, in train
    booster = Booster(params=params, train_set=train_set)
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 2605, in __init__
    train_set.construct()
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 1815, in construct
    self._lazy_init(self.data, label=self.label,
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 1573, in _lazy_init
    return self.set_feature_name(feature_name)
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 2142, in set_feature_name
    _safe_call(_LIB.LGBM_DatasetSetFeatureNames(
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 125, in _safe_call
    raise LightGBMError(_LIB.LGBM_GetLastError().decode('utf-8'))
lightgbm.basic.LightGBMError: Feature (salary_currency_USD) appears more than one time.


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 808, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\sklearn.py", line 895, in fit
    super().fit(X, y, sample_weight=sample_weight, init_score=init_score,
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\sklearn.py", line 748, in fit
    self._Booster = train(
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\engine.py", line 271, in train
    booster = Booster(params=params, train_set=train_set)
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 2605, in __init__
    train_set.construct()
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 1815, in construct
    self._lazy_init(self.data, label=self.label,
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 1573, in _lazy_init
    return self.set_feature_name(feature_name)
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 2142, in set_feature_name
    _safe_call(_LIB.LGBM_DatasetSetFeatureNames(
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 125, in _safe_call
    raise LightGBMError(_LIB.LGBM_GetLastError().decode('utf-8'))
lightgbm.basic.LightGBMError: Feature (salary_currency_USD) appears more than one time.


2023-07-11 09:39:01,716:INFO:Initializing Dummy Regressor
2023-07-11 09:39:01,716:INFO:Total runtime is 1.4313257137934368 minutes
2023-07-11 09:39:01,716:INFO:SubProcess create_model() called ==================================
2023-07-11 09:39:01,716:INFO:Initializing create_model()
2023-07-11 09:39:01,716:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D786A8B0>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D78022E0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:39:01,716:INFO:Checking exceptions
2023-07-11 09:39:01,716:INFO:Importing libraries
2023-07-11 09:39:01,716:INFO:Copying training dataset
2023-07-11 09:39:01,722:INFO:Defining folds
2023-07-11 09:39:01,722:INFO:Declaring metric variables
2023-07-11 09:39:01,722:INFO:Importing untrained model
2023-07-11 09:39:01,722:INFO:Dummy Regressor Imported successfully
2023-07-11 09:39:01,722:INFO:Starting cross validation
2023-07-11 09:39:01,724:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:39:04,993:INFO:Calculating mean and std
2023-07-11 09:39:04,993:INFO:Creating metrics dataframe
2023-07-11 09:39:05,529:INFO:Uploading results into container
2023-07-11 09:39:05,529:INFO:Uploading model into container now
2023-07-11 09:39:05,530:INFO:_master_model_container: 17
2023-07-11 09:39:05,530:INFO:_display_container: 2
2023-07-11 09:39:05,530:INFO:DummyRegressor()
2023-07-11 09:39:05,530:INFO:create_model() successfully completed......................................
2023-07-11 09:39:05,658:INFO:SubProcess create_model() end ==================================
2023-07-11 09:39:05,658:INFO:Creating metrics dataframe
2023-07-11 09:39:05,666:INFO:Initializing create_model()
2023-07-11 09:39:05,666:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D786A8B0>, estimator=GradientBoostingRegressor(random_state=5049), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:39:05,666:INFO:Checking exceptions
2023-07-11 09:39:05,666:INFO:Importing libraries
2023-07-11 09:39:05,666:INFO:Copying training dataset
2023-07-11 09:39:05,677:INFO:Defining folds
2023-07-11 09:39:05,678:INFO:Declaring metric variables
2023-07-11 09:39:05,678:INFO:Importing untrained model
2023-07-11 09:39:05,678:INFO:Declaring custom model
2023-07-11 09:39:05,679:INFO:Gradient Boosting Regressor Imported successfully
2023-07-11 09:39:05,681:INFO:Cross validation set to False
2023-07-11 09:39:05,681:INFO:Fitting Model
2023-07-11 09:39:06,622:INFO:GradientBoostingRegressor(random_state=5049)
2023-07-11 09:39:06,622:INFO:create_model() successfully completed......................................
2023-07-11 09:39:06,776:INFO:_master_model_container: 17
2023-07-11 09:39:06,776:INFO:_display_container: 2
2023-07-11 09:39:06,777:INFO:GradientBoostingRegressor(random_state=5049)
2023-07-11 09:39:06,777:INFO:compare_models() successfully completed......................................
2023-07-11 09:39:06,847:INFO:Initializing save_model()
2023-07-11 09:39:06,847:INFO:save_model(model=GradientBoostingRegressor(random_state=5049), model_name=best_model.pkl, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\didit\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['salary', 'salary_in_usd',
                                             'remote_ratio'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['experience_level',
                                             'employment_type', 'job_title',
                                             'salary_currency',
                                             'employee_residence',
                                             'compa...
                                    transformer=OneHotEncoder(cols=['experience_level',
                                                                    'employment_type',
                                                                    'salary_currency',
                                                                    'company_size'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('rest_encoding',
                 TransformerWrapper(include=['job_title', 'employee_residence',
                                             'company_location'],
                                    transformer=TargetEncoder(cols=['job_title',
                                                                    'employee_residence',
                                                                    'company_location'],
                                                              handle_missing='return_nan')))]), verbose=True, use_case=MLUsecase.REGRESSION, kwargs={})
2023-07-11 09:39:06,848:INFO:Adding model into prep_pipe
2023-07-11 09:39:06,860:INFO:best_model.pkl.pkl saved in current working directory
2023-07-11 09:39:06,867:INFO:Pipeline(memory=FastMemory(location=C:\Users\didit\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['salary', 'salary_in_usd',
                                             'remote_ratio'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['experience_level',
                                             'employment_type', 'job_title',
                                             'salary_currency',
                                             'employee_residence',
                                             'compa...
                                                                    'salary_currency',
                                                                    'company_size'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('rest_encoding',
                 TransformerWrapper(include=['job_title', 'employee_residence',
                                             'company_location'],
                                    transformer=TargetEncoder(cols=['job_title',
                                                                    'employee_residence',
                                                                    'company_location'],
                                                              handle_missing='return_nan'))),
                ('trained_model',
                 GradientBoostingRegressor(random_state=5049))])
2023-07-11 09:39:06,867:INFO:save_model() successfully completed......................................
2023-07-11 09:41:32,569:INFO:PyCaret ClassificationExperiment
2023-07-11 09:41:32,569:INFO:Logging name: clf-default-name
2023-07-11 09:41:32,569:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2023-07-11 09:41:32,569:INFO:version 3.0.2
2023-07-11 09:41:32,569:INFO:Initializing setup()
2023-07-11 09:41:32,569:INFO:self.USI: 3e0c
2023-07-11 09:41:32,569:INFO:self._variable_keys: {'_ml_usecase', 'log_plots_param', 'USI', 'X_train', 'pipeline', 'gpu_param', 'exp_name_log', 'y', 'is_multiclass', 'html_param', 'X', 'data', 'exp_id', 'y_train', 'target_param', 'fix_imbalance', 'X_test', 'fold_groups_param', 'n_jobs_param', 'idx', 'fold_generator', 'logging_param', 'memory', 'gpu_n_jobs_param', 'fold_shuffle_param', '_available_plots', 'seed', 'y_test'}
2023-07-11 09:41:32,569:INFO:Checking environment
2023-07-11 09:41:32,569:INFO:python_version: 3.9.13
2023-07-11 09:41:32,569:INFO:python_build: ('main', 'Oct 13 2022 21:23:06')
2023-07-11 09:41:32,569:INFO:machine: AMD64
2023-07-11 09:41:32,569:INFO:platform: Windows-10-10.0.22621-SP0
2023-07-11 09:41:32,569:INFO:Memory: svmem(total=16893358080, available=3152019456, percent=81.3, used=13741338624, free=3152019456)
2023-07-11 09:41:32,569:INFO:Physical Core: 8
2023-07-11 09:41:32,569:INFO:Logical Core: 16
2023-07-11 09:41:32,569:INFO:Checking libraries
2023-07-11 09:41:32,569:INFO:System:
2023-07-11 09:41:32,569:INFO:    python: 3.9.13 (main, Oct 13 2022, 21:23:06) [MSC v.1916 64 bit (AMD64)]
2023-07-11 09:41:32,569:INFO:executable: C:\Users\didit\anaconda3\python.exe
2023-07-11 09:41:32,569:INFO:   machine: Windows-10-10.0.22621-SP0
2023-07-11 09:41:32,569:INFO:PyCaret required dependencies:
2023-07-11 09:41:32,569:INFO:                 pip: 23.0.1
2023-07-11 09:41:32,569:INFO:          setuptools: 67.8.0
2023-07-11 09:41:32,569:INFO:             pycaret: 3.0.2
2023-07-11 09:41:32,569:INFO:             IPython: 8.12.0
2023-07-11 09:41:32,569:INFO:          ipywidgets: 8.0.4
2023-07-11 09:41:32,569:INFO:                tqdm: 4.65.0
2023-07-11 09:41:32,569:INFO:               numpy: 1.21.5
2023-07-11 09:41:32,569:INFO:              pandas: 1.5.3
2023-07-11 09:41:32,569:INFO:              jinja2: 3.1.2
2023-07-11 09:41:32,569:INFO:               scipy: 1.10.1
2023-07-11 09:41:32,569:INFO:              joblib: 1.2.0
2023-07-11 09:41:32,569:INFO:             sklearn: 1.2.2
2023-07-11 09:41:32,569:INFO:                pyod: 1.0.9
2023-07-11 09:41:32,569:INFO:            imblearn: 0.10.1
2023-07-11 09:41:32,569:INFO:   category_encoders: 2.6.1
2023-07-11 09:41:32,569:INFO:            lightgbm: 3.3.5
2023-07-11 09:41:32,569:INFO:               numba: 0.57.0
2023-07-11 09:41:32,569:INFO:            requests: 2.29.0
2023-07-11 09:41:32,569:INFO:          matplotlib: 3.7.1
2023-07-11 09:41:32,569:INFO:          scikitplot: 0.3.7
2023-07-11 09:41:32,569:INFO:         yellowbrick: 1.5
2023-07-11 09:41:32,569:INFO:              plotly: 5.9.0
2023-07-11 09:41:32,569:INFO:             kaleido: 0.2.1
2023-07-11 09:41:32,569:INFO:         statsmodels: 0.13.5
2023-07-11 09:41:32,569:INFO:              sktime: 0.17.0
2023-07-11 09:41:32,569:INFO:               tbats: 1.1.3
2023-07-11 09:41:32,569:INFO:            pmdarima: 2.0.3
2023-07-11 09:41:32,569:INFO:              psutil: 5.9.0
2023-07-11 09:41:32,569:INFO:PyCaret optional dependencies:
2023-07-11 09:41:32,569:INFO:                shap: 0.41.0
2023-07-11 09:41:32,569:INFO:           interpret: Not installed
2023-07-11 09:41:32,569:INFO:                umap: Not installed
2023-07-11 09:41:32,569:INFO:    pandas_profiling: 4.3.1
2023-07-11 09:41:32,569:INFO:  explainerdashboard: Not installed
2023-07-11 09:41:32,569:INFO:             autoviz: Not installed
2023-07-11 09:41:32,569:INFO:           fairlearn: Not installed
2023-07-11 09:41:32,569:INFO:             xgboost: 1.7.6
2023-07-11 09:41:32,569:INFO:            catboost: Not installed
2023-07-11 09:41:32,569:INFO:              kmodes: Not installed
2023-07-11 09:41:32,569:INFO:             mlxtend: Not installed
2023-07-11 09:41:32,569:INFO:       statsforecast: Not installed
2023-07-11 09:41:32,569:INFO:        tune_sklearn: Not installed
2023-07-11 09:41:32,569:INFO:                 ray: Not installed
2023-07-11 09:41:32,569:INFO:            hyperopt: Not installed
2023-07-11 09:41:32,569:INFO:              optuna: Not installed
2023-07-11 09:41:32,569:INFO:               skopt: Not installed
2023-07-11 09:41:32,569:INFO:              mlflow: 2.4.2
2023-07-11 09:41:32,569:INFO:              gradio: Not installed
2023-07-11 09:41:32,569:INFO:             fastapi: 0.95.2
2023-07-11 09:41:32,569:INFO:             uvicorn: 0.22.0
2023-07-11 09:41:32,569:INFO:              m2cgen: Not installed
2023-07-11 09:41:32,569:INFO:           evidently: Not installed
2023-07-11 09:41:32,569:INFO:               fugue: Not installed
2023-07-11 09:41:32,569:INFO:           streamlit: 1.23.1
2023-07-11 09:41:32,569:INFO:             prophet: Not installed
2023-07-11 09:41:32,569:INFO:None
2023-07-11 09:41:32,569:INFO:Set up data.
2023-07-11 09:41:32,585:INFO:Set up train/test split.
2023-07-11 09:41:32,592:INFO:Set up index.
2023-07-11 09:41:32,592:INFO:Set up folding strategy.
2023-07-11 09:41:32,592:INFO:Assigning column types.
2023-07-11 09:41:32,592:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-07-11 09:41:32,623:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 09:41:32,642:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-11 09:41:32,668:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:41:32,670:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:41:32,695:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 09:41:32,695:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-11 09:41:32,710:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:41:32,710:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:41:32,710:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-07-11 09:41:32,757:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-11 09:41:32,773:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:41:32,773:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:41:32,805:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-11 09:41:32,829:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:41:32,829:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:41:32,829:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2023-07-11 09:41:32,900:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:41:32,902:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:41:32,956:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:41:32,958:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:41:32,959:INFO:Preparing preprocessing pipeline...
2023-07-11 09:41:32,970:INFO:Set up label encoding.
2023-07-11 09:41:32,970:INFO:Set up simple imputation.
2023-07-11 09:41:32,973:INFO:Set up encoding of categorical features.
2023-07-11 09:41:33,094:INFO:Finished creating preprocessing pipeline.
2023-07-11 09:41:33,094:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\didit\AppData\Local\Temp\joblib),
         steps=[('label_encoding',
                 TransformerWrapperWithInverse(exclude=None, include=None,
                                               transformer=LabelEncoder())),
                ('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['salary', 'salary_in_usd',
                                             'remote_ratio'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fil...
                ('rest_encoding',
                 TransformerWrapper(exclude=None,
                                    include=['job_title', 'employee_residence',
                                             'company_location'],
                                    transformer=TargetEncoder(cols=['job_title',
                                                                    'employee_residence',
                                                                    'company_location'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              hierarchy=None,
                                                              min_samples_leaf=20,
                                                              return_df=True,
                                                              smoothing=10,
                                                              verbose=0)))],
         verbose=False)
2023-07-11 09:41:33,094:INFO:Creating final display dataframe.
2023-07-11 09:41:33,395:INFO:Setup _display_container:                     Description                               Value
0                    Session id                                2940
1                        Target                           work_year
2                   Target type                          Multiclass
3                Target mapping  2020: 0, 2021: 1, 2022: 2, 2023: 3
4           Original data shape                          (3755, 11)
5        Transformed data shape                         (3755, 170)
6   Transformed train set shape                         (2628, 170)
7    Transformed test set shape                         (1127, 170)
8              Numeric features                                   3
9          Categorical features                                   7
10                   Preprocess                                True
11              Imputation type                              simple
12           Numeric imputation                                mean
13       Categorical imputation                                mode
14     Maximum one-hot encoding                                  25
15              Encoding method                                None
16               Fold Generator                     StratifiedKFold
17                  Fold Number                                  10
18                     CPU Jobs                                  -1
19                      Use GPU                               False
20               Log Experiment                               False
21              Experiment Name                    clf-default-name
22                          USI                                3e0c
2023-07-11 09:41:33,462:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:41:33,462:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:41:33,529:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:41:33,529:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:41:33,529:INFO:setup() successfully completed in 1.26s...............
2023-07-11 09:41:33,536:INFO:Initializing compare_models()
2023-07-11 09:41:33,536:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E1D6CB1D60>, include=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000001E1D6CB1D60>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2023-07-11 09:41:33,536:INFO:Checking exceptions
2023-07-11 09:41:33,550:INFO:Preparing display monitor
2023-07-11 09:41:33,552:INFO:Initializing Logistic Regression
2023-07-11 09:41:33,552:INFO:Total runtime is 0.0 minutes
2023-07-11 09:41:33,552:INFO:SubProcess create_model() called ==================================
2023-07-11 09:41:33,552:INFO:Initializing create_model()
2023-07-11 09:41:33,552:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E1D6CB1D60>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D6F478E0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:41:33,552:INFO:Checking exceptions
2023-07-11 09:41:33,552:INFO:Importing libraries
2023-07-11 09:41:33,552:INFO:Copying training dataset
2023-07-11 09:41:33,555:INFO:Defining folds
2023-07-11 09:41:33,555:INFO:Declaring metric variables
2023-07-11 09:41:33,555:INFO:Importing untrained model
2023-07-11 09:41:33,555:INFO:Logistic Regression Imported successfully
2023-07-11 09:41:33,555:INFO:Starting cross validation
2023-07-11 09:41:33,556:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:41:34,161:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:34,164:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:34,166:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 09:41:34,167:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:34,348:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:34,351:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:34,352:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 09:41:34,354:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:34,370:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:34,373:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:34,374:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 09:41:34,375:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:34,385:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:34,388:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:34,389:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 09:41:34,390:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:34,402:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:34,404:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:34,405:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:34,406:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 09:41:34,406:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:34,407:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:34,408:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 09:41:34,408:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:34,422:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:34,424:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:34,426:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 09:41:34,427:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:34,431:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:34,433:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:34,434:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 09:41:34,435:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:34,441:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:34,443:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:34,445:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 09:41:34,445:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:34,446:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:34,446:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:34,446:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 09:41:34,446:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:37,273:INFO:Calculating mean and std
2023-07-11 09:41:37,273:INFO:Creating metrics dataframe
2023-07-11 09:41:37,895:INFO:Uploading results into container
2023-07-11 09:41:37,895:INFO:Uploading model into container now
2023-07-11 09:41:37,895:INFO:_master_model_container: 1
2023-07-11 09:41:37,895:INFO:_display_container: 2
2023-07-11 09:41:37,895:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=2940, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2023-07-11 09:41:37,895:INFO:create_model() successfully completed......................................
2023-07-11 09:41:38,021:INFO:SubProcess create_model() end ==================================
2023-07-11 09:41:38,021:INFO:Creating metrics dataframe
2023-07-11 09:41:38,038:INFO:Initializing K Neighbors Classifier
2023-07-11 09:41:38,038:INFO:Total runtime is 0.07476168076197307 minutes
2023-07-11 09:41:38,038:INFO:SubProcess create_model() called ==================================
2023-07-11 09:41:38,038:INFO:Initializing create_model()
2023-07-11 09:41:38,038:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E1D6CB1D60>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D6F478E0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:41:38,038:INFO:Checking exceptions
2023-07-11 09:41:38,038:INFO:Importing libraries
2023-07-11 09:41:38,038:INFO:Copying training dataset
2023-07-11 09:41:38,038:INFO:Defining folds
2023-07-11 09:41:38,038:INFO:Declaring metric variables
2023-07-11 09:41:38,038:INFO:Importing untrained model
2023-07-11 09:41:38,038:INFO:K Neighbors Classifier Imported successfully
2023-07-11 09:41:38,038:INFO:Starting cross validation
2023-07-11 09:41:38,045:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:41:38,678:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:38,678:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:38,678:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:38,694:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:38,694:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:38,694:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 09:41:38,694:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:38,725:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:38,725:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:38,725:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:38,741:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:38,741:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:38,741:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 09:41:38,741:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:38,741:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:38,741:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:38,756:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:38,756:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:38,756:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:38,756:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:38,756:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:38,756:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:38,756:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:38,756:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:38,772:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:38,772:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:38,776:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:38,776:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:38,776:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:38,776:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:38,776:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:38,776:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:41,680:INFO:Calculating mean and std
2023-07-11 09:41:41,680:INFO:Creating metrics dataframe
2023-07-11 09:41:42,295:INFO:Uploading results into container
2023-07-11 09:41:42,311:INFO:Uploading model into container now
2023-07-11 09:41:42,311:INFO:_master_model_container: 2
2023-07-11 09:41:42,311:INFO:_display_container: 2
2023-07-11 09:41:42,311:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2023-07-11 09:41:42,311:INFO:create_model() successfully completed......................................
2023-07-11 09:41:42,421:INFO:SubProcess create_model() end ==================================
2023-07-11 09:41:42,421:INFO:Creating metrics dataframe
2023-07-11 09:41:42,421:INFO:Initializing Naive Bayes
2023-07-11 09:41:42,421:INFO:Total runtime is 0.1478066682815552 minutes
2023-07-11 09:41:42,421:INFO:SubProcess create_model() called ==================================
2023-07-11 09:41:42,421:INFO:Initializing create_model()
2023-07-11 09:41:42,421:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E1D6CB1D60>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D6F478E0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:41:42,421:INFO:Checking exceptions
2023-07-11 09:41:42,421:INFO:Importing libraries
2023-07-11 09:41:42,421:INFO:Copying training dataset
2023-07-11 09:41:42,421:INFO:Defining folds
2023-07-11 09:41:42,421:INFO:Declaring metric variables
2023-07-11 09:41:42,421:INFO:Importing untrained model
2023-07-11 09:41:42,421:INFO:Naive Bayes Imported successfully
2023-07-11 09:41:42,421:INFO:Starting cross validation
2023-07-11 09:41:42,421:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:41:42,896:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:42,896:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:42,896:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 09:41:42,896:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:42,927:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:42,927:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:42,927:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:42,958:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:42,958:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:42,958:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 09:41:42,958:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:42,974:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:42,974:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:42,982:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 09:41:42,982:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:42,990:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:43,006:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:43,006:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 09:41:43,006:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:43,006:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:43,006:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:43,006:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 09:41:43,006:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:43,021:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:43,021:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:43,021:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:43,038:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:43,038:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:43,045:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 09:41:43,046:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:43,065:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:43,067:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:43,068:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 09:41:43,068:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:43,069:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:43,070:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:43,071:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 09:41:43,072:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:45,988:INFO:Calculating mean and std
2023-07-11 09:41:45,988:INFO:Creating metrics dataframe
2023-07-11 09:41:46,558:INFO:Uploading results into container
2023-07-11 09:41:46,558:INFO:Uploading model into container now
2023-07-11 09:41:46,558:INFO:_master_model_container: 3
2023-07-11 09:41:46,558:INFO:_display_container: 2
2023-07-11 09:41:46,558:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2023-07-11 09:41:46,558:INFO:create_model() successfully completed......................................
2023-07-11 09:41:46,710:INFO:SubProcess create_model() end ==================================
2023-07-11 09:41:46,710:INFO:Creating metrics dataframe
2023-07-11 09:41:46,725:INFO:Initializing Decision Tree Classifier
2023-07-11 09:41:46,725:INFO:Total runtime is 0.21955110232035321 minutes
2023-07-11 09:41:46,725:INFO:SubProcess create_model() called ==================================
2023-07-11 09:41:46,725:INFO:Initializing create_model()
2023-07-11 09:41:46,725:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E1D6CB1D60>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D6F478E0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:41:46,725:INFO:Checking exceptions
2023-07-11 09:41:46,725:INFO:Importing libraries
2023-07-11 09:41:46,725:INFO:Copying training dataset
2023-07-11 09:41:46,725:INFO:Defining folds
2023-07-11 09:41:46,725:INFO:Declaring metric variables
2023-07-11 09:41:46,725:INFO:Importing untrained model
2023-07-11 09:41:46,725:INFO:Decision Tree Classifier Imported successfully
2023-07-11 09:41:46,725:INFO:Starting cross validation
2023-07-11 09:41:46,725:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:41:47,242:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:47,244:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:47,245:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:47,270:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:47,272:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:47,273:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:47,295:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:47,295:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:47,295:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:47,295:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:47,311:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:47,311:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:47,311:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:47,311:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:47,311:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:47,311:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:47,311:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:47,311:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:47,326:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:47,326:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:47,326:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:47,342:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:47,342:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:47,342:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:47,358:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:47,358:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:47,358:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:47,373:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:47,373:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:47,389:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:50,326:INFO:Calculating mean and std
2023-07-11 09:41:50,326:INFO:Creating metrics dataframe
2023-07-11 09:41:50,941:INFO:Uploading results into container
2023-07-11 09:41:50,941:INFO:Uploading model into container now
2023-07-11 09:41:50,941:INFO:_master_model_container: 4
2023-07-11 09:41:50,941:INFO:_display_container: 2
2023-07-11 09:41:50,941:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=2940, splitter='best')
2023-07-11 09:41:50,941:INFO:create_model() successfully completed......................................
2023-07-11 09:41:51,071:INFO:SubProcess create_model() end ==================================
2023-07-11 09:41:51,072:INFO:Creating metrics dataframe
2023-07-11 09:41:51,075:INFO:Initializing SVM - Linear Kernel
2023-07-11 09:41:51,075:INFO:Total runtime is 0.29204766352971395 minutes
2023-07-11 09:41:51,075:INFO:SubProcess create_model() called ==================================
2023-07-11 09:41:51,075:INFO:Initializing create_model()
2023-07-11 09:41:51,075:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E1D6CB1D60>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D6F478E0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:41:51,075:INFO:Checking exceptions
2023-07-11 09:41:51,075:INFO:Importing libraries
2023-07-11 09:41:51,075:INFO:Copying training dataset
2023-07-11 09:41:51,079:INFO:Defining folds
2023-07-11 09:41:51,079:INFO:Declaring metric variables
2023-07-11 09:41:51,079:INFO:Importing untrained model
2023-07-11 09:41:51,079:INFO:SVM - Linear Kernel Imported successfully
2023-07-11 09:41:51,079:INFO:Starting cross validation
2023-07-11 09:41:51,081:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:41:51,495:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-11 09:41:51,495:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:51,495:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:51,495:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 09:41:51,495:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:51,511:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-11 09:41:51,511:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-11 09:41:51,511:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:51,511:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:51,511:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:51,511:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 09:41:51,511:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:51,511:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:51,511:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 09:41:51,511:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:51,526:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-11 09:41:51,526:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:51,526:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:51,526:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 09:41:51,526:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:51,574:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-11 09:41:51,574:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:51,589:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:51,589:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 09:41:51,589:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:51,605:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-11 09:41:51,605:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:51,605:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:51,605:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 09:41:51,605:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:51,621:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-11 09:41:51,621:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:51,621:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:51,621:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 09:41:51,621:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:51,621:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-11 09:41:51,621:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:51,621:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:51,621:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 09:41:51,621:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:51,638:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-11 09:41:51,638:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:51,646:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:51,647:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 09:41:51,648:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:51,670:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-11 09:41:51,671:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:51,673:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:51,674:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 09:41:51,675:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:54,649:INFO:Calculating mean and std
2023-07-11 09:41:54,649:INFO:Creating metrics dataframe
2023-07-11 09:41:55,245:INFO:Uploading results into container
2023-07-11 09:41:55,245:INFO:Uploading model into container now
2023-07-11 09:41:55,246:INFO:_master_model_container: 5
2023-07-11 09:41:55,246:INFO:_display_container: 2
2023-07-11 09:41:55,246:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=2940, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2023-07-11 09:41:55,246:INFO:create_model() successfully completed......................................
2023-07-11 09:41:55,383:INFO:SubProcess create_model() end ==================================
2023-07-11 09:41:55,384:INFO:Creating metrics dataframe
2023-07-11 09:41:55,384:INFO:Initializing Ridge Classifier
2023-07-11 09:41:55,384:INFO:Total runtime is 0.36385866403579714 minutes
2023-07-11 09:41:55,384:INFO:SubProcess create_model() called ==================================
2023-07-11 09:41:55,384:INFO:Initializing create_model()
2023-07-11 09:41:55,384:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E1D6CB1D60>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D6F478E0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:41:55,384:INFO:Checking exceptions
2023-07-11 09:41:55,384:INFO:Importing libraries
2023-07-11 09:41:55,384:INFO:Copying training dataset
2023-07-11 09:41:55,384:INFO:Defining folds
2023-07-11 09:41:55,384:INFO:Declaring metric variables
2023-07-11 09:41:55,384:INFO:Importing untrained model
2023-07-11 09:41:55,384:INFO:Ridge Classifier Imported successfully
2023-07-11 09:41:55,384:INFO:Starting cross validation
2023-07-11 09:41:55,395:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:41:55,814:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-11 09:41:55,815:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:55,818:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:55,819:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 09:41:55,820:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:55,829:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-11 09:41:55,829:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:55,829:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:55,844:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 09:41:55,845:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:55,848:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-11 09:41:55,849:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:55,851:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:55,852:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 09:41:55,853:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:55,855:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-11 09:41:55,856:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:55,858:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:55,859:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 09:41:55,861:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:55,867:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-11 09:41:55,868:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:55,871:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:55,872:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 09:41:55,873:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:55,874:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-11 09:41:55,875:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:55,876:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-11 09:41:55,877:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:55,877:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:55,878:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 09:41:55,879:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:55,879:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:55,880:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 09:41:55,881:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:55,896:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-11 09:41:55,897:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:55,899:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:55,900:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 09:41:55,902:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:55,911:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-11 09:41:55,912:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:55,915:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:55,916:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 09:41:55,917:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:55,933:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-11 09:41:55,934:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:55,936:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:55,937:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 09:41:55,938:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:41:58,926:INFO:Calculating mean and std
2023-07-11 09:41:58,926:INFO:Creating metrics dataframe
2023-07-11 09:41:59,558:INFO:Uploading results into container
2023-07-11 09:41:59,558:INFO:Uploading model into container now
2023-07-11 09:41:59,558:INFO:_master_model_container: 6
2023-07-11 09:41:59,558:INFO:_display_container: 2
2023-07-11 09:41:59,558:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=2940, solver='auto',
                tol=0.0001)
2023-07-11 09:41:59,558:INFO:create_model() successfully completed......................................
2023-07-11 09:41:59,695:INFO:SubProcess create_model() end ==================================
2023-07-11 09:41:59,695:INFO:Creating metrics dataframe
2023-07-11 09:41:59,695:INFO:Initializing Random Forest Classifier
2023-07-11 09:41:59,695:INFO:Total runtime is 0.4357089678446452 minutes
2023-07-11 09:41:59,695:INFO:SubProcess create_model() called ==================================
2023-07-11 09:41:59,695:INFO:Initializing create_model()
2023-07-11 09:41:59,695:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E1D6CB1D60>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D6F478E0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:41:59,695:INFO:Checking exceptions
2023-07-11 09:41:59,695:INFO:Importing libraries
2023-07-11 09:41:59,695:INFO:Copying training dataset
2023-07-11 09:41:59,707:INFO:Defining folds
2023-07-11 09:41:59,707:INFO:Declaring metric variables
2023-07-11 09:41:59,707:INFO:Importing untrained model
2023-07-11 09:41:59,707:INFO:Random Forest Classifier Imported successfully
2023-07-11 09:41:59,707:INFO:Starting cross validation
2023-07-11 09:41:59,711:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:42:01,246:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:01,246:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:01,261:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:01,343:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:01,343:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:01,359:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:01,375:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:01,390:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:01,390:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:01,428:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:01,430:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:01,430:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:01,447:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:01,447:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:01,447:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:01,447:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:01,447:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:01,462:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:01,464:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:01,464:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:01,464:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:01,464:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:01,464:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:01,464:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:01,481:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:01,481:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:01,481:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:01,556:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:01,558:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:01,560:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:04,616:INFO:Calculating mean and std
2023-07-11 09:42:04,616:INFO:Creating metrics dataframe
2023-07-11 09:42:05,302:INFO:Uploading results into container
2023-07-11 09:42:05,303:INFO:Uploading model into container now
2023-07-11 09:42:05,304:INFO:_master_model_container: 7
2023-07-11 09:42:05,304:INFO:_display_container: 2
2023-07-11 09:42:05,305:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=2940, verbose=0, warm_start=False)
2023-07-11 09:42:05,305:INFO:create_model() successfully completed......................................
2023-07-11 09:42:05,440:INFO:SubProcess create_model() end ==================================
2023-07-11 09:42:05,440:INFO:Creating metrics dataframe
2023-07-11 09:42:05,440:INFO:Initializing Quadratic Discriminant Analysis
2023-07-11 09:42:05,440:INFO:Total runtime is 0.5314557313919067 minutes
2023-07-11 09:42:05,440:INFO:SubProcess create_model() called ==================================
2023-07-11 09:42:05,440:INFO:Initializing create_model()
2023-07-11 09:42:05,440:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E1D6CB1D60>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D6F478E0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:42:05,440:INFO:Checking exceptions
2023-07-11 09:42:05,440:INFO:Importing libraries
2023-07-11 09:42:05,440:INFO:Copying training dataset
2023-07-11 09:42:05,445:INFO:Defining folds
2023-07-11 09:42:05,446:INFO:Declaring metric variables
2023-07-11 09:42:05,446:INFO:Importing untrained model
2023-07-11 09:42:05,446:INFO:Quadratic Discriminant Analysis Imported successfully
2023-07-11 09:42:05,446:INFO:Starting cross validation
2023-07-11 09:42:05,447:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:42:05,728:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-07-11 09:42:05,728:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-07-11 09:42:05,728:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-07-11 09:42:05,728:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-07-11 09:42:05,728:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-07-11 09:42:05,743:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-07-11 09:42:05,743:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-07-11 09:42:05,759:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-07-11 09:42:05,774:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-07-11 09:42:05,774:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-07-11 09:42:06,020:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:06,020:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:06,020:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 09:42:06,020:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:06,037:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:06,039:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:06,041:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 09:42:06,042:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:06,046:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:06,046:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:06,046:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 09:42:06,046:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:06,063:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:06,064:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:06,065:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 09:42:06,066:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:06,076:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:06,078:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:06,080:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:06,118:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:06,118:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:06,118:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:06,118:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:06,118:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 09:42:06,118:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 09:42:06,118:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:06,118:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:06,118:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:06,118:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:06,118:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:06,128:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 09:42:06,128:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:06,128:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:06,128:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:06,169:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:06,171:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:06,172:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 09:42:06,172:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:09,269:INFO:Calculating mean and std
2023-07-11 09:42:09,270:INFO:Creating metrics dataframe
2023-07-11 09:42:09,868:INFO:Uploading results into container
2023-07-11 09:42:09,869:INFO:Uploading model into container now
2023-07-11 09:42:09,869:INFO:_master_model_container: 8
2023-07-11 09:42:09,869:INFO:_display_container: 2
2023-07-11 09:42:09,869:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2023-07-11 09:42:09,869:INFO:create_model() successfully completed......................................
2023-07-11 09:42:09,989:INFO:SubProcess create_model() end ==================================
2023-07-11 09:42:09,989:INFO:Creating metrics dataframe
2023-07-11 09:42:09,989:INFO:Initializing Ada Boost Classifier
2023-07-11 09:42:09,989:INFO:Total runtime is 0.60727934439977 minutes
2023-07-11 09:42:09,989:INFO:SubProcess create_model() called ==================================
2023-07-11 09:42:09,989:INFO:Initializing create_model()
2023-07-11 09:42:09,989:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E1D6CB1D60>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D6F478E0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:42:09,989:INFO:Checking exceptions
2023-07-11 09:42:09,989:INFO:Importing libraries
2023-07-11 09:42:09,989:INFO:Copying training dataset
2023-07-11 09:42:10,005:INFO:Defining folds
2023-07-11 09:42:10,005:INFO:Declaring metric variables
2023-07-11 09:42:10,005:INFO:Importing untrained model
2023-07-11 09:42:10,005:INFO:Ada Boost Classifier Imported successfully
2023-07-11 09:42:10,005:INFO:Starting cross validation
2023-07-11 09:42:10,005:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:42:11,443:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:11,445:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:11,446:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:11,446:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:11,446:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:11,446:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:11,480:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:11,480:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:11,480:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:11,512:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:11,514:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:11,514:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:11,530:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:11,530:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:11,530:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:11,546:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:11,548:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:11,548:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:11,548:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:11,548:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:11,548:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:11,548:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:11,548:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:11,548:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:11,592:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:11,594:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:11,596:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:11,620:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:11,623:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:11,625:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:14,758:INFO:Calculating mean and std
2023-07-11 09:42:14,774:INFO:Creating metrics dataframe
2023-07-11 09:42:15,358:INFO:Uploading results into container
2023-07-11 09:42:15,358:INFO:Uploading model into container now
2023-07-11 09:42:15,358:INFO:_master_model_container: 9
2023-07-11 09:42:15,358:INFO:_display_container: 2
2023-07-11 09:42:15,358:INFO:AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=2940)
2023-07-11 09:42:15,358:INFO:create_model() successfully completed......................................
2023-07-11 09:42:15,490:INFO:SubProcess create_model() end ==================================
2023-07-11 09:42:15,490:INFO:Creating metrics dataframe
2023-07-11 09:42:15,495:INFO:Initializing Gradient Boosting Classifier
2023-07-11 09:42:15,495:INFO:Total runtime is 0.6990482886632283 minutes
2023-07-11 09:42:15,495:INFO:SubProcess create_model() called ==================================
2023-07-11 09:42:15,495:INFO:Initializing create_model()
2023-07-11 09:42:15,495:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E1D6CB1D60>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D6F478E0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:42:15,495:INFO:Checking exceptions
2023-07-11 09:42:15,495:INFO:Importing libraries
2023-07-11 09:42:15,495:INFO:Copying training dataset
2023-07-11 09:42:15,495:INFO:Defining folds
2023-07-11 09:42:15,495:INFO:Declaring metric variables
2023-07-11 09:42:15,495:INFO:Importing untrained model
2023-07-11 09:42:15,495:INFO:Gradient Boosting Classifier Imported successfully
2023-07-11 09:42:15,495:INFO:Starting cross validation
2023-07-11 09:42:15,495:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:42:20,406:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:20,406:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:20,422:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:20,589:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:20,589:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:20,589:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 09:42:20,604:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:20,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:20,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:20,680:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:20,695:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:20,695:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:20,695:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:20,804:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:20,804:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:20,804:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:20,837:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:20,838:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:20,840:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:20,873:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:20,876:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:20,877:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 09:42:20,878:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:20,894:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:20,896:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:20,896:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:20,896:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:20,896:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:20,896:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 09:42:20,896:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:21,127:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:21,143:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:21,143:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:24,592:INFO:Calculating mean and std
2023-07-11 09:42:24,592:INFO:Creating metrics dataframe
2023-07-11 09:42:25,221:INFO:Uploading results into container
2023-07-11 09:42:25,221:INFO:Uploading model into container now
2023-07-11 09:42:25,221:INFO:_master_model_container: 10
2023-07-11 09:42:25,221:INFO:_display_container: 2
2023-07-11 09:42:25,221:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=2940, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2023-07-11 09:42:25,221:INFO:create_model() successfully completed......................................
2023-07-11 09:42:25,358:INFO:SubProcess create_model() end ==================================
2023-07-11 09:42:25,358:INFO:Creating metrics dataframe
2023-07-11 09:42:25,358:INFO:Initializing Linear Discriminant Analysis
2023-07-11 09:42:25,358:INFO:Total runtime is 0.863434366385142 minutes
2023-07-11 09:42:25,358:INFO:SubProcess create_model() called ==================================
2023-07-11 09:42:25,358:INFO:Initializing create_model()
2023-07-11 09:42:25,358:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E1D6CB1D60>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D6F478E0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:42:25,358:INFO:Checking exceptions
2023-07-11 09:42:25,358:INFO:Importing libraries
2023-07-11 09:42:25,358:INFO:Copying training dataset
2023-07-11 09:42:25,374:INFO:Defining folds
2023-07-11 09:42:25,374:INFO:Declaring metric variables
2023-07-11 09:42:25,374:INFO:Importing untrained model
2023-07-11 09:42:25,374:INFO:Linear Discriminant Analysis Imported successfully
2023-07-11 09:42:25,374:INFO:Starting cross validation
2023-07-11 09:42:25,374:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:42:26,179:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:26,179:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:26,179:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:26,221:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:26,221:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:26,238:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:26,238:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:26,240:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:26,240:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:26,242:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:26,242:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:26,243:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:26,245:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:26,246:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:26,246:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:26,272:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:26,274:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:26,275:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:26,288:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:26,290:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:26,293:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:26,312:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:26,312:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:26,312:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:26,328:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:26,328:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:26,328:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:26,353:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:26,353:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:26,353:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:29,606:INFO:Calculating mean and std
2023-07-11 09:42:29,606:INFO:Creating metrics dataframe
2023-07-11 09:42:30,444:INFO:Uploading results into container
2023-07-11 09:42:30,444:INFO:Uploading model into container now
2023-07-11 09:42:30,445:INFO:_master_model_container: 11
2023-07-11 09:42:30,445:INFO:_display_container: 2
2023-07-11 09:42:30,445:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2023-07-11 09:42:30,445:INFO:create_model() successfully completed......................................
2023-07-11 09:42:30,560:INFO:SubProcess create_model() end ==================================
2023-07-11 09:42:30,560:INFO:Creating metrics dataframe
2023-07-11 09:42:30,560:INFO:Initializing Extra Trees Classifier
2023-07-11 09:42:30,560:INFO:Total runtime is 0.9501304189364116 minutes
2023-07-11 09:42:30,560:INFO:SubProcess create_model() called ==================================
2023-07-11 09:42:30,560:INFO:Initializing create_model()
2023-07-11 09:42:30,560:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E1D6CB1D60>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D6F478E0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:42:30,560:INFO:Checking exceptions
2023-07-11 09:42:30,560:INFO:Importing libraries
2023-07-11 09:42:30,560:INFO:Copying training dataset
2023-07-11 09:42:30,575:INFO:Defining folds
2023-07-11 09:42:30,575:INFO:Declaring metric variables
2023-07-11 09:42:30,575:INFO:Importing untrained model
2023-07-11 09:42:30,575:INFO:Extra Trees Classifier Imported successfully
2023-07-11 09:42:30,575:INFO:Starting cross validation
2023-07-11 09:42:30,575:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:42:32,129:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:32,129:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:32,144:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:32,176:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:32,176:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:32,176:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:32,207:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:32,207:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:32,207:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:32,237:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:32,240:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:32,240:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:32,247:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:32,249:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:32,251:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:32,252:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:32,254:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:32,256:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:32,259:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:32,262:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:32,264:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:32,405:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:32,405:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:32,405:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:32,405:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:32,420:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:32,420:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:32,479:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:32,479:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:32,479:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:35,712:INFO:Calculating mean and std
2023-07-11 09:42:35,712:INFO:Creating metrics dataframe
2023-07-11 09:42:36,389:INFO:Uploading results into container
2023-07-11 09:42:36,389:INFO:Uploading model into container now
2023-07-11 09:42:36,389:INFO:_master_model_container: 12
2023-07-11 09:42:36,389:INFO:_display_container: 2
2023-07-11 09:42:36,389:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=2940, verbose=0, warm_start=False)
2023-07-11 09:42:36,389:INFO:create_model() successfully completed......................................
2023-07-11 09:42:36,528:INFO:SubProcess create_model() end ==================================
2023-07-11 09:42:36,528:INFO:Creating metrics dataframe
2023-07-11 09:42:36,528:INFO:Initializing Extreme Gradient Boosting
2023-07-11 09:42:36,528:INFO:Total runtime is 1.0495950977007549 minutes
2023-07-11 09:42:36,528:INFO:SubProcess create_model() called ==================================
2023-07-11 09:42:36,528:INFO:Initializing create_model()
2023-07-11 09:42:36,528:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E1D6CB1D60>, estimator=xgboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D6F478E0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:42:36,528:INFO:Checking exceptions
2023-07-11 09:42:36,528:INFO:Importing libraries
2023-07-11 09:42:36,528:INFO:Copying training dataset
2023-07-11 09:42:36,544:INFO:Defining folds
2023-07-11 09:42:36,544:INFO:Declaring metric variables
2023-07-11 09:42:36,544:INFO:Importing untrained model
2023-07-11 09:42:36,544:INFO:Extreme Gradient Boosting Imported successfully
2023-07-11 09:42:36,544:INFO:Starting cross validation
2023-07-11 09:42:36,544:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:42:40,806:WARNING:create_model() for xgboost raised an exception or returned all 0.0, trying without fit_kwargs:
2023-07-11 09:42:40,806:WARNING:Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 792, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\core.py", line 620, in inner_f
    return func(**kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\sklearn.py", line 1471, in fit
    train_dmatrix, evals = _wrap_evaluation_matrices(
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\sklearn.py", line 448, in _wrap_evaluation_matrices
    train_dmatrix = create_dmatrix(
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\sklearn.py", line 908, in _create_dmatrix
    return DMatrix(**kwargs, nthread=self.n_jobs)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\core.py", line 620, in inner_f
    return func(**kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\core.py", line 766, in __init__
    self.feature_names = feature_names
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\core.py", line 1155, in feature_names
    raise ValueError('feature_names must be unique')
ValueError: feature_names must be unique


2023-07-11 09:42:40,821:INFO:Initializing create_model()
2023-07-11 09:42:40,821:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E1D6CB1D60>, estimator=xgboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D6F478E0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:42:40,821:INFO:Checking exceptions
2023-07-11 09:42:40,821:INFO:Importing libraries
2023-07-11 09:42:40,821:INFO:Copying training dataset
2023-07-11 09:42:40,840:INFO:Defining folds
2023-07-11 09:42:40,840:INFO:Declaring metric variables
2023-07-11 09:42:40,840:INFO:Importing untrained model
2023-07-11 09:42:40,841:INFO:Extreme Gradient Boosting Imported successfully
2023-07-11 09:42:40,841:INFO:Starting cross validation
2023-07-11 09:42:40,843:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:42:45,647:ERROR:create_model() for xgboost raised an exception or returned all 0.0:
2023-07-11 09:42:45,647:ERROR:Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 792, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\core.py", line 620, in inner_f
    return func(**kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\sklearn.py", line 1471, in fit
    train_dmatrix, evals = _wrap_evaluation_matrices(
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\sklearn.py", line 448, in _wrap_evaluation_matrices
    train_dmatrix = create_dmatrix(
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\sklearn.py", line 908, in _create_dmatrix
    return DMatrix(**kwargs, nthread=self.n_jobs)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\core.py", line 620, in inner_f
    return func(**kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\core.py", line 766, in __init__
    self.feature_names = feature_names
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\core.py", line 1155, in feature_names
    raise ValueError('feature_names must be unique')
ValueError: feature_names must be unique


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 808, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\core.py", line 620, in inner_f
    return func(**kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\sklearn.py", line 1471, in fit
    train_dmatrix, evals = _wrap_evaluation_matrices(
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\sklearn.py", line 448, in _wrap_evaluation_matrices
    train_dmatrix = create_dmatrix(
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\sklearn.py", line 908, in _create_dmatrix
    return DMatrix(**kwargs, nthread=self.n_jobs)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\core.py", line 620, in inner_f
    return func(**kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\core.py", line 766, in __init__
    self.feature_names = feature_names
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\core.py", line 1155, in feature_names
    raise ValueError('feature_names must be unique')
ValueError: feature_names must be unique


2023-07-11 09:42:46,695:INFO:Initializing Light Gradient Boosting Machine
2023-07-11 09:42:46,695:INFO:Total runtime is 1.2190474033355714 minutes
2023-07-11 09:42:46,695:INFO:SubProcess create_model() called ==================================
2023-07-11 09:42:46,695:INFO:Initializing create_model()
2023-07-11 09:42:46,695:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E1D6CB1D60>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D6F478E0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:42:46,695:INFO:Checking exceptions
2023-07-11 09:42:46,695:INFO:Importing libraries
2023-07-11 09:42:46,695:INFO:Copying training dataset
2023-07-11 09:42:46,695:INFO:Defining folds
2023-07-11 09:42:46,695:INFO:Declaring metric variables
2023-07-11 09:42:46,695:INFO:Importing untrained model
2023-07-11 09:42:46,711:INFO:Light Gradient Boosting Machine Imported successfully
2023-07-11 09:42:46,711:INFO:Starting cross validation
2023-07-11 09:42:46,711:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:42:51,193:WARNING:create_model() for lightgbm raised an exception or returned all 0.0, trying without fit_kwargs:
2023-07-11 09:42:51,205:WARNING:Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 792, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
1 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\sklearn.py", line 967, in fit
    super().fit(X, _y, sample_weight=sample_weight, init_score=init_score, eval_set=valid_sets,
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\sklearn.py", line 748, in fit
    self._Booster = train(
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\engine.py", line 271, in train
    booster = Booster(params=params, train_set=train_set)
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 2605, in __init__
    train_set.construct()
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 1815, in construct
    self._lazy_init(self.data, label=self.label,
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 1573, in _lazy_init
    return self.set_feature_name(feature_name)
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 2142, in set_feature_name
    _safe_call(_LIB.LGBM_DatasetSetFeatureNames(
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 125, in _safe_call
    raise LightGBMError(_LIB.LGBM_GetLastError().decode('utf-8'))
lightgbm.basic.LightGBMError: Feature (salary_currency_USD) appears more than one time.

--------------------------------------------------------------------------------
9 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\sklearn.py", line 967, in fit
    super().fit(X, _y, sample_weight=sample_weight, init_score=init_score, eval_set=valid_sets,
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\sklearn.py", line 748, in fit
    self._Booster = train(
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\engine.py", line 271, in train
    booster = Booster(params=params, train_set=train_set)
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 2605, in __init__
    train_set.construct()
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 1815, in construct
    self._lazy_init(self.data, label=self.label,
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 1573, in _lazy_init
    return self.set_feature_name(feature_name)
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 2142, in set_feature_name
    _safe_call(_LIB.LGBM_DatasetSetFeatureNames(
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 125, in _safe_call
    raise LightGBMError(_LIB.LGBM_GetLastError().decode('utf-8'))
lightgbm.basic.LightGBMError: Feature (salary_currency_GBP) appears more than one time.


2023-07-11 09:42:51,205:INFO:Initializing create_model()
2023-07-11 09:42:51,205:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E1D6CB1D60>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D6F478E0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:42:51,205:INFO:Checking exceptions
2023-07-11 09:42:51,205:INFO:Importing libraries
2023-07-11 09:42:51,205:INFO:Copying training dataset
2023-07-11 09:42:51,221:INFO:Defining folds
2023-07-11 09:42:51,221:INFO:Declaring metric variables
2023-07-11 09:42:51,221:INFO:Importing untrained model
2023-07-11 09:42:51,221:INFO:Light Gradient Boosting Machine Imported successfully
2023-07-11 09:42:51,221:INFO:Starting cross validation
2023-07-11 09:42:51,221:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:42:55,822:ERROR:create_model() for lightgbm raised an exception or returned all 0.0:
2023-07-11 09:42:55,822:ERROR:Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 792, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
1 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\sklearn.py", line 967, in fit
    super().fit(X, _y, sample_weight=sample_weight, init_score=init_score, eval_set=valid_sets,
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\sklearn.py", line 748, in fit
    self._Booster = train(
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\engine.py", line 271, in train
    booster = Booster(params=params, train_set=train_set)
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 2605, in __init__
    train_set.construct()
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 1815, in construct
    self._lazy_init(self.data, label=self.label,
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 1573, in _lazy_init
    return self.set_feature_name(feature_name)
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 2142, in set_feature_name
    _safe_call(_LIB.LGBM_DatasetSetFeatureNames(
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 125, in _safe_call
    raise LightGBMError(_LIB.LGBM_GetLastError().decode('utf-8'))
lightgbm.basic.LightGBMError: Feature (salary_currency_USD) appears more than one time.

--------------------------------------------------------------------------------
9 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\sklearn.py", line 967, in fit
    super().fit(X, _y, sample_weight=sample_weight, init_score=init_score, eval_set=valid_sets,
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\sklearn.py", line 748, in fit
    self._Booster = train(
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\engine.py", line 271, in train
    booster = Booster(params=params, train_set=train_set)
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 2605, in __init__
    train_set.construct()
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 1815, in construct
    self._lazy_init(self.data, label=self.label,
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 1573, in _lazy_init
    return self.set_feature_name(feature_name)
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 2142, in set_feature_name
    _safe_call(_LIB.LGBM_DatasetSetFeatureNames(
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 125, in _safe_call
    raise LightGBMError(_LIB.LGBM_GetLastError().decode('utf-8'))
lightgbm.basic.LightGBMError: Feature (salary_currency_GBP) appears more than one time.


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 808, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
1 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\sklearn.py", line 967, in fit
    super().fit(X, _y, sample_weight=sample_weight, init_score=init_score, eval_set=valid_sets,
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\sklearn.py", line 748, in fit
    self._Booster = train(
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\engine.py", line 271, in train
    booster = Booster(params=params, train_set=train_set)
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 2605, in __init__
    train_set.construct()
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 1815, in construct
    self._lazy_init(self.data, label=self.label,
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 1573, in _lazy_init
    return self.set_feature_name(feature_name)
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 2142, in set_feature_name
    _safe_call(_LIB.LGBM_DatasetSetFeatureNames(
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 125, in _safe_call
    raise LightGBMError(_LIB.LGBM_GetLastError().decode('utf-8'))
lightgbm.basic.LightGBMError: Feature (salary_currency_USD) appears more than one time.

--------------------------------------------------------------------------------
9 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\sklearn.py", line 967, in fit
    super().fit(X, _y, sample_weight=sample_weight, init_score=init_score, eval_set=valid_sets,
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\sklearn.py", line 748, in fit
    self._Booster = train(
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\engine.py", line 271, in train
    booster = Booster(params=params, train_set=train_set)
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 2605, in __init__
    train_set.construct()
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 1815, in construct
    self._lazy_init(self.data, label=self.label,
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 1573, in _lazy_init
    return self.set_feature_name(feature_name)
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 2142, in set_feature_name
    _safe_call(_LIB.LGBM_DatasetSetFeatureNames(
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 125, in _safe_call
    raise LightGBMError(_LIB.LGBM_GetLastError().decode('utf-8'))
lightgbm.basic.LightGBMError: Feature (salary_currency_GBP) appears more than one time.


2023-07-11 09:42:56,869:INFO:Initializing Dummy Classifier
2023-07-11 09:42:56,869:INFO:Total runtime is 1.3886135578155518 minutes
2023-07-11 09:42:56,870:INFO:SubProcess create_model() called ==================================
2023-07-11 09:42:56,870:INFO:Initializing create_model()
2023-07-11 09:42:56,870:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E1D6CB1D60>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D6F478E0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:42:56,870:INFO:Checking exceptions
2023-07-11 09:42:56,870:INFO:Importing libraries
2023-07-11 09:42:56,870:INFO:Copying training dataset
2023-07-11 09:42:56,873:INFO:Defining folds
2023-07-11 09:42:56,873:INFO:Declaring metric variables
2023-07-11 09:42:56,873:INFO:Importing untrained model
2023-07-11 09:42:56,873:INFO:Dummy Classifier Imported successfully
2023-07-11 09:42:56,873:INFO:Starting cross validation
2023-07-11 09:42:56,874:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:42:57,378:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:57,380:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:57,380:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 09:42:57,380:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:57,380:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:57,380:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:57,380:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 09:42:57,380:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:57,404:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:57,404:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:57,404:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 09:42:57,404:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:57,427:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:57,427:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:57,427:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 09:42:57,427:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:57,427:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:57,444:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:57,446:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 09:42:57,447:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:57,448:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:57,448:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:57,448:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:57,448:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 09:42:57,448:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:57,448:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:57,448:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 09:42:57,448:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:57,484:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:57,484:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:57,484:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 09:42:57,484:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:57,517:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:57,517:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:57,517:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 09:42:57,517:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:57,550:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:57,550:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:42:57,550:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 09:42:57,550:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 2023) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-07-11 09:43:00,887:INFO:Calculating mean and std
2023-07-11 09:43:00,888:INFO:Creating metrics dataframe
2023-07-11 09:43:01,548:INFO:Uploading results into container
2023-07-11 09:43:01,548:INFO:Uploading model into container now
2023-07-11 09:43:01,548:INFO:_master_model_container: 13
2023-07-11 09:43:01,548:INFO:_display_container: 2
2023-07-11 09:43:01,548:INFO:DummyClassifier(constant=None, random_state=2940, strategy='prior')
2023-07-11 09:43:01,548:INFO:create_model() successfully completed......................................
2023-07-11 09:43:01,699:INFO:SubProcess create_model() end ==================================
2023-07-11 09:43:01,699:INFO:Creating metrics dataframe
2023-07-11 09:43:01,708:INFO:Initializing create_model()
2023-07-11 09:43:01,708:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E1D6CB1D60>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=2940, verbose=0, warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:43:01,708:INFO:Checking exceptions
2023-07-11 09:43:01,709:INFO:Importing libraries
2023-07-11 09:43:01,709:INFO:Copying training dataset
2023-07-11 09:43:01,714:INFO:Defining folds
2023-07-11 09:43:01,714:INFO:Declaring metric variables
2023-07-11 09:43:01,714:INFO:Importing untrained model
2023-07-11 09:43:01,714:INFO:Declaring custom model
2023-07-11 09:43:01,715:INFO:Random Forest Classifier Imported successfully
2023-07-11 09:43:01,716:INFO:Cross validation set to False
2023-07-11 09:43:01,716:INFO:Fitting Model
2023-07-11 09:43:02,541:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=2940, verbose=0, warm_start=False)
2023-07-11 09:43:02,541:INFO:create_model() successfully completed......................................
2023-07-11 09:43:02,693:INFO:_master_model_container: 13
2023-07-11 09:43:02,693:INFO:_display_container: 2
2023-07-11 09:43:02,694:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=2940, verbose=0, warm_start=False)
2023-07-11 09:43:02,694:INFO:compare_models() successfully completed......................................
2023-07-11 09:43:02,724:INFO:Initializing save_model()
2023-07-11 09:43:02,724:INFO:save_model(model=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=2940, verbose=0, warm_start=False), model_name=best_model.pkl, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\didit\AppData\Local\Temp\joblib),
         steps=[('label_encoding',
                 TransformerWrapperWithInverse(exclude=None, include=None,
                                               transformer=LabelEncoder())),
                ('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['salary', 'salary_in_usd',
                                             'remote_ratio'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fil...
                ('rest_encoding',
                 TransformerWrapper(exclude=None,
                                    include=['job_title', 'employee_residence',
                                             'company_location'],
                                    transformer=TargetEncoder(cols=['job_title',
                                                                    'employee_residence',
                                                                    'company_location'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              hierarchy=None,
                                                              min_samples_leaf=20,
                                                              return_df=True,
                                                              smoothing=10,
                                                              verbose=0)))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2023-07-11 09:43:02,724:INFO:Adding model into prep_pipe
2023-07-11 09:43:02,771:INFO:best_model.pkl.pkl saved in current working directory
2023-07-11 09:43:02,780:INFO:Pipeline(memory=FastMemory(location=C:\Users\didit\AppData\Local\Temp\joblib),
         steps=[('label_encoding',
                 TransformerWrapperWithInverse(exclude=None, include=None,
                                               transformer=LabelEncoder())),
                ('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['salary', 'salary_in_usd',
                                             'remote_ratio'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fil...
                 RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                                        class_weight=None, criterion='gini',
                                        max_depth=None, max_features='sqrt',
                                        max_leaf_nodes=None, max_samples=None,
                                        min_impurity_decrease=0.0,
                                        min_samples_leaf=1, min_samples_split=2,
                                        min_weight_fraction_leaf=0.0,
                                        n_estimators=100, n_jobs=-1,
                                        oob_score=False, random_state=2940,
                                        verbose=0, warm_start=False))],
         verbose=False)
2023-07-11 09:43:02,781:INFO:save_model() successfully completed......................................
2023-07-11 09:43:31,481:INFO:PyCaret ClassificationExperiment
2023-07-11 09:43:31,481:INFO:Logging name: clf-default-name
2023-07-11 09:43:31,481:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2023-07-11 09:43:31,481:INFO:version 3.0.2
2023-07-11 09:43:31,481:INFO:Initializing setup()
2023-07-11 09:43:31,481:INFO:self.USI: 1a6f
2023-07-11 09:43:31,481:INFO:self._variable_keys: {'_ml_usecase', 'log_plots_param', 'USI', 'X_train', 'pipeline', 'gpu_param', 'exp_name_log', 'y', 'is_multiclass', 'html_param', 'X', 'data', 'exp_id', 'y_train', 'target_param', 'fix_imbalance', 'X_test', 'fold_groups_param', 'n_jobs_param', 'idx', 'fold_generator', 'logging_param', 'memory', 'gpu_n_jobs_param', 'fold_shuffle_param', '_available_plots', 'seed', 'y_test'}
2023-07-11 09:43:31,481:INFO:Checking environment
2023-07-11 09:43:31,481:INFO:python_version: 3.9.13
2023-07-11 09:43:31,481:INFO:python_build: ('main', 'Oct 13 2022 21:23:06')
2023-07-11 09:43:31,481:INFO:machine: AMD64
2023-07-11 09:43:31,481:INFO:platform: Windows-10-10.0.22621-SP0
2023-07-11 09:43:31,481:INFO:Memory: svmem(total=16893358080, available=2888781824, percent=82.9, used=14004576256, free=2888781824)
2023-07-11 09:43:31,481:INFO:Physical Core: 8
2023-07-11 09:43:31,481:INFO:Logical Core: 16
2023-07-11 09:43:31,481:INFO:Checking libraries
2023-07-11 09:43:31,481:INFO:System:
2023-07-11 09:43:31,481:INFO:    python: 3.9.13 (main, Oct 13 2022, 21:23:06) [MSC v.1916 64 bit (AMD64)]
2023-07-11 09:43:31,481:INFO:executable: C:\Users\didit\anaconda3\python.exe
2023-07-11 09:43:31,481:INFO:   machine: Windows-10-10.0.22621-SP0
2023-07-11 09:43:31,481:INFO:PyCaret required dependencies:
2023-07-11 09:43:31,497:INFO:                 pip: 23.0.1
2023-07-11 09:43:31,497:INFO:          setuptools: 67.8.0
2023-07-11 09:43:31,497:INFO:             pycaret: 3.0.2
2023-07-11 09:43:31,497:INFO:             IPython: 8.12.0
2023-07-11 09:43:31,497:INFO:          ipywidgets: 8.0.4
2023-07-11 09:43:31,497:INFO:                tqdm: 4.65.0
2023-07-11 09:43:31,497:INFO:               numpy: 1.21.5
2023-07-11 09:43:31,497:INFO:              pandas: 1.5.3
2023-07-11 09:43:31,497:INFO:              jinja2: 3.1.2
2023-07-11 09:43:31,497:INFO:               scipy: 1.10.1
2023-07-11 09:43:31,497:INFO:              joblib: 1.2.0
2023-07-11 09:43:31,497:INFO:             sklearn: 1.2.2
2023-07-11 09:43:31,497:INFO:                pyod: 1.0.9
2023-07-11 09:43:31,497:INFO:            imblearn: 0.10.1
2023-07-11 09:43:31,497:INFO:   category_encoders: 2.6.1
2023-07-11 09:43:31,497:INFO:            lightgbm: 3.3.5
2023-07-11 09:43:31,497:INFO:               numba: 0.57.0
2023-07-11 09:43:31,497:INFO:            requests: 2.29.0
2023-07-11 09:43:31,497:INFO:          matplotlib: 3.7.1
2023-07-11 09:43:31,497:INFO:          scikitplot: 0.3.7
2023-07-11 09:43:31,497:INFO:         yellowbrick: 1.5
2023-07-11 09:43:31,497:INFO:              plotly: 5.9.0
2023-07-11 09:43:31,497:INFO:             kaleido: 0.2.1
2023-07-11 09:43:31,497:INFO:         statsmodels: 0.13.5
2023-07-11 09:43:31,497:INFO:              sktime: 0.17.0
2023-07-11 09:43:31,497:INFO:               tbats: 1.1.3
2023-07-11 09:43:31,497:INFO:            pmdarima: 2.0.3
2023-07-11 09:43:31,497:INFO:              psutil: 5.9.0
2023-07-11 09:43:31,497:INFO:PyCaret optional dependencies:
2023-07-11 09:43:31,497:INFO:                shap: 0.41.0
2023-07-11 09:43:31,497:INFO:           interpret: Not installed
2023-07-11 09:43:31,497:INFO:                umap: Not installed
2023-07-11 09:43:31,497:INFO:    pandas_profiling: 4.3.1
2023-07-11 09:43:31,497:INFO:  explainerdashboard: Not installed
2023-07-11 09:43:31,497:INFO:             autoviz: Not installed
2023-07-11 09:43:31,497:INFO:           fairlearn: Not installed
2023-07-11 09:43:31,497:INFO:             xgboost: 1.7.6
2023-07-11 09:43:31,497:INFO:            catboost: Not installed
2023-07-11 09:43:31,497:INFO:              kmodes: Not installed
2023-07-11 09:43:31,497:INFO:             mlxtend: Not installed
2023-07-11 09:43:31,497:INFO:       statsforecast: Not installed
2023-07-11 09:43:31,497:INFO:        tune_sklearn: Not installed
2023-07-11 09:43:31,497:INFO:                 ray: Not installed
2023-07-11 09:43:31,497:INFO:            hyperopt: Not installed
2023-07-11 09:43:31,497:INFO:              optuna: Not installed
2023-07-11 09:43:31,497:INFO:               skopt: Not installed
2023-07-11 09:43:31,497:INFO:              mlflow: 2.4.2
2023-07-11 09:43:31,497:INFO:              gradio: Not installed
2023-07-11 09:43:31,497:INFO:             fastapi: 0.95.2
2023-07-11 09:43:31,497:INFO:             uvicorn: 0.22.0
2023-07-11 09:43:31,497:INFO:              m2cgen: Not installed
2023-07-11 09:43:31,497:INFO:           evidently: Not installed
2023-07-11 09:43:31,497:INFO:               fugue: Not installed
2023-07-11 09:43:31,497:INFO:           streamlit: 1.23.1
2023-07-11 09:43:31,498:INFO:             prophet: Not installed
2023-07-11 09:43:31,498:INFO:None
2023-07-11 09:43:31,498:INFO:Set up data.
2023-07-11 09:43:31,503:INFO:Set up train/test split.
2023-07-11 09:44:14,951:INFO:PyCaret RegressionExperiment
2023-07-11 09:44:14,951:INFO:Logging name: reg-default-name
2023-07-11 09:44:14,952:INFO:ML Usecase: MLUsecase.REGRESSION
2023-07-11 09:44:14,952:INFO:version 3.0.2
2023-07-11 09:44:14,952:INFO:Initializing setup()
2023-07-11 09:44:14,952:INFO:self.USI: faf2
2023-07-11 09:44:14,952:INFO:self._variable_keys: {'_ml_usecase', 'log_plots_param', 'USI', 'X_train', 'pipeline', 'gpu_param', 'exp_name_log', 'y', 'html_param', 'X', 'data', 'exp_id', 'y_train', 'target_param', 'X_test', 'fold_groups_param', 'n_jobs_param', 'idx', 'fold_generator', 'logging_param', 'memory', 'gpu_n_jobs_param', 'fold_shuffle_param', '_available_plots', 'seed', 'y_test', 'transform_target_param'}
2023-07-11 09:44:14,952:INFO:Checking environment
2023-07-11 09:44:14,952:INFO:python_version: 3.9.13
2023-07-11 09:44:14,953:INFO:python_build: ('main', 'Oct 13 2022 21:23:06')
2023-07-11 09:44:14,953:INFO:machine: AMD64
2023-07-11 09:44:14,953:INFO:platform: Windows-10-10.0.22621-SP0
2023-07-11 09:44:14,953:INFO:Memory: svmem(total=16893358080, available=2465095680, percent=85.4, used=14428262400, free=2465095680)
2023-07-11 09:44:14,954:INFO:Physical Core: 8
2023-07-11 09:44:14,954:INFO:Logical Core: 16
2023-07-11 09:44:14,954:INFO:Checking libraries
2023-07-11 09:44:14,954:INFO:System:
2023-07-11 09:44:14,954:INFO:    python: 3.9.13 (main, Oct 13 2022, 21:23:06) [MSC v.1916 64 bit (AMD64)]
2023-07-11 09:44:14,954:INFO:executable: C:\Users\didit\anaconda3\python.exe
2023-07-11 09:44:14,955:INFO:   machine: Windows-10-10.0.22621-SP0
2023-07-11 09:44:14,955:INFO:PyCaret required dependencies:
2023-07-11 09:44:14,955:INFO:                 pip: 23.0.1
2023-07-11 09:44:14,955:INFO:          setuptools: 67.8.0
2023-07-11 09:44:14,956:INFO:             pycaret: 3.0.2
2023-07-11 09:44:14,956:INFO:             IPython: 8.12.0
2023-07-11 09:44:14,956:INFO:          ipywidgets: 8.0.4
2023-07-11 09:44:14,956:INFO:                tqdm: 4.65.0
2023-07-11 09:44:14,956:INFO:               numpy: 1.21.5
2023-07-11 09:44:14,957:INFO:              pandas: 1.5.3
2023-07-11 09:44:14,957:INFO:              jinja2: 3.1.2
2023-07-11 09:44:14,957:INFO:               scipy: 1.10.1
2023-07-11 09:44:14,957:INFO:              joblib: 1.2.0
2023-07-11 09:44:14,957:INFO:             sklearn: 1.2.2
2023-07-11 09:44:14,957:INFO:                pyod: 1.0.9
2023-07-11 09:44:14,957:INFO:            imblearn: 0.10.1
2023-07-11 09:44:14,957:INFO:   category_encoders: 2.6.1
2023-07-11 09:44:14,957:INFO:            lightgbm: 3.3.5
2023-07-11 09:44:14,957:INFO:               numba: 0.57.0
2023-07-11 09:44:14,957:INFO:            requests: 2.29.0
2023-07-11 09:44:14,957:INFO:          matplotlib: 3.7.1
2023-07-11 09:44:14,957:INFO:          scikitplot: 0.3.7
2023-07-11 09:44:14,957:INFO:         yellowbrick: 1.5
2023-07-11 09:44:14,957:INFO:              plotly: 5.9.0
2023-07-11 09:44:14,958:INFO:             kaleido: 0.2.1
2023-07-11 09:44:14,958:INFO:         statsmodels: 0.13.5
2023-07-11 09:44:14,958:INFO:              sktime: 0.17.0
2023-07-11 09:44:14,958:INFO:               tbats: 1.1.3
2023-07-11 09:44:14,958:INFO:            pmdarima: 2.0.3
2023-07-11 09:44:14,958:INFO:              psutil: 5.9.0
2023-07-11 09:44:14,958:INFO:PyCaret optional dependencies:
2023-07-11 09:44:14,959:INFO:                shap: 0.41.0
2023-07-11 09:44:14,959:INFO:           interpret: Not installed
2023-07-11 09:44:14,959:INFO:                umap: Not installed
2023-07-11 09:44:14,959:INFO:    pandas_profiling: 4.3.1
2023-07-11 09:44:14,959:INFO:  explainerdashboard: Not installed
2023-07-11 09:44:14,959:INFO:             autoviz: Not installed
2023-07-11 09:44:14,959:INFO:           fairlearn: Not installed
2023-07-11 09:44:14,960:INFO:             xgboost: 1.7.6
2023-07-11 09:44:14,960:INFO:            catboost: Not installed
2023-07-11 09:44:14,960:INFO:              kmodes: Not installed
2023-07-11 09:44:14,960:INFO:             mlxtend: Not installed
2023-07-11 09:44:14,960:INFO:       statsforecast: Not installed
2023-07-11 09:44:14,960:INFO:        tune_sklearn: Not installed
2023-07-11 09:44:14,960:INFO:                 ray: Not installed
2023-07-11 09:44:14,960:INFO:            hyperopt: Not installed
2023-07-11 09:44:14,960:INFO:              optuna: Not installed
2023-07-11 09:44:14,960:INFO:               skopt: Not installed
2023-07-11 09:44:14,960:INFO:              mlflow: 2.4.2
2023-07-11 09:44:14,961:INFO:              gradio: Not installed
2023-07-11 09:44:14,961:INFO:             fastapi: 0.95.2
2023-07-11 09:44:14,961:INFO:             uvicorn: 0.22.0
2023-07-11 09:44:14,961:INFO:              m2cgen: Not installed
2023-07-11 09:44:14,961:INFO:           evidently: Not installed
2023-07-11 09:44:14,961:INFO:               fugue: Not installed
2023-07-11 09:44:14,961:INFO:           streamlit: 1.23.1
2023-07-11 09:44:14,961:INFO:             prophet: Not installed
2023-07-11 09:44:14,961:INFO:None
2023-07-11 09:44:14,961:INFO:Set up data.
2023-07-11 09:44:14,965:INFO:Set up train/test split.
2023-07-11 09:44:14,965:INFO:Set up index.
2023-07-11 09:44:14,965:INFO:Set up folding strategy.
2023-07-11 09:44:14,965:INFO:Assigning column types.
2023-07-11 09:44:14,965:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-07-11 09:44:14,965:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-07-11 09:44:14,981:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-07-11 09:44:14,981:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 09:44:15,028:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 09:44:15,060:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 09:44:15,060:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:44:15,060:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:44:15,060:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-07-11 09:44:15,075:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-07-11 09:44:15,075:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 09:44:15,115:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 09:44:15,165:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 09:44:15,165:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:44:15,165:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:44:15,165:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-07-11 09:44:15,165:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-07-11 09:44:15,165:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 09:44:15,212:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 09:44:15,244:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 09:44:15,244:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:44:15,244:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:44:15,259:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-07-11 09:44:15,259:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 09:44:15,306:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 09:44:15,365:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 09:44:15,365:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:44:15,365:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:44:15,365:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-07-11 09:44:15,365:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 09:44:15,412:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 09:44:15,444:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 09:44:15,444:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:44:15,444:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:44:15,444:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 09:44:15,491:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 09:44:15,538:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 09:44:15,539:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:44:15,540:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:44:15,541:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-07-11 09:44:15,596:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 09:44:15,627:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 09:44:15,627:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:44:15,627:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:44:15,690:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 09:44:15,716:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 09:44:15,716:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:44:15,716:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:44:15,716:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-07-11 09:44:15,766:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 09:44:15,797:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:44:15,813:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:44:15,861:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 09:44:15,892:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:44:15,892:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:44:15,892:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-07-11 09:44:15,980:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:44:15,980:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:44:16,074:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:44:16,074:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:44:16,074:INFO:Preparing preprocessing pipeline...
2023-07-11 09:44:16,074:INFO:Set up simple imputation.
2023-07-11 09:44:16,074:INFO:Set up encoding of categorical features.
2023-07-11 09:44:16,198:INFO:Finished creating preprocessing pipeline.
2023-07-11 09:44:16,198:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\didit\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['work_year', 'salary',
                                             'remote_ratio'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['experience_level',
                                             'employment_type', 'job_title',
                                             'salary_currency',
                                             'employee_residence',
                                             'company_l...
                                    transformer=OneHotEncoder(cols=['experience_level',
                                                                    'employment_type',
                                                                    'salary_currency',
                                                                    'company_size'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('rest_encoding',
                 TransformerWrapper(include=['job_title', 'employee_residence',
                                             'company_location'],
                                    transformer=TargetEncoder(cols=['job_title',
                                                                    'employee_residence',
                                                                    'company_location'],
                                                              handle_missing='return_nan')))])
2023-07-11 09:44:16,198:INFO:Creating final display dataframe.
2023-07-11 09:44:16,444:INFO:Setup _display_container:                     Description             Value
0                    Session id              8333
1                        Target     salary_in_usd
2                   Target type        Regression
3           Original data shape        (3755, 11)
4        Transformed data shape       (3755, 162)
5   Transformed train set shape       (2628, 162)
6    Transformed test set shape       (1127, 162)
7              Numeric features                 3
8          Categorical features                 7
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13     Maximum one-hot encoding                25
14              Encoding method              None
15               Fold Generator             KFold
16                  Fold Number                10
17                     CPU Jobs                -1
18                      Use GPU             False
19               Log Experiment             False
20              Experiment Name  reg-default-name
21                          USI              faf2
2023-07-11 09:44:16,541:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:44:16,544:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:44:16,611:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:44:16,611:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:44:16,611:INFO:setup() successfully completed in 1.96s...............
2023-07-11 09:44:16,628:INFO:Initializing compare_models()
2023-07-11 09:44:16,628:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D6AFB820>, include=None, fold=None, round=4, cross_validation=True, sort=R2, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x000001E1D6AFB820>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2023-07-11 09:44:16,628:INFO:Checking exceptions
2023-07-11 09:44:16,630:INFO:Preparing display monitor
2023-07-11 09:44:16,632:INFO:Initializing Linear Regression
2023-07-11 09:44:16,632:INFO:Total runtime is 0.0 minutes
2023-07-11 09:44:16,632:INFO:SubProcess create_model() called ==================================
2023-07-11 09:44:16,632:INFO:Initializing create_model()
2023-07-11 09:44:16,632:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D6AFB820>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D74FAA00>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:44:16,632:INFO:Checking exceptions
2023-07-11 09:44:16,632:INFO:Importing libraries
2023-07-11 09:44:16,632:INFO:Copying training dataset
2023-07-11 09:44:16,634:INFO:Defining folds
2023-07-11 09:44:16,634:INFO:Declaring metric variables
2023-07-11 09:44:16,634:INFO:Importing untrained model
2023-07-11 09:44:16,634:INFO:Linear Regression Imported successfully
2023-07-11 09:44:16,635:INFO:Starting cross validation
2023-07-11 09:44:16,635:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:44:20,181:INFO:Calculating mean and std
2023-07-11 09:44:20,181:INFO:Creating metrics dataframe
2023-07-11 09:44:20,945:INFO:Uploading results into container
2023-07-11 09:44:20,945:INFO:Uploading model into container now
2023-07-11 09:44:20,946:INFO:_master_model_container: 1
2023-07-11 09:44:20,946:INFO:_display_container: 2
2023-07-11 09:44:20,946:INFO:LinearRegression(n_jobs=-1)
2023-07-11 09:44:20,946:INFO:create_model() successfully completed......................................
2023-07-11 09:44:21,060:INFO:SubProcess create_model() end ==================================
2023-07-11 09:44:21,060:INFO:Creating metrics dataframe
2023-07-11 09:44:21,060:INFO:Initializing Lasso Regression
2023-07-11 09:44:21,060:INFO:Total runtime is 0.07380090554555258 minutes
2023-07-11 09:44:21,060:INFO:SubProcess create_model() called ==================================
2023-07-11 09:44:21,060:INFO:Initializing create_model()
2023-07-11 09:44:21,060:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D6AFB820>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D74FAA00>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:44:21,060:INFO:Checking exceptions
2023-07-11 09:44:21,060:INFO:Importing libraries
2023-07-11 09:44:21,060:INFO:Copying training dataset
2023-07-11 09:44:21,060:INFO:Defining folds
2023-07-11 09:44:21,060:INFO:Declaring metric variables
2023-07-11 09:44:21,060:INFO:Importing untrained model
2023-07-11 09:44:21,060:INFO:Lasso Regression Imported successfully
2023-07-11 09:44:21,060:INFO:Starting cross validation
2023-07-11 09:44:21,060:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:44:21,742:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.360e+10, tolerance: 9.508e+08
  model = cd_fast.enet_coordinate_descent(

2023-07-11 09:44:21,859:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.466e+12, tolerance: 9.537e+08
  model = cd_fast.enet_coordinate_descent(

2023-07-11 09:44:25,089:INFO:Calculating mean and std
2023-07-11 09:44:25,105:INFO:Creating metrics dataframe
2023-07-11 09:44:25,781:INFO:Uploading results into container
2023-07-11 09:44:25,781:INFO:Uploading model into container now
2023-07-11 09:44:25,781:INFO:_master_model_container: 2
2023-07-11 09:44:25,781:INFO:_display_container: 2
2023-07-11 09:44:25,781:INFO:Lasso(random_state=8333)
2023-07-11 09:44:25,781:INFO:create_model() successfully completed......................................
2023-07-11 09:44:25,891:INFO:SubProcess create_model() end ==================================
2023-07-11 09:44:25,891:INFO:Creating metrics dataframe
2023-07-11 09:44:25,891:INFO:Initializing Ridge Regression
2023-07-11 09:44:25,891:INFO:Total runtime is 0.154315714041392 minutes
2023-07-11 09:44:25,891:INFO:SubProcess create_model() called ==================================
2023-07-11 09:44:25,891:INFO:Initializing create_model()
2023-07-11 09:44:25,891:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D6AFB820>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D74FAA00>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:44:25,891:INFO:Checking exceptions
2023-07-11 09:44:25,891:INFO:Importing libraries
2023-07-11 09:44:25,891:INFO:Copying training dataset
2023-07-11 09:44:25,891:INFO:Defining folds
2023-07-11 09:44:25,891:INFO:Declaring metric variables
2023-07-11 09:44:25,891:INFO:Importing untrained model
2023-07-11 09:44:25,891:INFO:Ridge Regression Imported successfully
2023-07-11 09:44:25,891:INFO:Starting cross validation
2023-07-11 09:44:25,891:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:44:29,475:INFO:Calculating mean and std
2023-07-11 09:44:29,475:INFO:Creating metrics dataframe
2023-07-11 09:44:30,180:INFO:Uploading results into container
2023-07-11 09:44:30,180:INFO:Uploading model into container now
2023-07-11 09:44:30,180:INFO:_master_model_container: 3
2023-07-11 09:44:30,180:INFO:_display_container: 2
2023-07-11 09:44:30,180:INFO:Ridge(random_state=8333)
2023-07-11 09:44:30,180:INFO:create_model() successfully completed......................................
2023-07-11 09:44:30,307:INFO:SubProcess create_model() end ==================================
2023-07-11 09:44:30,307:INFO:Creating metrics dataframe
2023-07-11 09:44:30,320:INFO:Initializing Elastic Net
2023-07-11 09:44:30,321:INFO:Total runtime is 0.22815411488215126 minutes
2023-07-11 09:44:30,321:INFO:SubProcess create_model() called ==================================
2023-07-11 09:44:30,321:INFO:Initializing create_model()
2023-07-11 09:44:30,321:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D6AFB820>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D74FAA00>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:44:30,321:INFO:Checking exceptions
2023-07-11 09:44:30,321:INFO:Importing libraries
2023-07-11 09:44:30,321:INFO:Copying training dataset
2023-07-11 09:44:30,324:INFO:Defining folds
2023-07-11 09:44:30,324:INFO:Declaring metric variables
2023-07-11 09:44:30,324:INFO:Importing untrained model
2023-07-11 09:44:30,325:INFO:Elastic Net Imported successfully
2023-07-11 09:44:30,325:INFO:Starting cross validation
2023-07-11 09:44:30,326:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:44:34,044:INFO:Calculating mean and std
2023-07-11 09:44:34,044:INFO:Creating metrics dataframe
2023-07-11 09:44:34,770:INFO:Uploading results into container
2023-07-11 09:44:34,770:INFO:Uploading model into container now
2023-07-11 09:44:34,770:INFO:_master_model_container: 4
2023-07-11 09:44:34,770:INFO:_display_container: 2
2023-07-11 09:44:34,770:INFO:ElasticNet(random_state=8333)
2023-07-11 09:44:34,770:INFO:create_model() successfully completed......................................
2023-07-11 09:44:34,892:INFO:SubProcess create_model() end ==================================
2023-07-11 09:44:34,892:INFO:Creating metrics dataframe
2023-07-11 09:44:34,892:INFO:Initializing Least Angle Regression
2023-07-11 09:44:34,892:INFO:Total runtime is 0.3043359756469726 minutes
2023-07-11 09:44:34,892:INFO:SubProcess create_model() called ==================================
2023-07-11 09:44:34,892:INFO:Initializing create_model()
2023-07-11 09:44:34,892:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D6AFB820>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D74FAA00>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:44:34,892:INFO:Checking exceptions
2023-07-11 09:44:34,892:INFO:Importing libraries
2023-07-11 09:44:34,892:INFO:Copying training dataset
2023-07-11 09:44:34,892:INFO:Defining folds
2023-07-11 09:44:34,892:INFO:Declaring metric variables
2023-07-11 09:44:34,892:INFO:Importing untrained model
2023-07-11 09:44:34,892:INFO:Least Angle Regression Imported successfully
2023-07-11 09:44:34,892:INFO:Starting cross validation
2023-07-11 09:44:34,892:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:44:35,118:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=7.369e+02, with an active set of 9 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,118:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=1.966e+03, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,118:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=1.459e+03, with an active set of 13 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,118:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=1.645e+03, with an active set of 14 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,118:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=9.511e+02, with an active set of 16 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,118:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=9.317e+02, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,118:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=9.239e+02, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,118:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=5.976e+02, with an active set of 18 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,118:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=5.186e+02, with an active set of 19 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,118:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=4.357e+02, with an active set of 21 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,118:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=4.351e+02, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,118:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=3.971e+02, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,118:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=3.960e+02, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,118:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=3.597e+02, with an active set of 24 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,118:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=3.582e+02, with an active set of 25 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,118:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=3.085e+02, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,118:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=3.084e+02, with an active set of 26 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,118:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=2.996e+02, with an active set of 26 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,118:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=2.936e+02, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,118:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=2.936e+02, with an active set of 27 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,118:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=2.929e+02, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,132:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=2.787e+02, with an active set of 28 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,132:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=2.783e+02, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,133:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=2.539e+02, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,133:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=2.382e+02, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,133:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=2.339e+02, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,133:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=2.239e+02, with an active set of 28 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,134:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=2.186e+02, with an active set of 29 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,135:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=2.097e+02, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,135:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=2.097e+02, with an active set of 31 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,136:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=2.097e+02, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,136:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=1.938e+02, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,137:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=1.644e+02, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,137:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=1.508e+02, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,137:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=1.473e+02, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,137:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=1.364e+02, with an active set of 32 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,138:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=1.344e+02, with an active set of 33 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,139:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=1.344e+02, with an active set of 33 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,139:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=1.343e+02, with an active set of 33 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,140:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=1.062e+02, with an active set of 34 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,140:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=9.881e+01, with an active set of 34 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,141:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=8.213e+01, with an active set of 34 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,141:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=8.165e+01, with an active set of 34 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,141:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=6.468e+01, with an active set of 34 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,142:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=5.206e+01, with an active set of 35 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,142:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=7.157e+02, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,142:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=5.204e+01, with an active set of 35 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,142:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=4.998e+01, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,143:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=4.981e+01, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,143:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=4.968e+01, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,143:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=4.405e+01, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,143:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=5.468e+02, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,144:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=4.396e+01, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,144:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=4.391e+01, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,144:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=5.166e+02, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,144:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=4.359e+01, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,144:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=3.275e+01, with an active set of 35 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,145:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=2.972e+01, with an active set of 35 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,145:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=4.649e+02, with an active set of 17 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,146:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=2.702e+01, with an active set of 35 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,145:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=5.427e+02, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,146:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=2.504e+01, with an active set of 35 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,146:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=2.082e+01, with an active set of 35 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,146:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=1.950e+01, with an active set of 35 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,146:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=3.989e+02, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,147:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=1.855e+01, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,147:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=1.835e+01, with an active set of 35 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,147:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=3.987e+02, with an active set of 20 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,147:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=1.833e+01, with an active set of 35 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,147:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=1.821e+01, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,147:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=3.668e+02, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,147:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=1.819e+01, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,148:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=1.817e+01, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,148:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=1.816e+01, with an active set of 35 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,148:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=3.037e+02, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,148:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=3.731e+02, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,148:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=1.814e+01, with an active set of 35 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,149:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=2.864e+02, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,149:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=1.617e+01, with an active set of 35 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,149:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=1.456e+01, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,149:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=1.173e+01, with an active set of 35 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,150:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=1.097e+01, with an active set of 35 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,150:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=1.018e+01, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,151:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=1.314e+03, with an active set of 29 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,151:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=1.017e+01, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,151:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=1.309e+03, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,152:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=3.160e+02, with an active set of 34 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,152:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=1.017e+01, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,152:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=7.290e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,152:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=5.049e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,152:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=3.140e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,153:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=1.289e+03, with an active set of 33 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,153:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=2.685e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,153:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=2.642e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,153:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=2.594e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,153:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=1.289e+03, with an active set of 34 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,154:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=2.552e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,154:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=4.040e-01, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,154:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=3.353e-01, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,155:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=3.593e+03, with an active set of 43 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,155:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=3.337e-01, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,156:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=3.321e-01, with an active set of 36 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,156:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=3.317e-01, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,157:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=3.305e-01, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,157:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=3.305e-01, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,158:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=3.305e-01, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,158:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 64 iterations, i.e. alpha=4.724e+03, with an active set of 52 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,158:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=3.305e-01, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,159:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=3.305e-01, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,159:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 65 iterations, i.e. alpha=4.727e+03, with an active set of 53 regressors, and the smallest cholesky pivot element being 7.814e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,159:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=3.299e-01, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,160:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=3.297e-01, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,160:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=3.289e-01, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,160:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=3.289e-01, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,160:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=2.201e-01, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,161:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=2.151e-01, with an active set of 36 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,162:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=5.010e-02, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,165:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 81 iterations, i.e. alpha=1.442e+06, with an active set of 63 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,165:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 82 iterations, i.e. alpha=1.442e+06, with an active set of 64 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,165:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 83 iterations, i.e. alpha=1.622e+06, with an active set of 65 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,181:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 152 iterations, i.e. alpha=2.039e+14, with an active set of 127 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,181:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 164 iterations, i.e. alpha=5.034e+14, with an active set of 139 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,181:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 164 iterations, i.e. alpha=4.207e+14, with an active set of 139 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,181:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=1.833e+05, with an active set of 31 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,181:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=1.826e+05, with an active set of 31 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,181:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=3.399e+02, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,181:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=2.148e+02, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,181:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=2.083e+02, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,181:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=1.733e+02, with an active set of 18 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,181:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=2.147e+05, with an active set of 36 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,196:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=2.115e+05, with an active set of 37 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,196:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=1.396e+03, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,196:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=1.291e+03, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,196:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=1.188e+03, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,196:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=2.115e+05, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,196:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=6.662e+02, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,196:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=6.714e+02, with an active set of 25 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,196:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=2.115e+05, with an active set of 39 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,196:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=6.636e+02, with an active set of 25 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,196:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=1.075e+03, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,196:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=3.680e+05, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,196:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=6.603e+02, with an active set of 27 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,196:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=3.680e+05, with an active set of 44 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,196:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=6.584e+02, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,196:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 53 iterations, i.e. alpha=3.695e+05, with an active set of 45 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,196:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=5.456e+02, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,196:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=5.148e+02, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,196:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 55 iterations, i.e. alpha=3.665e+05, with an active set of 47 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,196:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=5.084e+02, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,196:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=4.967e+02, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,196:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 56 iterations, i.e. alpha=3.666e+05, with an active set of 48 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,196:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=4.200e+02, with an active set of 29 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,196:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 58 iterations, i.e. alpha=3.665e+05, with an active set of 50 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,196:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 58 iterations, i.e. alpha=3.664e+05, with an active set of 50 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,196:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=4.142e+02, with an active set of 34 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,196:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=4.115e+02, with an active set of 38 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,196:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=4.113e+02, with an active set of 38 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,196:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=3.449e+02, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,196:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=4.095e+02, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,196:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=4.093e+02, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,196:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=4.093e+02, with an active set of 38 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,196:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.321e+02, with an active set of 19 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,196:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=3.780e+02, with an active set of 39 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,196:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.321e+02, with an active set of 19 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,196:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 51 iterations, i.e. alpha=3.769e+02, with an active set of 42 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,196:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=1.189e+02, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,196:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 51 iterations, i.e. alpha=3.769e+02, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,196:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=1.189e+02, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,196:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 51 iterations, i.e. alpha=3.768e+02, with an active set of 42 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,196:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 51 iterations, i.e. alpha=2.784e+02, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,196:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=1.173e+02, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,196:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 51 iterations, i.e. alpha=2.665e+02, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,196:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.142e+02, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,196:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=1.000e+02, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,196:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=6.666e+01, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,196:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=6.666e+01, with an active set of 27 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,212:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 65 iterations, i.e. alpha=4.720e+04, with an active set of 53 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,212:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=6.655e+01, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,212:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=6.652e+01, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,212:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=5.057e+01, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,212:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=5.056e+01, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,212:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=5.055e+01, with an active set of 28 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,212:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=5.017e+01, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,212:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=4.976e+01, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,212:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 72 iterations, i.e. alpha=2.741e+05, with an active set of 59 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,212:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=4.766e+01, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,212:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=4.765e+01, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,212:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=4.535e+01, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,212:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=4.534e+01, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,212:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=4.528e+01, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,212:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=4.527e+01, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,212:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=4.451e+01, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,212:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=4.398e+01, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,212:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=4.391e+01, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,212:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=4.375e+01, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,212:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=4.328e+01, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,212:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=4.326e+01, with an active set of 30 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,212:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=4.126e+01, with an active set of 30 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,212:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=4.013e+01, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,212:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=2.485e+01, with an active set of 32 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,212:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=2.485e+01, with an active set of 32 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,212:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=2.416e+01, with an active set of 33 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,212:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=2.323e+01, with an active set of 33 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,212:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=2.319e+01, with an active set of 33 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,212:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=2.643e+02, with an active set of 18 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,212:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=2.319e+01, with an active set of 33 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,212:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=2.318e+01, with an active set of 33 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,212:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=2.286e+01, with an active set of 33 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,212:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=1.629e+01, with an active set of 33 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,212:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=1.558e+01, with an active set of 33 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,212:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=1.540e+01, with an active set of 33 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,212:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=1.509e+01, with an active set of 33 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,212:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=1.508e+01, with an active set of 33 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,212:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=1.505e+01, with an active set of 33 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,212:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=1.438e+01, with an active set of 33 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,212:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=1.348e+01, with an active set of 33 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,212:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=1.327e+01, with an active set of 33 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,212:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=1.327e+01, with an active set of 33 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,212:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=1.316e+01, with an active set of 33 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,212:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=1.316e+01, with an active set of 33 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,212:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=1.316e+01, with an active set of 33 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,212:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=1.313e+01, with an active set of 33 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,212:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=1.231e+01, with an active set of 33 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,212:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=5.330e+00, with an active set of 34 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,212:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=8.073e+05, with an active set of 32 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,212:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=4.569e+00, with an active set of 34 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,212:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=4.164e+00, with an active set of 34 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,212:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=4.077e+00, with an active set of 34 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,212:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=4.014e+00, with an active set of 34 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,212:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=8.051e+05, with an active set of 34 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,212:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=3.998e+00, with an active set of 34 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,212:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=3.712e+00, with an active set of 34 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,212:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=3.712e+00, with an active set of 34 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,212:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=3.712e+00, with an active set of 34 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,212:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=7.978e+05, with an active set of 34 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,212:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=3.706e+00, with an active set of 34 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,212:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=3.688e+00, with an active set of 34 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,212:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=7.803e+05, with an active set of 34 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,212:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=3.665e+00, with an active set of 34 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,212:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=9.372e+02, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,212:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=3.653e+00, with an active set of 34 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,212:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=3.644e+00, with an active set of 34 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,212:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=3.629e+00, with an active set of 34 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,212:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=3.617e+00, with an active set of 34 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,212:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=7.160e+02, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,212:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=7.201e+05, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,212:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=3.614e+00, with an active set of 34 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,212:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=4.776e+02, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,212:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=3.164e+02, with an active set of 15 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,212:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=3.581e+00, with an active set of 34 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,212:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=3.110e+02, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,212:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=3.569e+00, with an active set of 34 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,212:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=9.573e+03, with an active set of 43 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,228:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=3.546e+00, with an active set of 34 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,212:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=2.593e+02, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,228:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=7.586e+05, with an active set of 39 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,228:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=2.426e+02, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,228:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=2.284e+00, with an active set of 34 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,228:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=1.006e+04, with an active set of 45 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,228:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=2.332e+02, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,228:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=1.824e+02, with an active set of 18 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,228:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=7.207e+05, with an active set of 40 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,228:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 56 iterations, i.e. alpha=9.553e+03, with an active set of 48 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,228:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=1.354e+02, with an active set of 19 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,228:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 57 iterations, i.e. alpha=9.567e+03, with an active set of 49 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,228:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=1.132e+02, with an active set of 20 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,228:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=7.203e+05, with an active set of 42 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,228:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 57 iterations, i.e. alpha=9.553e+03, with an active set of 49 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,228:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 62 iterations, i.e. alpha=9.554e+03, with an active set of 53 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,228:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=9.415e+01, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,228:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=9.412e+01, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,228:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=9.381e+01, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,228:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=6.649e+09, with an active set of 54 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,228:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=7.737e+01, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,228:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=7.510e+01, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,228:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=7.225e+01, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,228:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=6.632e+01, with an active set of 27 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,228:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 76 iterations, i.e. alpha=7.226e+10, with an active set of 64 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,228:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=5.686e+01, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,228:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 77 iterations, i.e. alpha=3.620e+09, with an active set of 63 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,228:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=5.434e+01, with an active set of 29 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,228:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=5.434e+01, with an active set of 29 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,228:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=5.342e+01, with an active set of 29 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,228:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 82 iterations, i.e. alpha=1.770e+09, with an active set of 67 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,228:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=5.011e+01, with an active set of 29 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,228:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=4.757e+01, with an active set of 29 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,228:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=4.665e+01, with an active set of 29 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,228:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=4.492e+01, with an active set of 29 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,228:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=4.450e+01, with an active set of 29 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,228:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=4.350e+01, with an active set of 29 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,228:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=4.335e+01, with an active set of 29 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,228:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=4.061e+01, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,228:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=4.061e+01, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,228:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=3.468e+01, with an active set of 34 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,228:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=3.245e+01, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,228:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=3.245e+01, with an active set of 35 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,243:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=3.244e+01, with an active set of 35 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,243:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=3.238e+01, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,243:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=3.199e+01, with an active set of 35 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,243:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=3.197e+01, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,243:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=3.135e+01, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,243:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=2.755e+01, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,243:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=2.345e+01, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,243:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=2.341e+01, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,243:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=2.336e+01, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,243:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=2.271e+01, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,243:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=2.267e+01, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,243:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=2.224e+01, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,243:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=2.202e+01, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,243:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=1.801e+01, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,243:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=1.698e+01, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,243:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=1.049e+01, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,243:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=1.045e+01, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,243:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=1.024e+01, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,243:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=9.762e+00, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,243:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=8.676e+00, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,243:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=8.216e+00, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,243:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=8.048e+00, with an active set of 38 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,243:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=8.023e+00, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,243:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=7.974e+00, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,243:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=7.916e+00, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,243:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=4.844e+00, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,243:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=4.796e+00, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,243:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=4.529e+00, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,243:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=3.055e+00, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,243:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=2.853e+00, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,243:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=2.834e+00, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,243:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=2.694e+00, with an active set of 38 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,243:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=2.693e+00, with an active set of 38 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,243:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=2.646e+00, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,243:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=2.640e+00, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,259:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=2.569e+00, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,259:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=2.286e+00, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,259:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=2.096e+00, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,259:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=2.057e+00, with an active set of 38 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,259:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=2.041e+00, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,259:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=2.039e+00, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,259:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=2.039e+00, with an active set of 38 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,259:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=9.837e-02, with an active set of 38 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,259:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=4.497e-02, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,271:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=1.021e+03, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,271:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=5.110e+02, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,271:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=4.956e+02, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,271:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=4.692e+02, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,271:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=4.594e+02, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,271:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=4.272e+02, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,271:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=2.815e+02, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,271:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=2.656e+02, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,275:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=2.648e+02, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,275:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=2.647e+02, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,275:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=1.500e+02, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,275:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=1.390e+02, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,275:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=1.361e+02, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,275:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=1.141e+02, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,275:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=1.140e+02, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,275:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=9.338e+01, with an active set of 28 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,275:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=9.218e+01, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,275:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=9.216e+01, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,275:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=8.782e+01, with an active set of 32 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,275:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=8.374e+01, with an active set of 34 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,275:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=8.271e+01, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,275:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=8.270e+01, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,275:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=8.270e+01, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,275:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=8.139e+01, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,275:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=6.560e+01, with an active set of 37 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,275:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=6.516e+01, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,275:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=6.516e+01, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,275:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=6.450e+01, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,275:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=5.096e+01, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,275:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=4.272e+01, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,291:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=4.266e+01, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,291:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=4.263e+01, with an active set of 42 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,291:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=4.260e+01, with an active set of 42 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,291:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=4.256e+01, with an active set of 42 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,291:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=3.941e+01, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,291:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=3.827e+01, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,291:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=3.826e+01, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,291:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=3.754e+01, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,291:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=3.433e+01, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,291:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=3.296e+01, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,291:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=2.918e+01, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,291:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=2.285e+01, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,291:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=2.272e+01, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,291:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=2.152e+01, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,291:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=1.998e+01, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,291:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=1.936e+01, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,291:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=1.822e+01, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,291:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=1.231e+01, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,291:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=1.102e+01, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,291:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=1.074e+01, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,291:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=9.651e+00, with an active set of 42 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,291:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=8.991e+00, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,291:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=8.776e+00, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,291:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=8.496e+00, with an active set of 43 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,291:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=8.180e+00, with an active set of 43 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,291:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=8.159e+00, with an active set of 43 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,291:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=7.744e+00, with an active set of 43 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,291:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=7.483e+00, with an active set of 43 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,291:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=6.826e+00, with an active set of 43 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,291:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=5.750e+00, with an active set of 43 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,291:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=5.456e+00, with an active set of 43 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,291:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=4.878e+00, with an active set of 43 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,291:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=4.661e+00, with an active set of 43 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,291:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=4.560e+00, with an active set of 43 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,291:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=3.342e+00, with an active set of 43 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,291:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=2.822e+00, with an active set of 43 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,291:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=1.734e+00, with an active set of 43 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,291:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 57 iterations, i.e. alpha=1.564e+00, with an active set of 46 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,291:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 57 iterations, i.e. alpha=1.248e+00, with an active set of 46 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,310:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 81 iterations, i.e. alpha=8.316e+01, with an active set of 57 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,311:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 82 iterations, i.e. alpha=7.536e+01, with an active set of 58 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,312:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 82 iterations, i.e. alpha=5.255e+01, with an active set of 58 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,312:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 82 iterations, i.e. alpha=5.212e+01, with an active set of 58 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,312:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 82 iterations, i.e. alpha=3.180e+01, with an active set of 58 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,313:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 82 iterations, i.e. alpha=2.554e+01, with an active set of 58 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,314:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 82 iterations, i.e. alpha=2.481e+01, with an active set of 58 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,314:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 82 iterations, i.e. alpha=2.447e+01, with an active set of 58 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,315:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 82 iterations, i.e. alpha=2.256e+01, with an active set of 58 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,315:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 82 iterations, i.e. alpha=2.230e+01, with an active set of 58 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,316:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 82 iterations, i.e. alpha=2.111e+01, with an active set of 58 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,316:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 82 iterations, i.e. alpha=2.101e+01, with an active set of 58 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,316:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 82 iterations, i.e. alpha=1.555e+01, with an active set of 58 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,316:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 82 iterations, i.e. alpha=1.527e+01, with an active set of 58 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,316:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 82 iterations, i.e. alpha=1.286e+01, with an active set of 58 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,316:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 82 iterations, i.e. alpha=1.255e+01, with an active set of 58 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,316:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 85 iterations, i.e. alpha=1.171e+01, with an active set of 61 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,316:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 85 iterations, i.e. alpha=1.104e+01, with an active set of 61 regressors, and the smallest cholesky pivot element being 2.788e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,316:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 89 iterations, i.e. alpha=5.204e+01, with an active set of 64 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:35,316:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 92 iterations, i.e. alpha=4.879e+01, with an active set of 66 regressors, and the smallest cholesky pivot element being 9.424e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:38,598:INFO:Calculating mean and std
2023-07-11 09:44:38,598:INFO:Creating metrics dataframe
2023-07-11 09:44:39,349:INFO:Uploading results into container
2023-07-11 09:44:39,349:INFO:Uploading model into container now
2023-07-11 09:44:39,349:INFO:_master_model_container: 5
2023-07-11 09:44:39,349:INFO:_display_container: 2
2023-07-11 09:44:39,349:INFO:Lars(random_state=8333)
2023-07-11 09:44:39,349:INFO:create_model() successfully completed......................................
2023-07-11 09:44:39,477:INFO:SubProcess create_model() end ==================================
2023-07-11 09:44:39,477:INFO:Creating metrics dataframe
2023-07-11 09:44:39,477:INFO:Initializing Lasso Least Angle Regression
2023-07-11 09:44:39,477:INFO:Total runtime is 0.3807418942451477 minutes
2023-07-11 09:44:39,477:INFO:SubProcess create_model() called ==================================
2023-07-11 09:44:39,477:INFO:Initializing create_model()
2023-07-11 09:44:39,477:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D6AFB820>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D74FAA00>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:44:39,477:INFO:Checking exceptions
2023-07-11 09:44:39,477:INFO:Importing libraries
2023-07-11 09:44:39,477:INFO:Copying training dataset
2023-07-11 09:44:39,491:INFO:Defining folds
2023-07-11 09:44:39,491:INFO:Declaring metric variables
2023-07-11 09:44:39,491:INFO:Importing untrained model
2023-07-11 09:44:39,491:INFO:Lasso Least Angle Regression Imported successfully
2023-07-11 09:44:39,491:INFO:Starting cross validation
2023-07-11 09:44:39,491:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:44:39,714:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=8.703e+01, with an active set of 18 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:39,716:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=8.703e+01, with an active set of 18 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:39,716:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=8.703e+01, with an active set of 18 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:39,716:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=5.539e+01, with an active set of 21 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:39,716:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=5.539e+01, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:39,716:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 36 iterations, alpha=4.930e+01, previous alpha=4.839e+01, with an active set of 23 regressors.
  warnings.warn(

2023-07-11 09:44:39,731:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.019e+03, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:39,731:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=8.575e+02, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:39,732:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=7.876e+02, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:39,733:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=5.246e+02, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:39,733:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 25 iterations, alpha=5.105e+02, previous alpha=5.020e+02, with an active set of 16 regressors.
  warnings.warn(

2023-07-11 09:44:39,740:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=1.001e+03, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:39,741:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=7.609e+02, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:39,741:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=7.213e+02, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:39,741:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=4.610e+02, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:39,742:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=5.292e+02, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:39,742:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=3.928e+02, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:39,742:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=3.901e+02, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:39,742:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=3.409e+02, with an active set of 18 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:39,743:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=2.604e+02, with an active set of 18 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:39,743:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=2.432e+02, with an active set of 18 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:39,743:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=2.006e+02, with an active set of 18 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:39,744:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.620e+02, with an active set of 18 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:39,744:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.324e+02, with an active set of 18 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:39,744:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.297e+02, with an active set of 18 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:39,744:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=9.783e+01, with an active set of 19 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:39,745:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 26 iterations, alpha=2.431e+02, previous alpha=9.783e+01, with an active set of 19 regressors.
  warnings.warn(

2023-07-11 09:44:39,765:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=2.462e+02, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:39,765:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=1.865e+02, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:39,765:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.118e+02, with an active set of 19 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:39,777:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.118e+02, with an active set of 19 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:39,777:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 28 iterations, alpha=1.118e+02, previous alpha=1.118e+02, with an active set of 19 regressors.
  warnings.warn(

2023-07-11 09:44:39,781:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=1.011e+03, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:39,781:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 19 iterations, alpha=8.308e+02, previous alpha=5.576e+02, with an active set of 16 regressors.
  warnings.warn(

2023-07-11 09:44:39,796:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=9.219e+02, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:39,796:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=7.127e+02, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:39,796:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=4.862e+02, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:39,796:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 17 iterations, alpha=6.012e+02, previous alpha=4.862e+02, with an active set of 14 regressors.
  warnings.warn(

2023-07-11 09:44:39,812:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=9.372e+02, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:39,812:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=7.160e+02, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:39,812:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 14 iterations, alpha=5.249e+02, previous alpha=4.824e+02, with an active set of 13 regressors.
  warnings.warn(

2023-07-11 09:44:39,828:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 21 iterations, alpha=5.649e+02, previous alpha=3.545e+02, with an active set of 16 regressors.
  warnings.warn(

2023-07-11 09:44:39,828:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 17 iterations, alpha=4.329e+02, previous alpha=3.970e+02, with an active set of 14 regressors.
  warnings.warn(

2023-07-11 09:44:40,628:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=9.801e+02, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:40,628:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=9.514e+02, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:40,628:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=7.226e+02, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:40,628:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=5.982e+02, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:40,628:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=4.976e+02, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:40,628:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=4.631e+02, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:40,628:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=4.225e+02, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:40,628:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=4.081e+02, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:40,628:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=4.948e+02, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-07-11 09:44:40,628:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 25 iterations, alpha=3.679e+02, previous alpha=2.391e+02, with an active set of 22 regressors.
  warnings.warn(

2023-07-11 09:44:43,228:INFO:Calculating mean and std
2023-07-11 09:44:43,228:INFO:Creating metrics dataframe
2023-07-11 09:44:43,985:INFO:Uploading results into container
2023-07-11 09:44:43,985:INFO:Uploading model into container now
2023-07-11 09:44:43,985:INFO:_master_model_container: 6
2023-07-11 09:44:43,985:INFO:_display_container: 2
2023-07-11 09:44:43,985:INFO:LassoLars(random_state=8333)
2023-07-11 09:44:43,985:INFO:create_model() successfully completed......................................
2023-07-11 09:44:44,117:INFO:SubProcess create_model() end ==================================
2023-07-11 09:44:44,117:INFO:Creating metrics dataframe
2023-07-11 09:44:44,134:INFO:Initializing Orthogonal Matching Pursuit
2023-07-11 09:44:44,134:INFO:Total runtime is 0.45836530526479086 minutes
2023-07-11 09:44:44,134:INFO:SubProcess create_model() called ==================================
2023-07-11 09:44:44,134:INFO:Initializing create_model()
2023-07-11 09:44:44,134:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D6AFB820>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D74FAA00>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:44:44,134:INFO:Checking exceptions
2023-07-11 09:44:44,134:INFO:Importing libraries
2023-07-11 09:44:44,134:INFO:Copying training dataset
2023-07-11 09:44:44,137:INFO:Defining folds
2023-07-11 09:44:44,137:INFO:Declaring metric variables
2023-07-11 09:44:44,137:INFO:Importing untrained model
2023-07-11 09:44:44,137:INFO:Orthogonal Matching Pursuit Imported successfully
2023-07-11 09:44:44,137:INFO:Starting cross validation
2023-07-11 09:44:44,138:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:44:47,829:INFO:Calculating mean and std
2023-07-11 09:44:47,829:INFO:Creating metrics dataframe
2023-07-11 09:44:48,533:INFO:Uploading results into container
2023-07-11 09:44:48,534:INFO:Uploading model into container now
2023-07-11 09:44:48,534:INFO:_master_model_container: 7
2023-07-11 09:44:48,534:INFO:_display_container: 2
2023-07-11 09:44:48,534:INFO:OrthogonalMatchingPursuit()
2023-07-11 09:44:48,534:INFO:create_model() successfully completed......................................
2023-07-11 09:44:48,644:INFO:SubProcess create_model() end ==================================
2023-07-11 09:44:48,644:INFO:Creating metrics dataframe
2023-07-11 09:44:48,644:INFO:Initializing Bayesian Ridge
2023-07-11 09:44:48,644:INFO:Total runtime is 0.5335395654042562 minutes
2023-07-11 09:44:48,644:INFO:SubProcess create_model() called ==================================
2023-07-11 09:44:48,644:INFO:Initializing create_model()
2023-07-11 09:44:48,644:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D6AFB820>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D74FAA00>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:44:48,644:INFO:Checking exceptions
2023-07-11 09:44:48,644:INFO:Importing libraries
2023-07-11 09:44:48,644:INFO:Copying training dataset
2023-07-11 09:44:48,644:INFO:Defining folds
2023-07-11 09:44:48,644:INFO:Declaring metric variables
2023-07-11 09:44:48,644:INFO:Importing untrained model
2023-07-11 09:44:48,644:INFO:Bayesian Ridge Imported successfully
2023-07-11 09:44:48,644:INFO:Starting cross validation
2023-07-11 09:44:48,644:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:44:52,303:INFO:Calculating mean and std
2023-07-11 09:44:52,303:INFO:Creating metrics dataframe
2023-07-11 09:44:52,916:INFO:Uploading results into container
2023-07-11 09:44:52,916:INFO:Uploading model into container now
2023-07-11 09:44:52,916:INFO:_master_model_container: 8
2023-07-11 09:44:52,916:INFO:_display_container: 2
2023-07-11 09:44:52,916:INFO:BayesianRidge()
2023-07-11 09:44:52,916:INFO:create_model() successfully completed......................................
2023-07-11 09:44:53,029:INFO:SubProcess create_model() end ==================================
2023-07-11 09:44:53,029:INFO:Creating metrics dataframe
2023-07-11 09:44:53,044:INFO:Initializing Passive Aggressive Regressor
2023-07-11 09:44:53,044:INFO:Total runtime is 0.6068704009056092 minutes
2023-07-11 09:44:53,044:INFO:SubProcess create_model() called ==================================
2023-07-11 09:44:53,044:INFO:Initializing create_model()
2023-07-11 09:44:53,044:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D6AFB820>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D74FAA00>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:44:53,044:INFO:Checking exceptions
2023-07-11 09:44:53,044:INFO:Importing libraries
2023-07-11 09:44:53,044:INFO:Copying training dataset
2023-07-11 09:44:53,044:INFO:Defining folds
2023-07-11 09:44:53,044:INFO:Declaring metric variables
2023-07-11 09:44:53,044:INFO:Importing untrained model
2023-07-11 09:44:53,044:INFO:Passive Aggressive Regressor Imported successfully
2023-07-11 09:44:53,044:INFO:Starting cross validation
2023-07-11 09:44:53,044:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:44:56,582:INFO:Calculating mean and std
2023-07-11 09:44:56,582:INFO:Creating metrics dataframe
2023-07-11 09:44:57,276:INFO:Uploading results into container
2023-07-11 09:44:57,276:INFO:Uploading model into container now
2023-07-11 09:44:57,276:INFO:_master_model_container: 9
2023-07-11 09:44:57,276:INFO:_display_container: 2
2023-07-11 09:44:57,276:INFO:PassiveAggressiveRegressor(random_state=8333)
2023-07-11 09:44:57,276:INFO:create_model() successfully completed......................................
2023-07-11 09:44:57,398:INFO:SubProcess create_model() end ==================================
2023-07-11 09:44:57,398:INFO:Creating metrics dataframe
2023-07-11 09:44:57,411:INFO:Initializing Huber Regressor
2023-07-11 09:44:57,411:INFO:Total runtime is 0.6796466271082561 minutes
2023-07-11 09:44:57,411:INFO:SubProcess create_model() called ==================================
2023-07-11 09:44:57,411:INFO:Initializing create_model()
2023-07-11 09:44:57,411:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D6AFB820>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D74FAA00>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:44:57,411:INFO:Checking exceptions
2023-07-11 09:44:57,411:INFO:Importing libraries
2023-07-11 09:44:57,411:INFO:Copying training dataset
2023-07-11 09:44:57,414:INFO:Defining folds
2023-07-11 09:44:57,414:INFO:Declaring metric variables
2023-07-11 09:44:57,414:INFO:Importing untrained model
2023-07-11 09:44:57,414:INFO:Huber Regressor Imported successfully
2023-07-11 09:44:57,414:INFO:Starting cross validation
2023-07-11 09:44:57,414:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:45:02,308:INFO:Calculating mean and std
2023-07-11 09:45:02,308:INFO:Creating metrics dataframe
2023-07-11 09:45:03,059:INFO:Uploading results into container
2023-07-11 09:45:03,059:INFO:Uploading model into container now
2023-07-11 09:45:03,059:INFO:_master_model_container: 10
2023-07-11 09:45:03,059:INFO:_display_container: 2
2023-07-11 09:45:03,059:INFO:HuberRegressor()
2023-07-11 09:45:03,059:INFO:create_model() successfully completed......................................
2023-07-11 09:45:03,196:INFO:SubProcess create_model() end ==================================
2023-07-11 09:45:03,196:INFO:Creating metrics dataframe
2023-07-11 09:45:03,196:INFO:Initializing K Neighbors Regressor
2023-07-11 09:45:03,196:INFO:Total runtime is 0.7760698954264323 minutes
2023-07-11 09:45:03,196:INFO:SubProcess create_model() called ==================================
2023-07-11 09:45:03,196:INFO:Initializing create_model()
2023-07-11 09:45:03,196:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D6AFB820>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D74FAA00>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:45:03,196:INFO:Checking exceptions
2023-07-11 09:45:03,196:INFO:Importing libraries
2023-07-11 09:45:03,196:INFO:Copying training dataset
2023-07-11 09:45:03,212:INFO:Defining folds
2023-07-11 09:45:03,212:INFO:Declaring metric variables
2023-07-11 09:45:03,212:INFO:Importing untrained model
2023-07-11 09:45:03,212:INFO:K Neighbors Regressor Imported successfully
2023-07-11 09:45:03,212:INFO:Starting cross validation
2023-07-11 09:45:03,212:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:45:06,907:INFO:Calculating mean and std
2023-07-11 09:45:06,907:INFO:Creating metrics dataframe
2023-07-11 09:45:07,597:INFO:Uploading results into container
2023-07-11 09:45:07,597:INFO:Uploading model into container now
2023-07-11 09:45:07,597:INFO:_master_model_container: 11
2023-07-11 09:45:07,597:INFO:_display_container: 2
2023-07-11 09:45:07,597:INFO:KNeighborsRegressor(n_jobs=-1)
2023-07-11 09:45:07,597:INFO:create_model() successfully completed......................................
2023-07-11 09:45:07,727:INFO:SubProcess create_model() end ==================================
2023-07-11 09:45:07,727:INFO:Creating metrics dataframe
2023-07-11 09:45:07,735:INFO:Initializing Decision Tree Regressor
2023-07-11 09:45:07,735:INFO:Total runtime is 0.8517088174819947 minutes
2023-07-11 09:45:07,735:INFO:SubProcess create_model() called ==================================
2023-07-11 09:45:07,735:INFO:Initializing create_model()
2023-07-11 09:45:07,735:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D6AFB820>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D74FAA00>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:45:07,735:INFO:Checking exceptions
2023-07-11 09:45:07,736:INFO:Importing libraries
2023-07-11 09:45:07,736:INFO:Copying training dataset
2023-07-11 09:45:07,738:INFO:Defining folds
2023-07-11 09:45:07,738:INFO:Declaring metric variables
2023-07-11 09:45:07,738:INFO:Importing untrained model
2023-07-11 09:45:07,739:INFO:Decision Tree Regressor Imported successfully
2023-07-11 09:45:07,739:INFO:Starting cross validation
2023-07-11 09:45:07,740:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:45:11,475:INFO:Calculating mean and std
2023-07-11 09:45:11,475:INFO:Creating metrics dataframe
2023-07-11 09:45:12,182:INFO:Uploading results into container
2023-07-11 09:45:12,182:INFO:Uploading model into container now
2023-07-11 09:45:12,182:INFO:_master_model_container: 12
2023-07-11 09:45:12,182:INFO:_display_container: 2
2023-07-11 09:45:12,182:INFO:DecisionTreeRegressor(random_state=8333)
2023-07-11 09:45:12,182:INFO:create_model() successfully completed......................................
2023-07-11 09:45:12,317:INFO:SubProcess create_model() end ==================================
2023-07-11 09:45:12,317:INFO:Creating metrics dataframe
2023-07-11 09:45:12,327:INFO:Initializing Random Forest Regressor
2023-07-11 09:45:12,327:INFO:Total runtime is 0.9282524943351746 minutes
2023-07-11 09:45:12,327:INFO:SubProcess create_model() called ==================================
2023-07-11 09:45:12,328:INFO:Initializing create_model()
2023-07-11 09:45:12,328:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D6AFB820>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D74FAA00>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:45:12,329:INFO:Checking exceptions
2023-07-11 09:45:12,329:INFO:Importing libraries
2023-07-11 09:45:12,329:INFO:Copying training dataset
2023-07-11 09:45:12,336:INFO:Defining folds
2023-07-11 09:45:12,336:INFO:Declaring metric variables
2023-07-11 09:45:12,336:INFO:Importing untrained model
2023-07-11 09:45:12,336:INFO:Random Forest Regressor Imported successfully
2023-07-11 09:45:12,337:INFO:Starting cross validation
2023-07-11 09:45:12,338:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:45:17,197:INFO:Calculating mean and std
2023-07-11 09:45:17,197:INFO:Creating metrics dataframe
2023-07-11 09:45:17,812:INFO:Uploading results into container
2023-07-11 09:45:17,828:INFO:Uploading model into container now
2023-07-11 09:45:17,828:INFO:_master_model_container: 13
2023-07-11 09:45:17,828:INFO:_display_container: 2
2023-07-11 09:45:17,828:INFO:RandomForestRegressor(n_jobs=-1, random_state=8333)
2023-07-11 09:45:17,828:INFO:create_model() successfully completed......................................
2023-07-11 09:45:17,966:INFO:SubProcess create_model() end ==================================
2023-07-11 09:45:17,966:INFO:Creating metrics dataframe
2023-07-11 09:45:17,966:INFO:Initializing Extra Trees Regressor
2023-07-11 09:45:17,966:INFO:Total runtime is 1.0222341736157736 minutes
2023-07-11 09:45:17,966:INFO:SubProcess create_model() called ==================================
2023-07-11 09:45:17,966:INFO:Initializing create_model()
2023-07-11 09:45:17,966:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D6AFB820>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D74FAA00>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:45:17,966:INFO:Checking exceptions
2023-07-11 09:45:17,966:INFO:Importing libraries
2023-07-11 09:45:17,966:INFO:Copying training dataset
2023-07-11 09:45:17,966:INFO:Defining folds
2023-07-11 09:45:17,966:INFO:Declaring metric variables
2023-07-11 09:45:17,966:INFO:Importing untrained model
2023-07-11 09:45:17,966:INFO:Extra Trees Regressor Imported successfully
2023-07-11 09:45:17,966:INFO:Starting cross validation
2023-07-11 09:45:17,966:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:45:22,720:INFO:Calculating mean and std
2023-07-11 09:45:22,721:INFO:Creating metrics dataframe
2023-07-11 09:45:23,382:INFO:Uploading results into container
2023-07-11 09:45:23,382:INFO:Uploading model into container now
2023-07-11 09:45:23,382:INFO:_master_model_container: 14
2023-07-11 09:45:23,382:INFO:_display_container: 2
2023-07-11 09:45:23,382:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=8333)
2023-07-11 09:45:23,382:INFO:create_model() successfully completed......................................
2023-07-11 09:45:23,509:INFO:SubProcess create_model() end ==================================
2023-07-11 09:45:23,509:INFO:Creating metrics dataframe
2023-07-11 09:45:23,517:INFO:Initializing AdaBoost Regressor
2023-07-11 09:45:23,517:INFO:Total runtime is 1.1147524237632753 minutes
2023-07-11 09:45:23,517:INFO:SubProcess create_model() called ==================================
2023-07-11 09:45:23,517:INFO:Initializing create_model()
2023-07-11 09:45:23,517:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D6AFB820>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D74FAA00>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:45:23,518:INFO:Checking exceptions
2023-07-11 09:45:23,518:INFO:Importing libraries
2023-07-11 09:45:23,518:INFO:Copying training dataset
2023-07-11 09:45:23,520:INFO:Defining folds
2023-07-11 09:45:23,521:INFO:Declaring metric variables
2023-07-11 09:45:23,521:INFO:Importing untrained model
2023-07-11 09:45:23,522:INFO:AdaBoost Regressor Imported successfully
2023-07-11 09:45:23,522:INFO:Starting cross validation
2023-07-11 09:45:23,525:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:45:28,261:INFO:Calculating mean and std
2023-07-11 09:45:28,261:INFO:Creating metrics dataframe
2023-07-11 09:45:28,877:INFO:Uploading results into container
2023-07-11 09:45:28,877:INFO:Uploading model into container now
2023-07-11 09:45:28,877:INFO:_master_model_container: 15
2023-07-11 09:45:28,877:INFO:_display_container: 2
2023-07-11 09:45:28,877:INFO:AdaBoostRegressor(random_state=8333)
2023-07-11 09:45:28,877:INFO:create_model() successfully completed......................................
2023-07-11 09:45:28,997:INFO:SubProcess create_model() end ==================================
2023-07-11 09:45:28,997:INFO:Creating metrics dataframe
2023-07-11 09:45:28,997:INFO:Initializing Gradient Boosting Regressor
2023-07-11 09:45:29,012:INFO:Total runtime is 1.206337058544159 minutes
2023-07-11 09:45:29,012:INFO:SubProcess create_model() called ==================================
2023-07-11 09:45:29,012:INFO:Initializing create_model()
2023-07-11 09:45:29,012:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D6AFB820>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D74FAA00>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:45:29,012:INFO:Checking exceptions
2023-07-11 09:45:29,012:INFO:Importing libraries
2023-07-11 09:45:29,012:INFO:Copying training dataset
2023-07-11 09:45:29,012:INFO:Defining folds
2023-07-11 09:45:29,012:INFO:Declaring metric variables
2023-07-11 09:45:29,012:INFO:Importing untrained model
2023-07-11 09:45:29,012:INFO:Gradient Boosting Regressor Imported successfully
2023-07-11 09:45:29,012:INFO:Starting cross validation
2023-07-11 09:45:29,028:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:45:33,751:INFO:Calculating mean and std
2023-07-11 09:45:33,751:INFO:Creating metrics dataframe
2023-07-11 09:45:34,413:INFO:Uploading results into container
2023-07-11 09:45:34,413:INFO:Uploading model into container now
2023-07-11 09:45:34,413:INFO:_master_model_container: 16
2023-07-11 09:45:34,413:INFO:_display_container: 2
2023-07-11 09:45:34,413:INFO:GradientBoostingRegressor(random_state=8333)
2023-07-11 09:45:34,413:INFO:create_model() successfully completed......................................
2023-07-11 09:45:34,558:INFO:SubProcess create_model() end ==================================
2023-07-11 09:45:34,558:INFO:Creating metrics dataframe
2023-07-11 09:45:34,561:INFO:Initializing Extreme Gradient Boosting
2023-07-11 09:45:34,561:INFO:Total runtime is 1.2988202412923178 minutes
2023-07-11 09:45:34,561:INFO:SubProcess create_model() called ==================================
2023-07-11 09:45:34,561:INFO:Initializing create_model()
2023-07-11 09:45:34,561:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D6AFB820>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D74FAA00>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:45:34,561:INFO:Checking exceptions
2023-07-11 09:45:34,561:INFO:Importing libraries
2023-07-11 09:45:34,562:INFO:Copying training dataset
2023-07-11 09:45:34,564:INFO:Defining folds
2023-07-11 09:45:34,564:INFO:Declaring metric variables
2023-07-11 09:45:34,564:INFO:Importing untrained model
2023-07-11 09:45:34,565:INFO:Extreme Gradient Boosting Imported successfully
2023-07-11 09:45:34,565:INFO:Starting cross validation
2023-07-11 09:45:34,566:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:45:38,861:WARNING:create_model() for xgboost raised an exception or returned all 0.0, trying without fit_kwargs:
2023-07-11 09:45:38,861:WARNING:Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 792, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\core.py", line 620, in inner_f
    return func(**kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\sklearn.py", line 988, in fit
    train_dmatrix, evals = _wrap_evaluation_matrices(
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\sklearn.py", line 448, in _wrap_evaluation_matrices
    train_dmatrix = create_dmatrix(
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\sklearn.py", line 908, in _create_dmatrix
    return DMatrix(**kwargs, nthread=self.n_jobs)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\core.py", line 620, in inner_f
    return func(**kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\core.py", line 766, in __init__
    self.feature_names = feature_names
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\core.py", line 1155, in feature_names
    raise ValueError('feature_names must be unique')
ValueError: feature_names must be unique


2023-07-11 09:45:38,861:INFO:Initializing create_model()
2023-07-11 09:45:38,861:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D6AFB820>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D74FAA00>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:45:38,861:INFO:Checking exceptions
2023-07-11 09:45:38,861:INFO:Importing libraries
2023-07-11 09:45:38,861:INFO:Copying training dataset
2023-07-11 09:45:38,876:INFO:Defining folds
2023-07-11 09:45:38,876:INFO:Declaring metric variables
2023-07-11 09:45:38,876:INFO:Importing untrained model
2023-07-11 09:45:38,876:INFO:Extreme Gradient Boosting Imported successfully
2023-07-11 09:45:38,880:INFO:Starting cross validation
2023-07-11 09:45:38,880:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:45:43,554:ERROR:create_model() for xgboost raised an exception or returned all 0.0:
2023-07-11 09:45:43,555:ERROR:Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 792, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\core.py", line 620, in inner_f
    return func(**kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\sklearn.py", line 988, in fit
    train_dmatrix, evals = _wrap_evaluation_matrices(
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\sklearn.py", line 448, in _wrap_evaluation_matrices
    train_dmatrix = create_dmatrix(
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\sklearn.py", line 908, in _create_dmatrix
    return DMatrix(**kwargs, nthread=self.n_jobs)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\core.py", line 620, in inner_f
    return func(**kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\core.py", line 766, in __init__
    self.feature_names = feature_names
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\core.py", line 1155, in feature_names
    raise ValueError('feature_names must be unique')
ValueError: feature_names must be unique


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 808, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\core.py", line 620, in inner_f
    return func(**kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\sklearn.py", line 988, in fit
    train_dmatrix, evals = _wrap_evaluation_matrices(
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\sklearn.py", line 448, in _wrap_evaluation_matrices
    train_dmatrix = create_dmatrix(
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\sklearn.py", line 908, in _create_dmatrix
    return DMatrix(**kwargs, nthread=self.n_jobs)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\core.py", line 620, in inner_f
    return func(**kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\core.py", line 766, in __init__
    self.feature_names = feature_names
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\core.py", line 1155, in feature_names
    raise ValueError('feature_names must be unique')
ValueError: feature_names must be unique


2023-07-11 09:45:44,615:INFO:Initializing Light Gradient Boosting Machine
2023-07-11 09:45:44,615:INFO:Total runtime is 1.4663811047871909 minutes
2023-07-11 09:45:44,615:INFO:SubProcess create_model() called ==================================
2023-07-11 09:45:44,615:INFO:Initializing create_model()
2023-07-11 09:45:44,615:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D6AFB820>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D74FAA00>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:45:44,615:INFO:Checking exceptions
2023-07-11 09:45:44,615:INFO:Importing libraries
2023-07-11 09:45:44,615:INFO:Copying training dataset
2023-07-11 09:45:44,631:INFO:Defining folds
2023-07-11 09:45:44,631:INFO:Declaring metric variables
2023-07-11 09:45:44,631:INFO:Importing untrained model
2023-07-11 09:45:44,631:INFO:Light Gradient Boosting Machine Imported successfully
2023-07-11 09:45:44,631:INFO:Starting cross validation
2023-07-11 09:45:44,631:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:45:48,878:WARNING:create_model() for lightgbm raised an exception or returned all 0.0, trying without fit_kwargs:
2023-07-11 09:45:48,878:WARNING:Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 792, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
1 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\sklearn.py", line 895, in fit
    super().fit(X, y, sample_weight=sample_weight, init_score=init_score,
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\sklearn.py", line 748, in fit
    self._Booster = train(
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\engine.py", line 271, in train
    booster = Booster(params=params, train_set=train_set)
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 2605, in __init__
    train_set.construct()
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 1815, in construct
    self._lazy_init(self.data, label=self.label,
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 1573, in _lazy_init
    return self.set_feature_name(feature_name)
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 2142, in set_feature_name
    _safe_call(_LIB.LGBM_DatasetSetFeatureNames(
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 125, in _safe_call
    raise LightGBMError(_LIB.LGBM_GetLastError().decode('utf-8'))
lightgbm.basic.LightGBMError: Feature (salary_currency_USD) appears more than one time.

--------------------------------------------------------------------------------
9 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\sklearn.py", line 895, in fit
    super().fit(X, y, sample_weight=sample_weight, init_score=init_score,
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\sklearn.py", line 748, in fit
    self._Booster = train(
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\engine.py", line 271, in train
    booster = Booster(params=params, train_set=train_set)
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 2605, in __init__
    train_set.construct()
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 1815, in construct
    self._lazy_init(self.data, label=self.label,
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 1573, in _lazy_init
    return self.set_feature_name(feature_name)
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 2142, in set_feature_name
    _safe_call(_LIB.LGBM_DatasetSetFeatureNames(
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 125, in _safe_call
    raise LightGBMError(_LIB.LGBM_GetLastError().decode('utf-8'))
lightgbm.basic.LightGBMError: Feature (salary_currency_GBP) appears more than one time.


2023-07-11 09:45:48,878:INFO:Initializing create_model()
2023-07-11 09:45:48,878:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D6AFB820>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D74FAA00>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:45:48,878:INFO:Checking exceptions
2023-07-11 09:45:48,878:INFO:Importing libraries
2023-07-11 09:45:48,878:INFO:Copying training dataset
2023-07-11 09:45:48,893:INFO:Defining folds
2023-07-11 09:45:48,893:INFO:Declaring metric variables
2023-07-11 09:45:48,893:INFO:Importing untrained model
2023-07-11 09:45:48,893:INFO:Light Gradient Boosting Machine Imported successfully
2023-07-11 09:45:48,893:INFO:Starting cross validation
2023-07-11 09:45:48,893:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:45:53,477:ERROR:create_model() for lightgbm raised an exception or returned all 0.0:
2023-07-11 09:45:53,477:ERROR:Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 792, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
1 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\sklearn.py", line 895, in fit
    super().fit(X, y, sample_weight=sample_weight, init_score=init_score,
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\sklearn.py", line 748, in fit
    self._Booster = train(
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\engine.py", line 271, in train
    booster = Booster(params=params, train_set=train_set)
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 2605, in __init__
    train_set.construct()
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 1815, in construct
    self._lazy_init(self.data, label=self.label,
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 1573, in _lazy_init
    return self.set_feature_name(feature_name)
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 2142, in set_feature_name
    _safe_call(_LIB.LGBM_DatasetSetFeatureNames(
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 125, in _safe_call
    raise LightGBMError(_LIB.LGBM_GetLastError().decode('utf-8'))
lightgbm.basic.LightGBMError: Feature (salary_currency_USD) appears more than one time.

--------------------------------------------------------------------------------
9 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\sklearn.py", line 895, in fit
    super().fit(X, y, sample_weight=sample_weight, init_score=init_score,
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\sklearn.py", line 748, in fit
    self._Booster = train(
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\engine.py", line 271, in train
    booster = Booster(params=params, train_set=train_set)
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 2605, in __init__
    train_set.construct()
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 1815, in construct
    self._lazy_init(self.data, label=self.label,
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 1573, in _lazy_init
    return self.set_feature_name(feature_name)
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 2142, in set_feature_name
    _safe_call(_LIB.LGBM_DatasetSetFeatureNames(
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 125, in _safe_call
    raise LightGBMError(_LIB.LGBM_GetLastError().decode('utf-8'))
lightgbm.basic.LightGBMError: Feature (salary_currency_GBP) appears more than one time.


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 808, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
1 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\sklearn.py", line 895, in fit
    super().fit(X, y, sample_weight=sample_weight, init_score=init_score,
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\sklearn.py", line 748, in fit
    self._Booster = train(
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\engine.py", line 271, in train
    booster = Booster(params=params, train_set=train_set)
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 2605, in __init__
    train_set.construct()
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 1815, in construct
    self._lazy_init(self.data, label=self.label,
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 1573, in _lazy_init
    return self.set_feature_name(feature_name)
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 2142, in set_feature_name
    _safe_call(_LIB.LGBM_DatasetSetFeatureNames(
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 125, in _safe_call
    raise LightGBMError(_LIB.LGBM_GetLastError().decode('utf-8'))
lightgbm.basic.LightGBMError: Feature (salary_currency_USD) appears more than one time.

--------------------------------------------------------------------------------
9 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\sklearn.py", line 895, in fit
    super().fit(X, y, sample_weight=sample_weight, init_score=init_score,
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\sklearn.py", line 748, in fit
    self._Booster = train(
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\engine.py", line 271, in train
    booster = Booster(params=params, train_set=train_set)
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 2605, in __init__
    train_set.construct()
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 1815, in construct
    self._lazy_init(self.data, label=self.label,
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 1573, in _lazy_init
    return self.set_feature_name(feature_name)
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 2142, in set_feature_name
    _safe_call(_LIB.LGBM_DatasetSetFeatureNames(
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 125, in _safe_call
    raise LightGBMError(_LIB.LGBM_GetLastError().decode('utf-8'))
lightgbm.basic.LightGBMError: Feature (salary_currency_GBP) appears more than one time.


2023-07-11 09:45:54,534:INFO:Initializing Dummy Regressor
2023-07-11 09:45:54,534:INFO:Total runtime is 1.631699868043264 minutes
2023-07-11 09:45:54,534:INFO:SubProcess create_model() called ==================================
2023-07-11 09:45:54,534:INFO:Initializing create_model()
2023-07-11 09:45:54,535:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D6AFB820>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D74FAA00>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:45:54,535:INFO:Checking exceptions
2023-07-11 09:45:54,535:INFO:Importing libraries
2023-07-11 09:45:54,535:INFO:Copying training dataset
2023-07-11 09:45:54,538:INFO:Defining folds
2023-07-11 09:45:54,538:INFO:Declaring metric variables
2023-07-11 09:45:54,538:INFO:Importing untrained model
2023-07-11 09:45:54,538:INFO:Dummy Regressor Imported successfully
2023-07-11 09:45:54,538:INFO:Starting cross validation
2023-07-11 09:45:54,539:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:45:58,416:INFO:Calculating mean and std
2023-07-11 09:45:58,416:INFO:Creating metrics dataframe
2023-07-11 09:45:59,092:INFO:Uploading results into container
2023-07-11 09:45:59,092:INFO:Uploading model into container now
2023-07-11 09:45:59,092:INFO:_master_model_container: 17
2023-07-11 09:45:59,092:INFO:_display_container: 2
2023-07-11 09:45:59,092:INFO:DummyRegressor()
2023-07-11 09:45:59,092:INFO:create_model() successfully completed......................................
2023-07-11 09:45:59,230:INFO:SubProcess create_model() end ==================================
2023-07-11 09:45:59,230:INFO:Creating metrics dataframe
2023-07-11 09:45:59,230:INFO:Initializing create_model()
2023-07-11 09:45:59,230:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D6AFB820>, estimator=ExtraTreesRegressor(n_jobs=-1, random_state=8333), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:45:59,230:INFO:Checking exceptions
2023-07-11 09:45:59,230:INFO:Importing libraries
2023-07-11 09:45:59,230:INFO:Copying training dataset
2023-07-11 09:45:59,230:INFO:Defining folds
2023-07-11 09:45:59,230:INFO:Declaring metric variables
2023-07-11 09:45:59,230:INFO:Importing untrained model
2023-07-11 09:45:59,230:INFO:Declaring custom model
2023-07-11 09:45:59,230:INFO:Extra Trees Regressor Imported successfully
2023-07-11 09:45:59,246:INFO:Cross validation set to False
2023-07-11 09:45:59,246:INFO:Fitting Model
2023-07-11 09:45:59,968:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=8333)
2023-07-11 09:45:59,968:INFO:create_model() successfully completed......................................
2023-07-11 09:46:00,118:INFO:_master_model_container: 17
2023-07-11 09:46:00,118:INFO:_display_container: 2
2023-07-11 09:46:00,118:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=8333)
2023-07-11 09:46:00,118:INFO:compare_models() successfully completed......................................
2023-07-11 09:46:00,155:INFO:Initializing save_model()
2023-07-11 09:46:00,156:INFO:save_model(model=ExtraTreesRegressor(n_jobs=-1, random_state=8333), model_name=best_model.pkl, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\didit\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['work_year', 'salary',
                                             'remote_ratio'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['experience_level',
                                             'employment_type', 'job_title',
                                             'salary_currency',
                                             'employee_residence',
                                             'company_l...
                                    transformer=OneHotEncoder(cols=['experience_level',
                                                                    'employment_type',
                                                                    'salary_currency',
                                                                    'company_size'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('rest_encoding',
                 TransformerWrapper(include=['job_title', 'employee_residence',
                                             'company_location'],
                                    transformer=TargetEncoder(cols=['job_title',
                                                                    'employee_residence',
                                                                    'company_location'],
                                                              handle_missing='return_nan')))]), verbose=True, use_case=MLUsecase.REGRESSION, kwargs={})
2023-07-11 09:46:00,156:INFO:Adding model into prep_pipe
2023-07-11 09:46:00,200:INFO:best_model.pkl.pkl saved in current working directory
2023-07-11 09:46:00,208:INFO:Pipeline(memory=FastMemory(location=C:\Users\didit\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['work_year', 'salary',
                                             'remote_ratio'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['experience_level',
                                             'employment_type', 'job_title',
                                             'salary_currency',
                                             'employee_residence',
                                             'company_l...
                                                                    'salary_currency',
                                                                    'company_size'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('rest_encoding',
                 TransformerWrapper(include=['job_title', 'employee_residence',
                                             'company_location'],
                                    transformer=TargetEncoder(cols=['job_title',
                                                                    'employee_residence',
                                                                    'company_location'],
                                                              handle_missing='return_nan'))),
                ('trained_model',
                 ExtraTreesRegressor(n_jobs=-1, random_state=8333))])
2023-07-11 09:46:00,208:INFO:save_model() successfully completed......................................
2023-07-11 09:54:14,977:INFO:PyCaret RegressionExperiment
2023-07-11 09:54:14,977:INFO:Logging name: reg-default-name
2023-07-11 09:54:14,993:INFO:ML Usecase: MLUsecase.REGRESSION
2023-07-11 09:54:14,993:INFO:version 3.0.2
2023-07-11 09:54:14,993:INFO:Initializing setup()
2023-07-11 09:54:14,993:INFO:self.USI: 0fba
2023-07-11 09:54:14,993:INFO:self._variable_keys: {'_ml_usecase', 'log_plots_param', 'USI', 'X_train', 'pipeline', 'gpu_param', 'exp_name_log', 'y', 'html_param', 'X', 'data', 'exp_id', 'y_train', 'target_param', 'X_test', 'fold_groups_param', 'n_jobs_param', 'idx', 'fold_generator', 'logging_param', 'memory', 'gpu_n_jobs_param', 'fold_shuffle_param', '_available_plots', 'seed', 'y_test', 'transform_target_param'}
2023-07-11 09:54:14,993:INFO:Checking environment
2023-07-11 09:54:14,993:INFO:python_version: 3.9.13
2023-07-11 09:54:14,993:INFO:python_build: ('main', 'Oct 13 2022 21:23:06')
2023-07-11 09:54:14,993:INFO:machine: AMD64
2023-07-11 09:54:14,993:INFO:platform: Windows-10-10.0.22621-SP0
2023-07-11 09:54:14,993:INFO:Memory: svmem(total=16893358080, available=4000604160, percent=76.3, used=12892753920, free=4000604160)
2023-07-11 09:54:14,993:INFO:Physical Core: 8
2023-07-11 09:54:14,993:INFO:Logical Core: 16
2023-07-11 09:54:14,993:INFO:Checking libraries
2023-07-11 09:54:14,993:INFO:System:
2023-07-11 09:54:14,993:INFO:    python: 3.9.13 (main, Oct 13 2022, 21:23:06) [MSC v.1916 64 bit (AMD64)]
2023-07-11 09:54:14,993:INFO:executable: C:\Users\didit\anaconda3\python.exe
2023-07-11 09:54:14,993:INFO:   machine: Windows-10-10.0.22621-SP0
2023-07-11 09:54:14,993:INFO:PyCaret required dependencies:
2023-07-11 09:54:14,993:INFO:                 pip: 23.0.1
2023-07-11 09:54:14,993:INFO:          setuptools: 67.8.0
2023-07-11 09:54:14,993:INFO:             pycaret: 3.0.2
2023-07-11 09:54:14,993:INFO:             IPython: 8.12.0
2023-07-11 09:54:14,993:INFO:          ipywidgets: 8.0.4
2023-07-11 09:54:14,993:INFO:                tqdm: 4.65.0
2023-07-11 09:54:14,993:INFO:               numpy: 1.21.5
2023-07-11 09:54:14,993:INFO:              pandas: 1.5.3
2023-07-11 09:54:14,993:INFO:              jinja2: 3.1.2
2023-07-11 09:54:14,993:INFO:               scipy: 1.10.1
2023-07-11 09:54:14,993:INFO:              joblib: 1.2.0
2023-07-11 09:54:14,993:INFO:             sklearn: 1.2.2
2023-07-11 09:54:14,993:INFO:                pyod: 1.0.9
2023-07-11 09:54:14,993:INFO:            imblearn: 0.10.1
2023-07-11 09:54:14,993:INFO:   category_encoders: 2.6.1
2023-07-11 09:54:14,993:INFO:            lightgbm: 3.3.5
2023-07-11 09:54:14,993:INFO:               numba: 0.57.0
2023-07-11 09:54:14,993:INFO:            requests: 2.29.0
2023-07-11 09:54:14,993:INFO:          matplotlib: 3.7.1
2023-07-11 09:54:14,993:INFO:          scikitplot: 0.3.7
2023-07-11 09:54:14,993:INFO:         yellowbrick: 1.5
2023-07-11 09:54:14,993:INFO:              plotly: 5.9.0
2023-07-11 09:54:14,993:INFO:             kaleido: 0.2.1
2023-07-11 09:54:14,993:INFO:         statsmodels: 0.13.5
2023-07-11 09:54:14,993:INFO:              sktime: 0.17.0
2023-07-11 09:54:14,993:INFO:               tbats: 1.1.3
2023-07-11 09:54:14,993:INFO:            pmdarima: 2.0.3
2023-07-11 09:54:14,993:INFO:              psutil: 5.9.0
2023-07-11 09:54:14,993:INFO:PyCaret optional dependencies:
2023-07-11 09:54:14,993:INFO:                shap: 0.41.0
2023-07-11 09:54:14,993:INFO:           interpret: Not installed
2023-07-11 09:54:14,993:INFO:                umap: Not installed
2023-07-11 09:54:14,993:INFO:    pandas_profiling: 4.3.1
2023-07-11 09:54:14,993:INFO:  explainerdashboard: Not installed
2023-07-11 09:54:14,993:INFO:             autoviz: Not installed
2023-07-11 09:54:14,993:INFO:           fairlearn: Not installed
2023-07-11 09:54:14,993:INFO:             xgboost: 1.7.6
2023-07-11 09:54:14,993:INFO:            catboost: Not installed
2023-07-11 09:54:14,993:INFO:              kmodes: Not installed
2023-07-11 09:54:14,993:INFO:             mlxtend: Not installed
2023-07-11 09:54:14,993:INFO:       statsforecast: Not installed
2023-07-11 09:54:14,993:INFO:        tune_sklearn: Not installed
2023-07-11 09:54:14,993:INFO:                 ray: Not installed
2023-07-11 09:54:14,993:INFO:            hyperopt: Not installed
2023-07-11 09:54:14,993:INFO:              optuna: Not installed
2023-07-11 09:54:14,993:INFO:               skopt: Not installed
2023-07-11 09:54:14,993:INFO:              mlflow: 2.4.2
2023-07-11 09:54:14,993:INFO:              gradio: Not installed
2023-07-11 09:54:14,993:INFO:             fastapi: 0.95.2
2023-07-11 09:54:14,993:INFO:             uvicorn: 0.22.0
2023-07-11 09:54:14,993:INFO:              m2cgen: Not installed
2023-07-11 09:54:14,993:INFO:           evidently: Not installed
2023-07-11 09:54:14,993:INFO:               fugue: Not installed
2023-07-11 09:54:14,993:INFO:           streamlit: 1.23.1
2023-07-11 09:54:14,993:INFO:             prophet: Not installed
2023-07-11 09:54:14,993:INFO:None
2023-07-11 09:54:14,993:INFO:Set up data.
2023-07-11 09:54:15,008:INFO:Set up train/test split.
2023-07-11 09:54:15,008:INFO:Set up index.
2023-07-11 09:54:15,008:INFO:Set up folding strategy.
2023-07-11 09:54:15,008:INFO:Assigning column types.
2023-07-11 09:54:15,008:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-07-11 09:54:15,008:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-07-11 09:54:15,008:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-07-11 09:54:15,022:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 09:54:15,071:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 09:54:15,113:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 09:54:15,113:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:54:15,113:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:54:15,113:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-07-11 09:54:15,122:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-07-11 09:54:15,122:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 09:54:15,164:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 09:54:15,195:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 09:54:15,195:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:54:15,195:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:54:15,195:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-07-11 09:54:15,195:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-07-11 09:54:15,195:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 09:54:15,242:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 09:54:15,274:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 09:54:15,274:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:54:15,274:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:54:15,274:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-07-11 09:54:15,274:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 09:54:15,321:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 09:54:15,347:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 09:54:15,347:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:54:15,347:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:54:15,347:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-07-11 09:54:15,347:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 09:54:15,394:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 09:54:15,425:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 09:54:15,425:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:54:15,425:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:54:15,425:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 09:54:15,472:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 09:54:15,513:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 09:54:15,513:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:54:15,513:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:54:15,513:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-07-11 09:54:15,563:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 09:54:15,594:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 09:54:15,594:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:54:15,594:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:54:15,641:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 09:54:15,673:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 09:54:15,673:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:54:15,673:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:54:15,673:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-07-11 09:54:15,725:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 09:54:15,747:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:54:15,747:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:54:15,794:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 09:54:15,841:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:54:15,841:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:54:15,841:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-07-11 09:54:15,913:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:54:15,913:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:54:16,009:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:54:16,009:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:54:16,009:INFO:Preparing preprocessing pipeline...
2023-07-11 09:54:16,009:INFO:Set up simple imputation.
2023-07-11 09:54:16,009:INFO:Set up encoding of ordinal features.
2023-07-11 09:54:16,009:INFO:Set up encoding of categorical features.
2023-07-11 09:54:16,009:INFO:Set up column name cleaning.
2023-07-11 09:54:16,088:INFO:Finished creating preprocessing pipeline.
2023-07-11 09:54:16,112:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\didit\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['Age', 'Credit Utilization Ratio',
                                             'Payment History',
                                             'Number of Credit Accounts',
                                             'Loan Amount', 'Interest Rate',
                                             'Loan Term'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['Gender', 'Education Lev...
                                                                         'mapping': Female    0
Male      1
NaN      -1
dtype: int64}]))),
                ('onehot_encoding',
                 TransformerWrapper(include=['Education Level',
                                             'Employment Status',
                                             'Type of Loan'],
                                    transformer=OneHotEncoder(cols=['Education '
                                                                    'Level',
                                                                    'Employment '
                                                                    'Status',
                                                                    'Type of '
                                                                    'Loan'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames()))])
2023-07-11 09:54:16,112:INFO:Creating final display dataframe.
2023-07-11 09:54:16,288:INFO:Setup _display_container:                     Description             Value
0                    Session id              3876
1                        Target    Marital Status
2                   Target type        Regression
3           Original data shape        (1000, 12)
4        Transformed data shape        (1000, 19)
5   Transformed train set shape         (700, 19)
6    Transformed test set shape         (300, 19)
7              Ordinal features                 1
8              Numeric features                 7
9          Categorical features                 4
10                   Preprocess              True
11              Imputation type            simple
12           Numeric imputation              mean
13       Categorical imputation              mode
14     Maximum one-hot encoding                25
15              Encoding method              None
16               Fold Generator             KFold
17                  Fold Number                10
18                     CPU Jobs                -1
19                      Use GPU             False
20               Log Experiment             False
21              Experiment Name  reg-default-name
22                          USI              0fba
2023-07-11 09:54:16,394:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:54:16,394:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:54:16,473:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:54:16,473:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:54:16,473:INFO:setup() successfully completed in 1.8s...............
2023-07-11 09:54:16,473:INFO:Initializing compare_models()
2023-07-11 09:54:16,473:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D76B8340>, include=None, fold=None, round=4, cross_validation=True, sort=R2, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x000001E1D76B8340>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2023-07-11 09:54:16,488:INFO:Checking exceptions
2023-07-11 09:54:16,489:INFO:Preparing display monitor
2023-07-11 09:54:16,490:INFO:Initializing Linear Regression
2023-07-11 09:54:16,490:INFO:Total runtime is 0.0 minutes
2023-07-11 09:54:16,491:INFO:SubProcess create_model() called ==================================
2023-07-11 09:54:16,491:INFO:Initializing create_model()
2023-07-11 09:54:16,491:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D76B8340>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D92A1D30>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:54:16,491:INFO:Checking exceptions
2023-07-11 09:54:16,491:INFO:Importing libraries
2023-07-11 09:54:16,491:INFO:Copying training dataset
2023-07-11 09:54:16,493:INFO:Defining folds
2023-07-11 09:54:16,493:INFO:Declaring metric variables
2023-07-11 09:54:16,493:INFO:Importing untrained model
2023-07-11 09:54:16,493:INFO:Linear Regression Imported successfully
2023-07-11 09:54:16,493:INFO:Starting cross validation
2023-07-11 09:54:16,494:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:54:25,554:WARNING:create_model() for lr raised an exception or returned all 0.0, trying without fit_kwargs:
2023-07-11 09:54:25,562:WARNING:Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 792, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
1 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_base.py", line 648, in fit
    X, y = self._validate_data(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1122, in check_X_y
    y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1147, in _check_y
    y = y.astype(np.float64)
ValueError: could not convert string to float: 'Divorced'

--------------------------------------------------------------------------------
9 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_base.py", line 648, in fit
    X, y = self._validate_data(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1122, in check_X_y
    y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1147, in _check_y
    y = y.astype(np.float64)
ValueError: could not convert string to float: 'Married'


2023-07-11 09:54:25,562:INFO:Initializing create_model()
2023-07-11 09:54:25,562:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D76B8340>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D92A1D30>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:54:25,562:INFO:Checking exceptions
2023-07-11 09:54:25,562:INFO:Importing libraries
2023-07-11 09:54:25,562:INFO:Copying training dataset
2023-07-11 09:54:25,562:INFO:Defining folds
2023-07-11 09:54:25,562:INFO:Declaring metric variables
2023-07-11 09:54:25,562:INFO:Importing untrained model
2023-07-11 09:54:25,562:INFO:Linear Regression Imported successfully
2023-07-11 09:54:25,562:INFO:Starting cross validation
2023-07-11 09:54:25,562:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:54:30,610:ERROR:create_model() for lr raised an exception or returned all 0.0:
2023-07-11 09:54:30,610:ERROR:Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 792, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
1 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_base.py", line 648, in fit
    X, y = self._validate_data(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1122, in check_X_y
    y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1147, in _check_y
    y = y.astype(np.float64)
ValueError: could not convert string to float: 'Divorced'

--------------------------------------------------------------------------------
9 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_base.py", line 648, in fit
    X, y = self._validate_data(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1122, in check_X_y
    y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1147, in _check_y
    y = y.astype(np.float64)
ValueError: could not convert string to float: 'Married'


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 808, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
1 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_base.py", line 648, in fit
    X, y = self._validate_data(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1122, in check_X_y
    y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1147, in _check_y
    y = y.astype(np.float64)
ValueError: could not convert string to float: 'Divorced'

--------------------------------------------------------------------------------
9 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_base.py", line 648, in fit
    X, y = self._validate_data(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1122, in check_X_y
    y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1147, in _check_y
    y = y.astype(np.float64)
ValueError: could not convert string to float: 'Married'


2023-07-11 09:54:31,639:INFO:Initializing Lasso Regression
2023-07-11 09:54:31,639:INFO:Total runtime is 0.2524935801823934 minutes
2023-07-11 09:54:31,639:INFO:SubProcess create_model() called ==================================
2023-07-11 09:54:31,639:INFO:Initializing create_model()
2023-07-11 09:54:31,639:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D76B8340>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D92A1D30>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:54:31,639:INFO:Checking exceptions
2023-07-11 09:54:31,639:INFO:Importing libraries
2023-07-11 09:54:31,639:INFO:Copying training dataset
2023-07-11 09:54:31,639:INFO:Defining folds
2023-07-11 09:54:31,639:INFO:Declaring metric variables
2023-07-11 09:54:31,639:INFO:Importing untrained model
2023-07-11 09:54:31,639:INFO:Lasso Regression Imported successfully
2023-07-11 09:54:31,639:INFO:Starting cross validation
2023-07-11 09:54:31,639:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:54:35,777:WARNING:create_model() for lasso raised an exception or returned all 0.0, trying without fit_kwargs:
2023-07-11 09:54:35,777:WARNING:Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 792, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
1 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py", line 908, in fit
    X, y = self._validate_data(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1122, in check_X_y
    y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1147, in _check_y
    y = y.astype(np.float64)
ValueError: could not convert string to float: 'Divorced'

--------------------------------------------------------------------------------
9 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py", line 908, in fit
    X, y = self._validate_data(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1122, in check_X_y
    y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1147, in _check_y
    y = y.astype(np.float64)
ValueError: could not convert string to float: 'Married'


2023-07-11 09:54:35,777:INFO:Initializing create_model()
2023-07-11 09:54:35,777:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D76B8340>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D92A1D30>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:54:35,777:INFO:Checking exceptions
2023-07-11 09:54:35,777:INFO:Importing libraries
2023-07-11 09:54:35,777:INFO:Copying training dataset
2023-07-11 09:54:35,793:INFO:Defining folds
2023-07-11 09:54:35,793:INFO:Declaring metric variables
2023-07-11 09:54:35,793:INFO:Importing untrained model
2023-07-11 09:54:35,793:INFO:Lasso Regression Imported successfully
2023-07-11 09:54:35,793:INFO:Starting cross validation
2023-07-11 09:54:35,793:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:54:40,040:ERROR:create_model() for lasso raised an exception or returned all 0.0:
2023-07-11 09:54:40,040:ERROR:Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 792, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
1 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py", line 908, in fit
    X, y = self._validate_data(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1122, in check_X_y
    y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1147, in _check_y
    y = y.astype(np.float64)
ValueError: could not convert string to float: 'Divorced'

--------------------------------------------------------------------------------
9 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py", line 908, in fit
    X, y = self._validate_data(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1122, in check_X_y
    y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1147, in _check_y
    y = y.astype(np.float64)
ValueError: could not convert string to float: 'Married'


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 808, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
1 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py", line 908, in fit
    X, y = self._validate_data(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1122, in check_X_y
    y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1147, in _check_y
    y = y.astype(np.float64)
ValueError: could not convert string to float: 'Divorced'

--------------------------------------------------------------------------------
9 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py", line 908, in fit
    X, y = self._validate_data(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1122, in check_X_y
    y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1147, in _check_y
    y = y.astype(np.float64)
ValueError: could not convert string to float: 'Married'


2023-07-11 09:54:41,062:INFO:Initializing Ridge Regression
2023-07-11 09:54:41,062:INFO:Total runtime is 0.4095427831013997 minutes
2023-07-11 09:54:41,063:INFO:SubProcess create_model() called ==================================
2023-07-11 09:54:41,063:INFO:Initializing create_model()
2023-07-11 09:54:41,063:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D76B8340>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D92A1D30>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:54:41,063:INFO:Checking exceptions
2023-07-11 09:54:41,063:INFO:Importing libraries
2023-07-11 09:54:41,063:INFO:Copying training dataset
2023-07-11 09:54:41,069:INFO:Defining folds
2023-07-11 09:54:41,069:INFO:Declaring metric variables
2023-07-11 09:54:41,069:INFO:Importing untrained model
2023-07-11 09:54:41,070:INFO:Ridge Regression Imported successfully
2023-07-11 09:54:41,070:INFO:Starting cross validation
2023-07-11 09:54:41,072:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:54:44,825:WARNING:create_model() for ridge raised an exception or returned all 0.0, trying without fit_kwargs:
2023-07-11 09:54:44,825:WARNING:Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 792, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
1 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py", line 1126, in fit
    X, y = self._validate_data(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1122, in check_X_y
    y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1147, in _check_y
    y = y.astype(np.float64)
ValueError: could not convert string to float: 'Divorced'

--------------------------------------------------------------------------------
9 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py", line 1126, in fit
    X, y = self._validate_data(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1122, in check_X_y
    y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1147, in _check_y
    y = y.astype(np.float64)
ValueError: could not convert string to float: 'Married'


2023-07-11 09:54:44,825:INFO:Initializing create_model()
2023-07-11 09:54:44,825:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D76B8340>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D92A1D30>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:54:44,825:INFO:Checking exceptions
2023-07-11 09:54:44,825:INFO:Importing libraries
2023-07-11 09:54:44,825:INFO:Copying training dataset
2023-07-11 09:54:44,825:INFO:Defining folds
2023-07-11 09:54:44,825:INFO:Declaring metric variables
2023-07-11 09:54:44,841:INFO:Importing untrained model
2023-07-11 09:54:44,841:INFO:Ridge Regression Imported successfully
2023-07-11 09:54:44,841:INFO:Starting cross validation
2023-07-11 09:54:44,841:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:54:49,133:ERROR:create_model() for ridge raised an exception or returned all 0.0:
2023-07-11 09:54:49,134:ERROR:Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 792, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
1 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py", line 1126, in fit
    X, y = self._validate_data(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1122, in check_X_y
    y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1147, in _check_y
    y = y.astype(np.float64)
ValueError: could not convert string to float: 'Divorced'

--------------------------------------------------------------------------------
9 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py", line 1126, in fit
    X, y = self._validate_data(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1122, in check_X_y
    y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1147, in _check_y
    y = y.astype(np.float64)
ValueError: could not convert string to float: 'Married'


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 808, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
1 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py", line 1126, in fit
    X, y = self._validate_data(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1122, in check_X_y
    y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1147, in _check_y
    y = y.astype(np.float64)
ValueError: could not convert string to float: 'Divorced'

--------------------------------------------------------------------------------
9 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py", line 1126, in fit
    X, y = self._validate_data(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1122, in check_X_y
    y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1147, in _check_y
    y = y.astype(np.float64)
ValueError: could not convert string to float: 'Married'


2023-07-11 09:54:50,129:INFO:Initializing Elastic Net
2023-07-11 09:54:50,130:INFO:Total runtime is 0.5606641888618469 minutes
2023-07-11 09:54:50,130:INFO:SubProcess create_model() called ==================================
2023-07-11 09:54:50,130:INFO:Initializing create_model()
2023-07-11 09:54:50,130:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D76B8340>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D92A1D30>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:54:50,130:INFO:Checking exceptions
2023-07-11 09:54:50,130:INFO:Importing libraries
2023-07-11 09:54:50,130:INFO:Copying training dataset
2023-07-11 09:54:50,133:INFO:Defining folds
2023-07-11 09:54:50,133:INFO:Declaring metric variables
2023-07-11 09:54:50,133:INFO:Importing untrained model
2023-07-11 09:54:50,133:INFO:Elastic Net Imported successfully
2023-07-11 09:54:50,133:INFO:Starting cross validation
2023-07-11 09:54:50,134:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:54:54,042:WARNING:create_model() for en raised an exception or returned all 0.0, trying without fit_kwargs:
2023-07-11 09:54:54,042:WARNING:Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 792, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
1 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py", line 908, in fit
    X, y = self._validate_data(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1122, in check_X_y
    y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1147, in _check_y
    y = y.astype(np.float64)
ValueError: could not convert string to float: 'Divorced'

--------------------------------------------------------------------------------
9 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py", line 908, in fit
    X, y = self._validate_data(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1122, in check_X_y
    y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1147, in _check_y
    y = y.astype(np.float64)
ValueError: could not convert string to float: 'Married'


2023-07-11 09:54:54,042:INFO:Initializing create_model()
2023-07-11 09:54:54,042:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D76B8340>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D92A1D30>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:54:54,042:INFO:Checking exceptions
2023-07-11 09:54:54,042:INFO:Importing libraries
2023-07-11 09:54:54,042:INFO:Copying training dataset
2023-07-11 09:54:54,042:INFO:Defining folds
2023-07-11 09:54:54,042:INFO:Declaring metric variables
2023-07-11 09:54:54,042:INFO:Importing untrained model
2023-07-11 09:54:54,042:INFO:Elastic Net Imported successfully
2023-07-11 09:54:54,042:INFO:Starting cross validation
2023-07-11 09:54:54,042:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:54:58,464:ERROR:create_model() for en raised an exception or returned all 0.0:
2023-07-11 09:54:58,464:ERROR:Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 792, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
1 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py", line 908, in fit
    X, y = self._validate_data(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1122, in check_X_y
    y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1147, in _check_y
    y = y.astype(np.float64)
ValueError: could not convert string to float: 'Divorced'

--------------------------------------------------------------------------------
9 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py", line 908, in fit
    X, y = self._validate_data(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1122, in check_X_y
    y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1147, in _check_y
    y = y.astype(np.float64)
ValueError: could not convert string to float: 'Married'


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 808, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
1 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py", line 908, in fit
    X, y = self._validate_data(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1122, in check_X_y
    y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1147, in _check_y
    y = y.astype(np.float64)
ValueError: could not convert string to float: 'Divorced'

--------------------------------------------------------------------------------
9 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py", line 908, in fit
    X, y = self._validate_data(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1122, in check_X_y
    y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1147, in _check_y
    y = y.astype(np.float64)
ValueError: could not convert string to float: 'Married'


2023-07-11 09:54:59,627:INFO:Initializing Least Angle Regression
2023-07-11 09:54:59,627:INFO:Total runtime is 0.7189483284950255 minutes
2023-07-11 09:54:59,627:INFO:SubProcess create_model() called ==================================
2023-07-11 09:54:59,627:INFO:Initializing create_model()
2023-07-11 09:54:59,627:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D76B8340>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D92A1D30>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:54:59,627:INFO:Checking exceptions
2023-07-11 09:54:59,627:INFO:Importing libraries
2023-07-11 09:54:59,627:INFO:Copying training dataset
2023-07-11 09:54:59,627:INFO:Defining folds
2023-07-11 09:54:59,627:INFO:Declaring metric variables
2023-07-11 09:54:59,627:INFO:Importing untrained model
2023-07-11 09:54:59,627:INFO:Least Angle Regression Imported successfully
2023-07-11 09:54:59,627:INFO:Starting cross validation
2023-07-11 09:54:59,627:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:55:03,806:WARNING:create_model() for lar raised an exception or returned all 0.0, trying without fit_kwargs:
2023-07-11 09:55:03,806:WARNING:Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 792, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
1 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py", line 1125, in fit
    X, y = self._validate_data(X, y, y_numeric=True, multi_output=True)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1122, in check_X_y
    y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1147, in _check_y
    y = y.astype(np.float64)
ValueError: could not convert string to float: 'Divorced'

--------------------------------------------------------------------------------
9 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py", line 1125, in fit
    X, y = self._validate_data(X, y, y_numeric=True, multi_output=True)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1122, in check_X_y
    y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1147, in _check_y
    y = y.astype(np.float64)
ValueError: could not convert string to float: 'Married'


2023-07-11 09:55:03,806:INFO:Initializing create_model()
2023-07-11 09:55:03,806:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D76B8340>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D92A1D30>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:55:03,806:INFO:Checking exceptions
2023-07-11 09:55:03,806:INFO:Importing libraries
2023-07-11 09:55:03,806:INFO:Copying training dataset
2023-07-11 09:55:03,822:INFO:Defining folds
2023-07-11 09:55:03,822:INFO:Declaring metric variables
2023-07-11 09:55:03,822:INFO:Importing untrained model
2023-07-11 09:55:03,822:INFO:Least Angle Regression Imported successfully
2023-07-11 09:55:03,822:INFO:Starting cross validation
2023-07-11 09:55:03,822:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:55:07,959:ERROR:create_model() for lar raised an exception or returned all 0.0:
2023-07-11 09:55:07,959:ERROR:Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 792, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
1 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py", line 1125, in fit
    X, y = self._validate_data(X, y, y_numeric=True, multi_output=True)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1122, in check_X_y
    y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1147, in _check_y
    y = y.astype(np.float64)
ValueError: could not convert string to float: 'Divorced'

--------------------------------------------------------------------------------
9 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py", line 1125, in fit
    X, y = self._validate_data(X, y, y_numeric=True, multi_output=True)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1122, in check_X_y
    y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1147, in _check_y
    y = y.astype(np.float64)
ValueError: could not convert string to float: 'Married'


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 808, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
1 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py", line 1125, in fit
    X, y = self._validate_data(X, y, y_numeric=True, multi_output=True)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1122, in check_X_y
    y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1147, in _check_y
    y = y.astype(np.float64)
ValueError: could not convert string to float: 'Divorced'

--------------------------------------------------------------------------------
9 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py", line 1125, in fit
    X, y = self._validate_data(X, y, y_numeric=True, multi_output=True)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1122, in check_X_y
    y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1147, in _check_y
    y = y.astype(np.float64)
ValueError: could not convert string to float: 'Married'


2023-07-11 09:55:09,150:INFO:Initializing Lasso Least Angle Regression
2023-07-11 09:55:09,150:INFO:Total runtime is 0.877671698729197 minutes
2023-07-11 09:55:09,150:INFO:SubProcess create_model() called ==================================
2023-07-11 09:55:09,150:INFO:Initializing create_model()
2023-07-11 09:55:09,150:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D76B8340>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D92A1D30>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:55:09,150:INFO:Checking exceptions
2023-07-11 09:55:09,150:INFO:Importing libraries
2023-07-11 09:55:09,150:INFO:Copying training dataset
2023-07-11 09:55:09,153:INFO:Defining folds
2023-07-11 09:55:09,153:INFO:Declaring metric variables
2023-07-11 09:55:09,153:INFO:Importing untrained model
2023-07-11 09:55:09,154:INFO:Lasso Least Angle Regression Imported successfully
2023-07-11 09:55:09,154:INFO:Starting cross validation
2023-07-11 09:55:09,155:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:55:13,165:WARNING:create_model() for llar raised an exception or returned all 0.0, trying without fit_kwargs:
2023-07-11 09:55:13,165:WARNING:Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 792, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
1 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py", line 1125, in fit
    X, y = self._validate_data(X, y, y_numeric=True, multi_output=True)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1122, in check_X_y
    y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1147, in _check_y
    y = y.astype(np.float64)
ValueError: could not convert string to float: 'Divorced'

--------------------------------------------------------------------------------
9 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py", line 1125, in fit
    X, y = self._validate_data(X, y, y_numeric=True, multi_output=True)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1122, in check_X_y
    y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1147, in _check_y
    y = y.astype(np.float64)
ValueError: could not convert string to float: 'Married'


2023-07-11 09:55:13,165:INFO:Initializing create_model()
2023-07-11 09:55:13,165:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D76B8340>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D92A1D30>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:55:13,165:INFO:Checking exceptions
2023-07-11 09:55:13,165:INFO:Importing libraries
2023-07-11 09:55:13,165:INFO:Copying training dataset
2023-07-11 09:55:13,165:INFO:Defining folds
2023-07-11 09:55:13,165:INFO:Declaring metric variables
2023-07-11 09:55:13,165:INFO:Importing untrained model
2023-07-11 09:55:13,165:INFO:Lasso Least Angle Regression Imported successfully
2023-07-11 09:55:13,165:INFO:Starting cross validation
2023-07-11 09:55:13,165:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:55:17,420:ERROR:create_model() for llar raised an exception or returned all 0.0:
2023-07-11 09:55:17,421:ERROR:Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 792, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
1 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py", line 1125, in fit
    X, y = self._validate_data(X, y, y_numeric=True, multi_output=True)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1122, in check_X_y
    y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1147, in _check_y
    y = y.astype(np.float64)
ValueError: could not convert string to float: 'Divorced'

--------------------------------------------------------------------------------
9 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py", line 1125, in fit
    X, y = self._validate_data(X, y, y_numeric=True, multi_output=True)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1122, in check_X_y
    y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1147, in _check_y
    y = y.astype(np.float64)
ValueError: could not convert string to float: 'Married'


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 808, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
1 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py", line 1125, in fit
    X, y = self._validate_data(X, y, y_numeric=True, multi_output=True)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1122, in check_X_y
    y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1147, in _check_y
    y = y.astype(np.float64)
ValueError: could not convert string to float: 'Divorced'

--------------------------------------------------------------------------------
9 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py", line 1125, in fit
    X, y = self._validate_data(X, y, y_numeric=True, multi_output=True)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1122, in check_X_y
    y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1147, in _check_y
    y = y.astype(np.float64)
ValueError: could not convert string to float: 'Married'


2023-07-11 09:55:18,512:INFO:Initializing Orthogonal Matching Pursuit
2023-07-11 09:55:18,512:INFO:Total runtime is 1.0336960037549336 minutes
2023-07-11 09:55:18,512:INFO:SubProcess create_model() called ==================================
2023-07-11 09:55:18,512:INFO:Initializing create_model()
2023-07-11 09:55:18,512:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D76B8340>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D92A1D30>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:55:18,512:INFO:Checking exceptions
2023-07-11 09:55:18,512:INFO:Importing libraries
2023-07-11 09:55:18,512:INFO:Copying training dataset
2023-07-11 09:55:18,521:INFO:Defining folds
2023-07-11 09:55:18,521:INFO:Declaring metric variables
2023-07-11 09:55:18,521:INFO:Importing untrained model
2023-07-11 09:55:18,521:INFO:Orthogonal Matching Pursuit Imported successfully
2023-07-11 09:55:18,521:INFO:Starting cross validation
2023-07-11 09:55:18,521:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:55:22,398:WARNING:create_model() for omp raised an exception or returned all 0.0, trying without fit_kwargs:
2023-07-11 09:55:22,398:WARNING:Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 792, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
1 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_omp.py", line 741, in fit
    X, y = self._validate_data(X, y, multi_output=True, y_numeric=True)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1122, in check_X_y
    y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1147, in _check_y
    y = y.astype(np.float64)
ValueError: could not convert string to float: 'Divorced'

--------------------------------------------------------------------------------
9 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_omp.py", line 741, in fit
    X, y = self._validate_data(X, y, multi_output=True, y_numeric=True)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1122, in check_X_y
    y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1147, in _check_y
    y = y.astype(np.float64)
ValueError: could not convert string to float: 'Married'


2023-07-11 09:55:22,398:INFO:Initializing create_model()
2023-07-11 09:55:22,398:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D76B8340>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D92A1D30>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:55:22,398:INFO:Checking exceptions
2023-07-11 09:55:22,398:INFO:Importing libraries
2023-07-11 09:55:22,398:INFO:Copying training dataset
2023-07-11 09:55:22,398:INFO:Defining folds
2023-07-11 09:55:22,398:INFO:Declaring metric variables
2023-07-11 09:55:22,398:INFO:Importing untrained model
2023-07-11 09:55:22,398:INFO:Orthogonal Matching Pursuit Imported successfully
2023-07-11 09:55:22,398:INFO:Starting cross validation
2023-07-11 09:55:22,398:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:55:26,690:ERROR:create_model() for omp raised an exception or returned all 0.0:
2023-07-11 09:55:26,690:ERROR:Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 792, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
1 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_omp.py", line 741, in fit
    X, y = self._validate_data(X, y, multi_output=True, y_numeric=True)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1122, in check_X_y
    y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1147, in _check_y
    y = y.astype(np.float64)
ValueError: could not convert string to float: 'Divorced'

--------------------------------------------------------------------------------
9 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_omp.py", line 741, in fit
    X, y = self._validate_data(X, y, multi_output=True, y_numeric=True)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1122, in check_X_y
    y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1147, in _check_y
    y = y.astype(np.float64)
ValueError: could not convert string to float: 'Married'


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 808, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
1 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_omp.py", line 741, in fit
    X, y = self._validate_data(X, y, multi_output=True, y_numeric=True)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1122, in check_X_y
    y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1147, in _check_y
    y = y.astype(np.float64)
ValueError: could not convert string to float: 'Divorced'

--------------------------------------------------------------------------------
9 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_omp.py", line 741, in fit
    X, y = self._validate_data(X, y, multi_output=True, y_numeric=True)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1122, in check_X_y
    y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1147, in _check_y
    y = y.astype(np.float64)
ValueError: could not convert string to float: 'Married'


2023-07-11 09:55:27,726:INFO:Initializing Bayesian Ridge
2023-07-11 09:55:27,726:INFO:Total runtime is 1.1872716426849366 minutes
2023-07-11 09:55:27,726:INFO:SubProcess create_model() called ==================================
2023-07-11 09:55:27,727:INFO:Initializing create_model()
2023-07-11 09:55:27,727:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D76B8340>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D92A1D30>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:55:27,727:INFO:Checking exceptions
2023-07-11 09:55:27,727:INFO:Importing libraries
2023-07-11 09:55:27,727:INFO:Copying training dataset
2023-07-11 09:55:27,730:INFO:Defining folds
2023-07-11 09:55:27,730:INFO:Declaring metric variables
2023-07-11 09:55:27,731:INFO:Importing untrained model
2023-07-11 09:55:27,731:INFO:Bayesian Ridge Imported successfully
2023-07-11 09:55:27,731:INFO:Starting cross validation
2023-07-11 09:55:27,732:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:55:31,853:WARNING:create_model() for br raised an exception or returned all 0.0, trying without fit_kwargs:
2023-07-11 09:55:31,853:WARNING:Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 792, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
1 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_bayes.py", line 231, in fit
    X, y = self._validate_data(X, y, dtype=[np.float64, np.float32], y_numeric=True)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1122, in check_X_y
    y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1147, in _check_y
    y = y.astype(np.float64)
ValueError: could not convert string to float: 'Divorced'

--------------------------------------------------------------------------------
9 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_bayes.py", line 231, in fit
    X, y = self._validate_data(X, y, dtype=[np.float64, np.float32], y_numeric=True)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1122, in check_X_y
    y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1147, in _check_y
    y = y.astype(np.float64)
ValueError: could not convert string to float: 'Married'


2023-07-11 09:55:31,853:INFO:Initializing create_model()
2023-07-11 09:55:31,853:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D76B8340>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D92A1D30>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:55:31,853:INFO:Checking exceptions
2023-07-11 09:55:31,853:INFO:Importing libraries
2023-07-11 09:55:31,853:INFO:Copying training dataset
2023-07-11 09:55:31,858:INFO:Defining folds
2023-07-11 09:55:31,858:INFO:Declaring metric variables
2023-07-11 09:55:31,858:INFO:Importing untrained model
2023-07-11 09:55:31,858:INFO:Bayesian Ridge Imported successfully
2023-07-11 09:55:31,858:INFO:Starting cross validation
2023-07-11 09:55:31,863:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:55:36,219:ERROR:create_model() for br raised an exception or returned all 0.0:
2023-07-11 09:55:36,219:ERROR:Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 792, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
1 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_bayes.py", line 231, in fit
    X, y = self._validate_data(X, y, dtype=[np.float64, np.float32], y_numeric=True)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1122, in check_X_y
    y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1147, in _check_y
    y = y.astype(np.float64)
ValueError: could not convert string to float: 'Divorced'

--------------------------------------------------------------------------------
9 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_bayes.py", line 231, in fit
    X, y = self._validate_data(X, y, dtype=[np.float64, np.float32], y_numeric=True)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1122, in check_X_y
    y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1147, in _check_y
    y = y.astype(np.float64)
ValueError: could not convert string to float: 'Married'


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 808, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
1 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_bayes.py", line 231, in fit
    X, y = self._validate_data(X, y, dtype=[np.float64, np.float32], y_numeric=True)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1122, in check_X_y
    y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1147, in _check_y
    y = y.astype(np.float64)
ValueError: could not convert string to float: 'Divorced'

--------------------------------------------------------------------------------
9 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_bayes.py", line 231, in fit
    X, y = self._validate_data(X, y, dtype=[np.float64, np.float32], y_numeric=True)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1122, in check_X_y
    y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1147, in _check_y
    y = y.astype(np.float64)
ValueError: could not convert string to float: 'Married'


2023-07-11 09:55:37,179:INFO:Initializing Passive Aggressive Regressor
2023-07-11 09:55:37,179:INFO:Total runtime is 1.344827198982239 minutes
2023-07-11 09:55:37,179:INFO:SubProcess create_model() called ==================================
2023-07-11 09:55:37,184:INFO:Initializing create_model()
2023-07-11 09:55:37,184:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D76B8340>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D92A1D30>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:55:37,184:INFO:Checking exceptions
2023-07-11 09:55:37,184:INFO:Importing libraries
2023-07-11 09:55:37,184:INFO:Copying training dataset
2023-07-11 09:55:37,184:INFO:Defining folds
2023-07-11 09:55:37,184:INFO:Declaring metric variables
2023-07-11 09:55:37,184:INFO:Importing untrained model
2023-07-11 09:55:37,184:INFO:Passive Aggressive Regressor Imported successfully
2023-07-11 09:55:37,184:INFO:Starting cross validation
2023-07-11 09:55:37,184:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:55:41,114:WARNING:create_model() for par raised an exception or returned all 0.0, trying without fit_kwargs:
2023-07-11 09:55:41,114:WARNING:Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 792, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
1 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_passive_aggressive.py", line 567, in fit
    return self._fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1530, in _fit
    self._partial_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1445, in _partial_fit
    y = y.astype(np.float64, copy=False)
ValueError: could not convert string to float: 'Divorced'

--------------------------------------------------------------------------------
9 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_passive_aggressive.py", line 567, in fit
    return self._fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1530, in _fit
    self._partial_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1445, in _partial_fit
    y = y.astype(np.float64, copy=False)
ValueError: could not convert string to float: 'Married'


2023-07-11 09:55:41,114:INFO:Initializing create_model()
2023-07-11 09:55:41,114:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D76B8340>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D92A1D30>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:55:41,114:INFO:Checking exceptions
2023-07-11 09:55:41,114:INFO:Importing libraries
2023-07-11 09:55:41,114:INFO:Copying training dataset
2023-07-11 09:55:41,131:INFO:Defining folds
2023-07-11 09:55:41,131:INFO:Declaring metric variables
2023-07-11 09:55:41,131:INFO:Importing untrained model
2023-07-11 09:55:41,131:INFO:Passive Aggressive Regressor Imported successfully
2023-07-11 09:55:41,131:INFO:Starting cross validation
2023-07-11 09:55:41,133:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:55:45,290:ERROR:create_model() for par raised an exception or returned all 0.0:
2023-07-11 09:55:45,290:ERROR:Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 792, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
1 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_passive_aggressive.py", line 567, in fit
    return self._fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1530, in _fit
    self._partial_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1445, in _partial_fit
    y = y.astype(np.float64, copy=False)
ValueError: could not convert string to float: 'Divorced'

--------------------------------------------------------------------------------
9 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_passive_aggressive.py", line 567, in fit
    return self._fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1530, in _fit
    self._partial_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1445, in _partial_fit
    y = y.astype(np.float64, copy=False)
ValueError: could not convert string to float: 'Married'


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 808, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
1 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_passive_aggressive.py", line 567, in fit
    return self._fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1530, in _fit
    self._partial_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1445, in _partial_fit
    y = y.astype(np.float64, copy=False)
ValueError: could not convert string to float: 'Divorced'

--------------------------------------------------------------------------------
9 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_passive_aggressive.py", line 567, in fit
    return self._fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1530, in _fit
    self._partial_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1445, in _partial_fit
    y = y.astype(np.float64, copy=False)
ValueError: could not convert string to float: 'Married'


2023-07-11 09:55:46,211:INFO:Initializing Huber Regressor
2023-07-11 09:55:46,211:INFO:Total runtime is 1.495358677705129 minutes
2023-07-11 09:55:46,211:INFO:SubProcess create_model() called ==================================
2023-07-11 09:55:46,211:INFO:Initializing create_model()
2023-07-11 09:55:46,211:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D76B8340>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D92A1D30>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:55:46,211:INFO:Checking exceptions
2023-07-11 09:55:46,211:INFO:Importing libraries
2023-07-11 09:55:46,211:INFO:Copying training dataset
2023-07-11 09:55:46,211:INFO:Defining folds
2023-07-11 09:55:46,211:INFO:Declaring metric variables
2023-07-11 09:55:46,211:INFO:Importing untrained model
2023-07-11 09:55:46,211:INFO:Huber Regressor Imported successfully
2023-07-11 09:55:46,211:INFO:Starting cross validation
2023-07-11 09:55:46,211:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:55:50,243:WARNING:create_model() for huber raised an exception or returned all 0.0, trying without fit_kwargs:
2023-07-11 09:55:50,243:WARNING:Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 792, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
1 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py", line 297, in fit
    X, y = self._validate_data(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1122, in check_X_y
    y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1147, in _check_y
    y = y.astype(np.float64)
ValueError: could not convert string to float: 'Divorced'

--------------------------------------------------------------------------------
9 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py", line 297, in fit
    X, y = self._validate_data(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1122, in check_X_y
    y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1147, in _check_y
    y = y.astype(np.float64)
ValueError: could not convert string to float: 'Married'


2023-07-11 09:55:50,243:INFO:Initializing create_model()
2023-07-11 09:55:50,243:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D76B8340>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D92A1D30>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:55:50,243:INFO:Checking exceptions
2023-07-11 09:55:50,243:INFO:Importing libraries
2023-07-11 09:55:50,243:INFO:Copying training dataset
2023-07-11 09:55:50,258:INFO:Defining folds
2023-07-11 09:55:50,258:INFO:Declaring metric variables
2023-07-11 09:55:50,262:INFO:Importing untrained model
2023-07-11 09:55:50,262:INFO:Huber Regressor Imported successfully
2023-07-11 09:55:50,262:INFO:Starting cross validation
2023-07-11 09:55:50,262:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:55:54,458:ERROR:create_model() for huber raised an exception or returned all 0.0:
2023-07-11 09:55:54,458:ERROR:Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 792, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
1 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py", line 297, in fit
    X, y = self._validate_data(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1122, in check_X_y
    y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1147, in _check_y
    y = y.astype(np.float64)
ValueError: could not convert string to float: 'Divorced'

--------------------------------------------------------------------------------
9 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py", line 297, in fit
    X, y = self._validate_data(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1122, in check_X_y
    y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1147, in _check_y
    y = y.astype(np.float64)
ValueError: could not convert string to float: 'Married'


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 808, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
1 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py", line 297, in fit
    X, y = self._validate_data(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1122, in check_X_y
    y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1147, in _check_y
    y = y.astype(np.float64)
ValueError: could not convert string to float: 'Divorced'

--------------------------------------------------------------------------------
9 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py", line 297, in fit
    X, y = self._validate_data(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1122, in check_X_y
    y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1147, in _check_y
    y = y.astype(np.float64)
ValueError: could not convert string to float: 'Married'


2023-07-11 09:55:55,443:INFO:Initializing K Neighbors Regressor
2023-07-11 09:55:55,443:INFO:Total runtime is 1.6492184201876323 minutes
2023-07-11 09:55:55,459:INFO:SubProcess create_model() called ==================================
2023-07-11 09:55:55,459:INFO:Initializing create_model()
2023-07-11 09:55:55,459:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D76B8340>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D92A1D30>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:55:55,459:INFO:Checking exceptions
2023-07-11 09:55:55,459:INFO:Importing libraries
2023-07-11 09:55:55,459:INFO:Copying training dataset
2023-07-11 09:55:55,459:INFO:Defining folds
2023-07-11 09:55:55,459:INFO:Declaring metric variables
2023-07-11 09:55:55,459:INFO:Importing untrained model
2023-07-11 09:55:55,459:INFO:K Neighbors Regressor Imported successfully
2023-07-11 09:55:55,459:INFO:Starting cross validation
2023-07-11 09:55:55,459:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:55:56,180:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py:192: FitFailedWarning: Metric 'make_scorer(root_mean_squared_log_error, greater_is_better=False)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 184, in _score
    return super()._score(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 276, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\neighbors\_regression.py", line 248, in predict
    y_pred = np.mean(_y[neigh_ind], axis=1)
  File "<__array_function__ internals>", line 5, in mean
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\fromnumeric.py", line 3440, in mean
    return _methods._mean(a, axis=axis, dtype=dtype,
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\_methods.py", line 181, in _mean
    ret = um.true_divide(
TypeError: unsupported operand type(s) for /: 'str' and 'int'

  warnings.warn(

2023-07-11 09:55:56,195:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py:192: FitFailedWarning: Metric 'make_scorer(root_mean_squared_log_error, greater_is_better=False)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 184, in _score
    return super()._score(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 276, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\neighbors\_regression.py", line 248, in predict
    y_pred = np.mean(_y[neigh_ind], axis=1)
  File "<__array_function__ internals>", line 5, in mean
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\fromnumeric.py", line 3440, in mean
    return _methods._mean(a, axis=axis, dtype=dtype,
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\_methods.py", line 181, in _mean
    ret = um.true_divide(
TypeError: unsupported operand type(s) for /: 'str' and 'int'

  warnings.warn(

2023-07-11 09:55:56,211:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py:192: FitFailedWarning: Metric 'make_scorer(root_mean_squared_log_error, greater_is_better=False)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 184, in _score
    return super()._score(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 276, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\neighbors\_regression.py", line 248, in predict
    y_pred = np.mean(_y[neigh_ind], axis=1)
  File "<__array_function__ internals>", line 5, in mean
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\fromnumeric.py", line 3440, in mean
    return _methods._mean(a, axis=axis, dtype=dtype,
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\_methods.py", line 181, in _mean
    ret = um.true_divide(
TypeError: unsupported operand type(s) for /: 'str' and 'int'

  warnings.warn(

2023-07-11 09:55:56,242:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py:192: FitFailedWarning: Metric 'make_scorer(root_mean_squared_log_error, greater_is_better=False)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 184, in _score
    return super()._score(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 276, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\neighbors\_regression.py", line 248, in predict
    y_pred = np.mean(_y[neigh_ind], axis=1)
  File "<__array_function__ internals>", line 5, in mean
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\fromnumeric.py", line 3440, in mean
    return _methods._mean(a, axis=axis, dtype=dtype,
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\_methods.py", line 181, in _mean
    ret = um.true_divide(
TypeError: unsupported operand type(s) for /: 'str' and 'int'

  warnings.warn(

2023-07-11 09:55:56,242:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py:192: FitFailedWarning: Metric 'make_scorer(root_mean_squared_log_error, greater_is_better=False)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 184, in _score
    return super()._score(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 276, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\neighbors\_regression.py", line 248, in predict
    y_pred = np.mean(_y[neigh_ind], axis=1)
  File "<__array_function__ internals>", line 5, in mean
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\fromnumeric.py", line 3440, in mean
    return _methods._mean(a, axis=axis, dtype=dtype,
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\_methods.py", line 181, in _mean
    ret = um.true_divide(
TypeError: unsupported operand type(s) for /: 'str' and 'int'

  warnings.warn(

2023-07-11 09:55:56,258:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py:192: FitFailedWarning: Metric 'make_scorer(root_mean_squared_log_error, greater_is_better=False)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 184, in _score
    return super()._score(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 276, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\neighbors\_regression.py", line 248, in predict
    y_pred = np.mean(_y[neigh_ind], axis=1)
  File "<__array_function__ internals>", line 5, in mean
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\fromnumeric.py", line 3440, in mean
    return _methods._mean(a, axis=axis, dtype=dtype,
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\_methods.py", line 181, in _mean
    ret = um.true_divide(
TypeError: unsupported operand type(s) for /: 'str' and 'int'

  warnings.warn(

2023-07-11 09:55:56,274:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py:192: FitFailedWarning: Metric 'make_scorer(root_mean_squared_log_error, greater_is_better=False)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 184, in _score
    return super()._score(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 276, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\neighbors\_regression.py", line 248, in predict
    y_pred = np.mean(_y[neigh_ind], axis=1)
  File "<__array_function__ internals>", line 5, in mean
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\fromnumeric.py", line 3440, in mean
    return _methods._mean(a, axis=axis, dtype=dtype,
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\_methods.py", line 181, in _mean
    ret = um.true_divide(
TypeError: unsupported operand type(s) for /: 'str' and 'int'

  warnings.warn(

2023-07-11 09:55:56,274:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py:192: FitFailedWarning: Metric 'make_scorer(root_mean_squared_log_error, greater_is_better=False)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 184, in _score
    return super()._score(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 276, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\neighbors\_regression.py", line 248, in predict
    y_pred = np.mean(_y[neigh_ind], axis=1)
  File "<__array_function__ internals>", line 5, in mean
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\fromnumeric.py", line 3440, in mean
    return _methods._mean(a, axis=axis, dtype=dtype,
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\_methods.py", line 181, in _mean
    ret = um.true_divide(
TypeError: unsupported operand type(s) for /: 'str' and 'int'

  warnings.warn(

2023-07-11 09:55:56,289:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py:192: FitFailedWarning: Metric 'make_scorer(mean_absolute_percentage_error, greater_is_better=False)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 184, in _score
    return super()._score(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 276, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\neighbors\_regression.py", line 248, in predict
    y_pred = np.mean(_y[neigh_ind], axis=1)
  File "<__array_function__ internals>", line 5, in mean
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\fromnumeric.py", line 3440, in mean
    return _methods._mean(a, axis=axis, dtype=dtype,
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\_methods.py", line 181, in _mean
    ret = um.true_divide(
TypeError: unsupported operand type(s) for /: 'str' and 'int'

  warnings.warn(

2023-07-11 09:55:56,289:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 276, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\neighbors\_regression.py", line 248, in predict
    y_pred = np.mean(_y[neigh_ind], axis=1)
  File "<__array_function__ internals>", line 5, in mean
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\fromnumeric.py", line 3440, in mean
    return _methods._mean(a, axis=axis, dtype=dtype,
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\_methods.py", line 181, in _mean
    ret = um.true_divide(
TypeError: unsupported operand type(s) for /: 'str' and 'int'

  warnings.warn(

2023-07-11 09:55:56,289:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py:192: FitFailedWarning: Metric 'make_scorer(root_mean_squared_log_error, greater_is_better=False)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 184, in _score
    return super()._score(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 276, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\neighbors\_regression.py", line 248, in predict
    y_pred = np.mean(_y[neigh_ind], axis=1)
  File "<__array_function__ internals>", line 5, in mean
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\fromnumeric.py", line 3440, in mean
    return _methods._mean(a, axis=axis, dtype=dtype,
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\_methods.py", line 181, in _mean
    ret = um.true_divide(
TypeError: unsupported operand type(s) for /: 'str' and 'int'

  warnings.warn(

2023-07-11 09:55:56,315:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py:192: FitFailedWarning: Metric 'make_scorer(mean_absolute_percentage_error, greater_is_better=False)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 184, in _score
    return super()._score(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 276, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\neighbors\_regression.py", line 248, in predict
    y_pred = np.mean(_y[neigh_ind], axis=1)
  File "<__array_function__ internals>", line 5, in mean
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\fromnumeric.py", line 3440, in mean
    return _methods._mean(a, axis=axis, dtype=dtype,
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\_methods.py", line 181, in _mean
    ret = um.true_divide(
TypeError: unsupported operand type(s) for /: 'str' and 'int'

  warnings.warn(

2023-07-11 09:55:56,315:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 276, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\neighbors\_regression.py", line 248, in predict
    y_pred = np.mean(_y[neigh_ind], axis=1)
  File "<__array_function__ internals>", line 5, in mean
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\fromnumeric.py", line 3440, in mean
    return _methods._mean(a, axis=axis, dtype=dtype,
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\_methods.py", line 181, in _mean
    ret = um.true_divide(
TypeError: unsupported operand type(s) for /: 'str' and 'int'

  warnings.warn(

2023-07-11 09:55:56,315:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py:192: FitFailedWarning: Metric 'make_scorer(root_mean_squared_log_error, greater_is_better=False)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 184, in _score
    return super()._score(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 276, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\neighbors\_regression.py", line 248, in predict
    y_pred = np.mean(_y[neigh_ind], axis=1)
  File "<__array_function__ internals>", line 5, in mean
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\fromnumeric.py", line 3440, in mean
    return _methods._mean(a, axis=axis, dtype=dtype,
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\_methods.py", line 181, in _mean
    ret = um.true_divide(
TypeError: unsupported operand type(s) for /: 'str' and 'int'

  warnings.warn(

2023-07-11 09:55:56,315:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py:192: FitFailedWarning: Metric 'make_scorer(mean_absolute_percentage_error, greater_is_better=False)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 184, in _score
    return super()._score(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 276, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\neighbors\_regression.py", line 248, in predict
    y_pred = np.mean(_y[neigh_ind], axis=1)
  File "<__array_function__ internals>", line 5, in mean
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\fromnumeric.py", line 3440, in mean
    return _methods._mean(a, axis=axis, dtype=dtype,
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\_methods.py", line 181, in _mean
    ret = um.true_divide(
TypeError: unsupported operand type(s) for /: 'str' and 'int'

  warnings.warn(

2023-07-11 09:55:56,315:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 276, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\neighbors\_regression.py", line 248, in predict
    y_pred = np.mean(_y[neigh_ind], axis=1)
  File "<__array_function__ internals>", line 5, in mean
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\fromnumeric.py", line 3440, in mean
    return _methods._mean(a, axis=axis, dtype=dtype,
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\_methods.py", line 181, in _mean
    ret = um.true_divide(
TypeError: unsupported operand type(s) for /: 'str' and 'int'

  warnings.warn(

2023-07-11 09:55:56,364:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py:192: FitFailedWarning: Metric 'make_scorer(mean_absolute_percentage_error, greater_is_better=False)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 184, in _score
    return super()._score(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 276, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\neighbors\_regression.py", line 248, in predict
    y_pred = np.mean(_y[neigh_ind], axis=1)
  File "<__array_function__ internals>", line 5, in mean
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\fromnumeric.py", line 3440, in mean
    return _methods._mean(a, axis=axis, dtype=dtype,
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\_methods.py", line 181, in _mean
    ret = um.true_divide(
TypeError: unsupported operand type(s) for /: 'str' and 'int'

  warnings.warn(

2023-07-11 09:55:56,364:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 276, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\neighbors\_regression.py", line 248, in predict
    y_pred = np.mean(_y[neigh_ind], axis=1)
  File "<__array_function__ internals>", line 5, in mean
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\fromnumeric.py", line 3440, in mean
    return _methods._mean(a, axis=axis, dtype=dtype,
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\_methods.py", line 181, in _mean
    ret = um.true_divide(
TypeError: unsupported operand type(s) for /: 'str' and 'int'

  warnings.warn(

2023-07-11 09:55:56,364:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py:192: FitFailedWarning: Metric 'make_scorer(mean_absolute_percentage_error, greater_is_better=False)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 184, in _score
    return super()._score(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 276, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\neighbors\_regression.py", line 248, in predict
    y_pred = np.mean(_y[neigh_ind], axis=1)
  File "<__array_function__ internals>", line 5, in mean
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\fromnumeric.py", line 3440, in mean
    return _methods._mean(a, axis=axis, dtype=dtype,
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\_methods.py", line 181, in _mean
    ret = um.true_divide(
TypeError: unsupported operand type(s) for /: 'str' and 'int'

  warnings.warn(

2023-07-11 09:55:56,364:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 276, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\neighbors\_regression.py", line 248, in predict
    y_pred = np.mean(_y[neigh_ind], axis=1)
  File "<__array_function__ internals>", line 5, in mean
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\fromnumeric.py", line 3440, in mean
    return _methods._mean(a, axis=axis, dtype=dtype,
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\_methods.py", line 181, in _mean
    ret = um.true_divide(
TypeError: unsupported operand type(s) for /: 'str' and 'int'

  warnings.warn(

2023-07-11 09:55:56,373:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py:192: FitFailedWarning: Metric 'make_scorer(mean_absolute_percentage_error, greater_is_better=False)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 184, in _score
    return super()._score(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 276, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\neighbors\_regression.py", line 248, in predict
    y_pred = np.mean(_y[neigh_ind], axis=1)
  File "<__array_function__ internals>", line 5, in mean
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\fromnumeric.py", line 3440, in mean
    return _methods._mean(a, axis=axis, dtype=dtype,
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\_methods.py", line 181, in _mean
    ret = um.true_divide(
TypeError: unsupported operand type(s) for /: 'str' and 'int'

  warnings.warn(

2023-07-11 09:55:56,373:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 276, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\neighbors\_regression.py", line 248, in predict
    y_pred = np.mean(_y[neigh_ind], axis=1)
  File "<__array_function__ internals>", line 5, in mean
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\fromnumeric.py", line 3440, in mean
    return _methods._mean(a, axis=axis, dtype=dtype,
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\_methods.py", line 181, in _mean
    ret = um.true_divide(
TypeError: unsupported operand type(s) for /: 'str' and 'int'

  warnings.warn(

2023-07-11 09:55:56,380:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py:192: FitFailedWarning: Metric 'make_scorer(mean_absolute_percentage_error, greater_is_better=False)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 184, in _score
    return super()._score(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 276, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\neighbors\_regression.py", line 248, in predict
    y_pred = np.mean(_y[neigh_ind], axis=1)
  File "<__array_function__ internals>", line 5, in mean
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\fromnumeric.py", line 3440, in mean
    return _methods._mean(a, axis=axis, dtype=dtype,
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\_methods.py", line 181, in _mean
    ret = um.true_divide(
TypeError: unsupported operand type(s) for /: 'str' and 'int'

  warnings.warn(

2023-07-11 09:55:56,380:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 276, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\neighbors\_regression.py", line 248, in predict
    y_pred = np.mean(_y[neigh_ind], axis=1)
  File "<__array_function__ internals>", line 5, in mean
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\fromnumeric.py", line 3440, in mean
    return _methods._mean(a, axis=axis, dtype=dtype,
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\_methods.py", line 181, in _mean
    ret = um.true_divide(
TypeError: unsupported operand type(s) for /: 'str' and 'int'

  warnings.warn(

2023-07-11 09:55:56,380:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py:192: FitFailedWarning: Metric 'make_scorer(mean_absolute_percentage_error, greater_is_better=False)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 184, in _score
    return super()._score(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 276, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\neighbors\_regression.py", line 248, in predict
    y_pred = np.mean(_y[neigh_ind], axis=1)
  File "<__array_function__ internals>", line 5, in mean
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\fromnumeric.py", line 3440, in mean
    return _methods._mean(a, axis=axis, dtype=dtype,
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\_methods.py", line 181, in _mean
    ret = um.true_divide(
TypeError: unsupported operand type(s) for /: 'str' and 'int'

  warnings.warn(

2023-07-11 09:55:56,380:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 276, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\neighbors\_regression.py", line 248, in predict
    y_pred = np.mean(_y[neigh_ind], axis=1)
  File "<__array_function__ internals>", line 5, in mean
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\fromnumeric.py", line 3440, in mean
    return _methods._mean(a, axis=axis, dtype=dtype,
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\_methods.py", line 181, in _mean
    ret = um.true_divide(
TypeError: unsupported operand type(s) for /: 'str' and 'int'

  warnings.warn(

2023-07-11 09:55:56,380:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py:192: FitFailedWarning: Metric 'make_scorer(mean_absolute_percentage_error, greater_is_better=False)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 184, in _score
    return super()._score(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 276, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\neighbors\_regression.py", line 248, in predict
    y_pred = np.mean(_y[neigh_ind], axis=1)
  File "<__array_function__ internals>", line 5, in mean
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\fromnumeric.py", line 3440, in mean
    return _methods._mean(a, axis=axis, dtype=dtype,
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\_methods.py", line 181, in _mean
    ret = um.true_divide(
TypeError: unsupported operand type(s) for /: 'str' and 'int'

  warnings.warn(

2023-07-11 09:55:56,380:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 276, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\neighbors\_regression.py", line 248, in predict
    y_pred = np.mean(_y[neigh_ind], axis=1)
  File "<__array_function__ internals>", line 5, in mean
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\fromnumeric.py", line 3440, in mean
    return _methods._mean(a, axis=axis, dtype=dtype,
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\_methods.py", line 181, in _mean
    ret = um.true_divide(
TypeError: unsupported operand type(s) for /: 'str' and 'int'

  warnings.warn(

2023-07-11 09:55:56,411:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py:192: FitFailedWarning: Metric 'make_scorer(mean_absolute_percentage_error, greater_is_better=False)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 184, in _score
    return super()._score(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 276, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\neighbors\_regression.py", line 248, in predict
    y_pred = np.mean(_y[neigh_ind], axis=1)
  File "<__array_function__ internals>", line 5, in mean
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\fromnumeric.py", line 3440, in mean
    return _methods._mean(a, axis=axis, dtype=dtype,
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\_methods.py", line 181, in _mean
    ret = um.true_divide(
TypeError: unsupported operand type(s) for /: 'str' and 'int'

  warnings.warn(

2023-07-11 09:55:56,411:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 276, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\neighbors\_regression.py", line 248, in predict
    y_pred = np.mean(_y[neigh_ind], axis=1)
  File "<__array_function__ internals>", line 5, in mean
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\fromnumeric.py", line 3440, in mean
    return _methods._mean(a, axis=axis, dtype=dtype,
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\_methods.py", line 181, in _mean
    ret = um.true_divide(
TypeError: unsupported operand type(s) for /: 'str' and 'int'

  warnings.warn(

2023-07-11 09:55:59,515:INFO:Calculating mean and std
2023-07-11 09:55:59,516:INFO:Creating metrics dataframe
2023-07-11 09:56:00,195:INFO:Uploading results into container
2023-07-11 09:56:00,195:INFO:Uploading model into container now
2023-07-11 09:56:00,195:INFO:_master_model_container: 1
2023-07-11 09:56:00,195:INFO:_display_container: 2
2023-07-11 09:56:00,195:INFO:KNeighborsRegressor(n_jobs=-1)
2023-07-11 09:56:00,195:INFO:create_model() successfully completed......................................
2023-07-11 09:56:00,315:WARNING:create_model() for KNeighborsRegressor(n_jobs=-1) raised an exception or returned all 0.0, trying without fit_kwargs:
2023-07-11 09:56:00,315:WARNING:Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 794, in compare_models
    assert (
AssertionError

2023-07-11 09:56:00,315:INFO:Initializing create_model()
2023-07-11 09:56:00,315:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D76B8340>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D92A1D30>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:56:00,315:INFO:Checking exceptions
2023-07-11 09:56:00,315:INFO:Importing libraries
2023-07-11 09:56:00,315:INFO:Copying training dataset
2023-07-11 09:56:00,315:INFO:Defining folds
2023-07-11 09:56:00,315:INFO:Declaring metric variables
2023-07-11 09:56:00,315:INFO:Importing untrained model
2023-07-11 09:56:00,315:INFO:K Neighbors Regressor Imported successfully
2023-07-11 09:56:00,315:INFO:Starting cross validation
2023-07-11 09:56:00,315:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:56:00,964:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py:192: FitFailedWarning: Metric 'make_scorer(root_mean_squared_log_error, greater_is_better=False)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 184, in _score
    return super()._score(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 276, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\neighbors\_regression.py", line 248, in predict
    y_pred = np.mean(_y[neigh_ind], axis=1)
  File "<__array_function__ internals>", line 5, in mean
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\fromnumeric.py", line 3440, in mean
    return _methods._mean(a, axis=axis, dtype=dtype,
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\_methods.py", line 181, in _mean
    ret = um.true_divide(
TypeError: unsupported operand type(s) for /: 'str' and 'int'

  warnings.warn(

2023-07-11 09:56:01,027:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py:192: FitFailedWarning: Metric 'make_scorer(root_mean_squared_log_error, greater_is_better=False)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 184, in _score
    return super()._score(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 276, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\neighbors\_regression.py", line 248, in predict
    y_pred = np.mean(_y[neigh_ind], axis=1)
  File "<__array_function__ internals>", line 5, in mean
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\fromnumeric.py", line 3440, in mean
    return _methods._mean(a, axis=axis, dtype=dtype,
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\_methods.py", line 181, in _mean
    ret = um.true_divide(
TypeError: unsupported operand type(s) for /: 'str' and 'int'

  warnings.warn(

2023-07-11 09:56:01,043:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py:192: FitFailedWarning: Metric 'make_scorer(root_mean_squared_log_error, greater_is_better=False)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 184, in _score
    return super()._score(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 276, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\neighbors\_regression.py", line 248, in predict
    y_pred = np.mean(_y[neigh_ind], axis=1)
  File "<__array_function__ internals>", line 5, in mean
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\fromnumeric.py", line 3440, in mean
    return _methods._mean(a, axis=axis, dtype=dtype,
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\_methods.py", line 181, in _mean
    ret = um.true_divide(
TypeError: unsupported operand type(s) for /: 'str' and 'int'

  warnings.warn(

2023-07-11 09:56:01,043:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py:192: FitFailedWarning: Metric 'make_scorer(root_mean_squared_log_error, greater_is_better=False)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 184, in _score
    return super()._score(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 276, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\neighbors\_regression.py", line 248, in predict
    y_pred = np.mean(_y[neigh_ind], axis=1)
  File "<__array_function__ internals>", line 5, in mean
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\fromnumeric.py", line 3440, in mean
    return _methods._mean(a, axis=axis, dtype=dtype,
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\_methods.py", line 181, in _mean
    ret = um.true_divide(
TypeError: unsupported operand type(s) for /: 'str' and 'int'

  warnings.warn(

2023-07-11 09:56:01,043:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py:192: FitFailedWarning: Metric 'make_scorer(root_mean_squared_log_error, greater_is_better=False)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 184, in _score
    return super()._score(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 276, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\neighbors\_regression.py", line 248, in predict
    y_pred = np.mean(_y[neigh_ind], axis=1)
  File "<__array_function__ internals>", line 5, in mean
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\fromnumeric.py", line 3440, in mean
    return _methods._mean(a, axis=axis, dtype=dtype,
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\_methods.py", line 181, in _mean
    ret = um.true_divide(
TypeError: unsupported operand type(s) for /: 'str' and 'int'

  warnings.warn(

2023-07-11 09:56:01,058:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py:192: FitFailedWarning: Metric 'make_scorer(mean_absolute_percentage_error, greater_is_better=False)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 184, in _score
    return super()._score(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 276, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\neighbors\_regression.py", line 248, in predict
    y_pred = np.mean(_y[neigh_ind], axis=1)
  File "<__array_function__ internals>", line 5, in mean
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\fromnumeric.py", line 3440, in mean
    return _methods._mean(a, axis=axis, dtype=dtype,
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\_methods.py", line 181, in _mean
    ret = um.true_divide(
TypeError: unsupported operand type(s) for /: 'str' and 'int'

  warnings.warn(

2023-07-11 09:56:01,058:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py:192: FitFailedWarning: Metric 'make_scorer(root_mean_squared_log_error, greater_is_better=False)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 184, in _score
    return super()._score(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 276, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\neighbors\_regression.py", line 248, in predict
    y_pred = np.mean(_y[neigh_ind], axis=1)
  File "<__array_function__ internals>", line 5, in mean
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\fromnumeric.py", line 3440, in mean
    return _methods._mean(a, axis=axis, dtype=dtype,
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\_methods.py", line 181, in _mean
    ret = um.true_divide(
TypeError: unsupported operand type(s) for /: 'str' and 'int'

  warnings.warn(

2023-07-11 09:56:01,058:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 276, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\neighbors\_regression.py", line 248, in predict
    y_pred = np.mean(_y[neigh_ind], axis=1)
  File "<__array_function__ internals>", line 5, in mean
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\fromnumeric.py", line 3440, in mean
    return _methods._mean(a, axis=axis, dtype=dtype,
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\_methods.py", line 181, in _mean
    ret = um.true_divide(
TypeError: unsupported operand type(s) for /: 'str' and 'int'

  warnings.warn(

2023-07-11 09:56:01,081:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py:192: FitFailedWarning: Metric 'make_scorer(root_mean_squared_log_error, greater_is_better=False)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 184, in _score
    return super()._score(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 276, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\neighbors\_regression.py", line 248, in predict
    y_pred = np.mean(_y[neigh_ind], axis=1)
  File "<__array_function__ internals>", line 5, in mean
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\fromnumeric.py", line 3440, in mean
    return _methods._mean(a, axis=axis, dtype=dtype,
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\_methods.py", line 181, in _mean
    ret = um.true_divide(
TypeError: unsupported operand type(s) for /: 'str' and 'int'

  warnings.warn(

2023-07-11 09:56:01,090:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py:192: FitFailedWarning: Metric 'make_scorer(root_mean_squared_log_error, greater_is_better=False)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 184, in _score
    return super()._score(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 276, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\neighbors\_regression.py", line 248, in predict
    y_pred = np.mean(_y[neigh_ind], axis=1)
  File "<__array_function__ internals>", line 5, in mean
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\fromnumeric.py", line 3440, in mean
    return _methods._mean(a, axis=axis, dtype=dtype,
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\_methods.py", line 181, in _mean
    ret = um.true_divide(
TypeError: unsupported operand type(s) for /: 'str' and 'int'

  warnings.warn(

2023-07-11 09:56:01,090:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py:192: FitFailedWarning: Metric 'make_scorer(root_mean_squared_log_error, greater_is_better=False)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 184, in _score
    return super()._score(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 276, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\neighbors\_regression.py", line 248, in predict
    y_pred = np.mean(_y[neigh_ind], axis=1)
  File "<__array_function__ internals>", line 5, in mean
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\fromnumeric.py", line 3440, in mean
    return _methods._mean(a, axis=axis, dtype=dtype,
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\_methods.py", line 181, in _mean
    ret = um.true_divide(
TypeError: unsupported operand type(s) for /: 'str' and 'int'

  warnings.warn(

2023-07-11 09:56:01,107:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py:192: FitFailedWarning: Metric 'make_scorer(root_mean_squared_log_error, greater_is_better=False)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 184, in _score
    return super()._score(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 276, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\neighbors\_regression.py", line 248, in predict
    y_pred = np.mean(_y[neigh_ind], axis=1)
  File "<__array_function__ internals>", line 5, in mean
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\fromnumeric.py", line 3440, in mean
    return _methods._mean(a, axis=axis, dtype=dtype,
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\_methods.py", line 181, in _mean
    ret = um.true_divide(
TypeError: unsupported operand type(s) for /: 'str' and 'int'

  warnings.warn(

2023-07-11 09:56:01,115:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py:192: FitFailedWarning: Metric 'make_scorer(mean_absolute_percentage_error, greater_is_better=False)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 184, in _score
    return super()._score(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 276, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\neighbors\_regression.py", line 248, in predict
    y_pred = np.mean(_y[neigh_ind], axis=1)
  File "<__array_function__ internals>", line 5, in mean
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\fromnumeric.py", line 3440, in mean
    return _methods._mean(a, axis=axis, dtype=dtype,
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\_methods.py", line 181, in _mean
    ret = um.true_divide(
TypeError: unsupported operand type(s) for /: 'str' and 'int'

  warnings.warn(

2023-07-11 09:56:01,115:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 276, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\neighbors\_regression.py", line 248, in predict
    y_pred = np.mean(_y[neigh_ind], axis=1)
  File "<__array_function__ internals>", line 5, in mean
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\fromnumeric.py", line 3440, in mean
    return _methods._mean(a, axis=axis, dtype=dtype,
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\_methods.py", line 181, in _mean
    ret = um.true_divide(
TypeError: unsupported operand type(s) for /: 'str' and 'int'

  warnings.warn(

2023-07-11 09:56:01,136:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py:192: FitFailedWarning: Metric 'make_scorer(mean_absolute_percentage_error, greater_is_better=False)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 184, in _score
    return super()._score(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 276, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\neighbors\_regression.py", line 248, in predict
    y_pred = np.mean(_y[neigh_ind], axis=1)
  File "<__array_function__ internals>", line 5, in mean
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\fromnumeric.py", line 3440, in mean
    return _methods._mean(a, axis=axis, dtype=dtype,
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\_methods.py", line 181, in _mean
    ret = um.true_divide(
TypeError: unsupported operand type(s) for /: 'str' and 'int'

  warnings.warn(

2023-07-11 09:56:01,136:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 276, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\neighbors\_regression.py", line 248, in predict
    y_pred = np.mean(_y[neigh_ind], axis=1)
  File "<__array_function__ internals>", line 5, in mean
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\fromnumeric.py", line 3440, in mean
    return _methods._mean(a, axis=axis, dtype=dtype,
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\_methods.py", line 181, in _mean
    ret = um.true_divide(
TypeError: unsupported operand type(s) for /: 'str' and 'int'

  warnings.warn(

2023-07-11 09:56:01,159:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py:192: FitFailedWarning: Metric 'make_scorer(mean_absolute_percentage_error, greater_is_better=False)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 184, in _score
    return super()._score(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 276, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\neighbors\_regression.py", line 248, in predict
    y_pred = np.mean(_y[neigh_ind], axis=1)
  File "<__array_function__ internals>", line 5, in mean
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\fromnumeric.py", line 3440, in mean
    return _methods._mean(a, axis=axis, dtype=dtype,
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\_methods.py", line 181, in _mean
    ret = um.true_divide(
TypeError: unsupported operand type(s) for /: 'str' and 'int'

  warnings.warn(

2023-07-11 09:56:01,160:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 276, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\neighbors\_regression.py", line 248, in predict
    y_pred = np.mean(_y[neigh_ind], axis=1)
  File "<__array_function__ internals>", line 5, in mean
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\fromnumeric.py", line 3440, in mean
    return _methods._mean(a, axis=axis, dtype=dtype,
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\_methods.py", line 181, in _mean
    ret = um.true_divide(
TypeError: unsupported operand type(s) for /: 'str' and 'int'

  warnings.warn(

2023-07-11 09:56:01,164:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py:192: FitFailedWarning: Metric 'make_scorer(mean_absolute_percentage_error, greater_is_better=False)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 184, in _score
    return super()._score(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 276, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\neighbors\_regression.py", line 248, in predict
    y_pred = np.mean(_y[neigh_ind], axis=1)
  File "<__array_function__ internals>", line 5, in mean
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\fromnumeric.py", line 3440, in mean
    return _methods._mean(a, axis=axis, dtype=dtype,
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\_methods.py", line 181, in _mean
    ret = um.true_divide(
TypeError: unsupported operand type(s) for /: 'str' and 'int'

  warnings.warn(

2023-07-11 09:56:01,164:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 276, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\neighbors\_regression.py", line 248, in predict
    y_pred = np.mean(_y[neigh_ind], axis=1)
  File "<__array_function__ internals>", line 5, in mean
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\fromnumeric.py", line 3440, in mean
    return _methods._mean(a, axis=axis, dtype=dtype,
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\_methods.py", line 181, in _mean
    ret = um.true_divide(
TypeError: unsupported operand type(s) for /: 'str' and 'int'

  warnings.warn(

2023-07-11 09:56:01,165:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py:192: FitFailedWarning: Metric 'make_scorer(mean_absolute_percentage_error, greater_is_better=False)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 184, in _score
    return super()._score(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 276, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\neighbors\_regression.py", line 248, in predict
    y_pred = np.mean(_y[neigh_ind], axis=1)
  File "<__array_function__ internals>", line 5, in mean
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\fromnumeric.py", line 3440, in mean
    return _methods._mean(a, axis=axis, dtype=dtype,
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\_methods.py", line 181, in _mean
    ret = um.true_divide(
TypeError: unsupported operand type(s) for /: 'str' and 'int'

  warnings.warn(

2023-07-11 09:56:01,165:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 276, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\neighbors\_regression.py", line 248, in predict
    y_pred = np.mean(_y[neigh_ind], axis=1)
  File "<__array_function__ internals>", line 5, in mean
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\fromnumeric.py", line 3440, in mean
    return _methods._mean(a, axis=axis, dtype=dtype,
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\_methods.py", line 181, in _mean
    ret = um.true_divide(
TypeError: unsupported operand type(s) for /: 'str' and 'int'

  warnings.warn(

2023-07-11 09:56:01,182:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py:192: FitFailedWarning: Metric 'make_scorer(mean_absolute_percentage_error, greater_is_better=False)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 184, in _score
    return super()._score(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 276, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\neighbors\_regression.py", line 248, in predict
    y_pred = np.mean(_y[neigh_ind], axis=1)
  File "<__array_function__ internals>", line 5, in mean
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\fromnumeric.py", line 3440, in mean
    return _methods._mean(a, axis=axis, dtype=dtype,
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\_methods.py", line 181, in _mean
    ret = um.true_divide(
TypeError: unsupported operand type(s) for /: 'str' and 'int'

  warnings.warn(

2023-07-11 09:56:01,182:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 276, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\neighbors\_regression.py", line 248, in predict
    y_pred = np.mean(_y[neigh_ind], axis=1)
  File "<__array_function__ internals>", line 5, in mean
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\fromnumeric.py", line 3440, in mean
    return _methods._mean(a, axis=axis, dtype=dtype,
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\_methods.py", line 181, in _mean
    ret = um.true_divide(
TypeError: unsupported operand type(s) for /: 'str' and 'int'

  warnings.warn(

2023-07-11 09:56:01,197:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py:192: FitFailedWarning: Metric 'make_scorer(mean_absolute_percentage_error, greater_is_better=False)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 184, in _score
    return super()._score(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 276, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\neighbors\_regression.py", line 248, in predict
    y_pred = np.mean(_y[neigh_ind], axis=1)
  File "<__array_function__ internals>", line 5, in mean
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\fromnumeric.py", line 3440, in mean
    return _methods._mean(a, axis=axis, dtype=dtype,
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\_methods.py", line 181, in _mean
    ret = um.true_divide(
TypeError: unsupported operand type(s) for /: 'str' and 'int'

  warnings.warn(

2023-07-11 09:56:01,197:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 276, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\neighbors\_regression.py", line 248, in predict
    y_pred = np.mean(_y[neigh_ind], axis=1)
  File "<__array_function__ internals>", line 5, in mean
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\fromnumeric.py", line 3440, in mean
    return _methods._mean(a, axis=axis, dtype=dtype,
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\_methods.py", line 181, in _mean
    ret = um.true_divide(
TypeError: unsupported operand type(s) for /: 'str' and 'int'

  warnings.warn(

2023-07-11 09:56:01,197:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py:192: FitFailedWarning: Metric 'make_scorer(mean_absolute_percentage_error, greater_is_better=False)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 184, in _score
    return super()._score(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 276, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\neighbors\_regression.py", line 248, in predict
    y_pred = np.mean(_y[neigh_ind], axis=1)
  File "<__array_function__ internals>", line 5, in mean
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\fromnumeric.py", line 3440, in mean
    return _methods._mean(a, axis=axis, dtype=dtype,
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\_methods.py", line 181, in _mean
    ret = um.true_divide(
TypeError: unsupported operand type(s) for /: 'str' and 'int'

  warnings.warn(

2023-07-11 09:56:01,197:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 276, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\neighbors\_regression.py", line 248, in predict
    y_pred = np.mean(_y[neigh_ind], axis=1)
  File "<__array_function__ internals>", line 5, in mean
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\fromnumeric.py", line 3440, in mean
    return _methods._mean(a, axis=axis, dtype=dtype,
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\_methods.py", line 181, in _mean
    ret = um.true_divide(
TypeError: unsupported operand type(s) for /: 'str' and 'int'

  warnings.warn(

2023-07-11 09:56:01,197:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py:192: FitFailedWarning: Metric 'make_scorer(mean_absolute_percentage_error, greater_is_better=False)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 184, in _score
    return super()._score(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 276, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\neighbors\_regression.py", line 248, in predict
    y_pred = np.mean(_y[neigh_ind], axis=1)
  File "<__array_function__ internals>", line 5, in mean
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\fromnumeric.py", line 3440, in mean
    return _methods._mean(a, axis=axis, dtype=dtype,
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\_methods.py", line 181, in _mean
    ret = um.true_divide(
TypeError: unsupported operand type(s) for /: 'str' and 'int'

  warnings.warn(

2023-07-11 09:56:01,197:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 276, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\neighbors\_regression.py", line 248, in predict
    y_pred = np.mean(_y[neigh_ind], axis=1)
  File "<__array_function__ internals>", line 5, in mean
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\fromnumeric.py", line 3440, in mean
    return _methods._mean(a, axis=axis, dtype=dtype,
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\_methods.py", line 181, in _mean
    ret = um.true_divide(
TypeError: unsupported operand type(s) for /: 'str' and 'int'

  warnings.warn(

2023-07-11 09:56:04,292:INFO:Calculating mean and std
2023-07-11 09:56:04,292:INFO:Creating metrics dataframe
2023-07-11 09:56:05,089:INFO:Uploading results into container
2023-07-11 09:56:05,089:INFO:Uploading model into container now
2023-07-11 09:56:05,089:INFO:_master_model_container: 2
2023-07-11 09:56:05,089:INFO:_display_container: 2
2023-07-11 09:56:05,091:INFO:KNeighborsRegressor(n_jobs=-1)
2023-07-11 09:56:05,091:INFO:create_model() successfully completed......................................
2023-07-11 09:56:05,227:ERROR:create_model() for KNeighborsRegressor(n_jobs=-1) raised an exception or returned all 0.0:
2023-07-11 09:56:05,227:ERROR:Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 794, in compare_models
    assert (
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 810, in compare_models
    assert (
AssertionError

2023-07-11 09:56:05,227:INFO:Initializing Decision Tree Regressor
2023-07-11 09:56:05,227:INFO:Total runtime is 1.8122912764549257 minutes
2023-07-11 09:56:05,227:INFO:SubProcess create_model() called ==================================
2023-07-11 09:56:05,227:INFO:Initializing create_model()
2023-07-11 09:56:05,227:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D76B8340>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D92A1D30>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:56:05,227:INFO:Checking exceptions
2023-07-11 09:56:05,227:INFO:Importing libraries
2023-07-11 09:56:05,227:INFO:Copying training dataset
2023-07-11 09:56:05,243:INFO:Defining folds
2023-07-11 09:56:05,243:INFO:Declaring metric variables
2023-07-11 09:56:05,243:INFO:Importing untrained model
2023-07-11 09:56:05,243:INFO:Decision Tree Regressor Imported successfully
2023-07-11 09:56:05,243:INFO:Starting cross validation
2023-07-11 09:56:05,243:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:56:09,027:WARNING:create_model() for dt raised an exception or returned all 0.0, trying without fit_kwargs:
2023-07-11 09:56:09,027:WARNING:Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 792, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
1 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\tree\_classes.py", line 1247, in fit
    super().fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\tree\_classes.py", line 248, in fit
    y = np.ascontiguousarray(y, dtype=DOUBLE)
ValueError: could not convert string to float: 'Divorced'

--------------------------------------------------------------------------------
9 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\tree\_classes.py", line 1247, in fit
    super().fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\tree\_classes.py", line 248, in fit
    y = np.ascontiguousarray(y, dtype=DOUBLE)
ValueError: could not convert string to float: 'Married'


2023-07-11 09:56:09,027:INFO:Initializing create_model()
2023-07-11 09:56:09,027:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D76B8340>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D92A1D30>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:56:09,027:INFO:Checking exceptions
2023-07-11 09:56:09,027:INFO:Importing libraries
2023-07-11 09:56:09,027:INFO:Copying training dataset
2023-07-11 09:56:09,043:INFO:Defining folds
2023-07-11 09:56:09,043:INFO:Declaring metric variables
2023-07-11 09:56:09,043:INFO:Importing untrained model
2023-07-11 09:56:09,043:INFO:Decision Tree Regressor Imported successfully
2023-07-11 09:56:09,043:INFO:Starting cross validation
2023-07-11 09:56:09,043:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:56:13,089:ERROR:create_model() for dt raised an exception or returned all 0.0:
2023-07-11 09:56:13,089:ERROR:Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 792, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
1 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\tree\_classes.py", line 1247, in fit
    super().fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\tree\_classes.py", line 248, in fit
    y = np.ascontiguousarray(y, dtype=DOUBLE)
ValueError: could not convert string to float: 'Divorced'

--------------------------------------------------------------------------------
9 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\tree\_classes.py", line 1247, in fit
    super().fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\tree\_classes.py", line 248, in fit
    y = np.ascontiguousarray(y, dtype=DOUBLE)
ValueError: could not convert string to float: 'Married'


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 808, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
1 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\tree\_classes.py", line 1247, in fit
    super().fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\tree\_classes.py", line 248, in fit
    y = np.ascontiguousarray(y, dtype=DOUBLE)
ValueError: could not convert string to float: 'Divorced'

--------------------------------------------------------------------------------
9 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\tree\_classes.py", line 1247, in fit
    super().fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\tree\_classes.py", line 248, in fit
    y = np.ascontiguousarray(y, dtype=DOUBLE)
ValueError: could not convert string to float: 'Married'


2023-07-11 09:56:14,091:INFO:Initializing Random Forest Regressor
2023-07-11 09:56:14,091:INFO:Total runtime is 1.9600249965985617 minutes
2023-07-11 09:56:14,091:INFO:SubProcess create_model() called ==================================
2023-07-11 09:56:14,091:INFO:Initializing create_model()
2023-07-11 09:56:14,091:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D76B8340>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D92A1D30>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:56:14,091:INFO:Checking exceptions
2023-07-11 09:56:14,091:INFO:Importing libraries
2023-07-11 09:56:14,091:INFO:Copying training dataset
2023-07-11 09:56:14,109:INFO:Defining folds
2023-07-11 09:56:14,109:INFO:Declaring metric variables
2023-07-11 09:56:14,109:INFO:Importing untrained model
2023-07-11 09:56:14,109:INFO:Random Forest Regressor Imported successfully
2023-07-11 09:56:14,109:INFO:Starting cross validation
2023-07-11 09:56:14,111:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:56:17,890:WARNING:create_model() for rf raised an exception or returned all 0.0, trying without fit_kwargs:
2023-07-11 09:56:17,890:WARNING:Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 792, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
1 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\ensemble\_forest.py", line 388, in fit
    y = np.ascontiguousarray(y, dtype=DOUBLE)
ValueError: could not convert string to float: 'Divorced'

--------------------------------------------------------------------------------
9 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\ensemble\_forest.py", line 388, in fit
    y = np.ascontiguousarray(y, dtype=DOUBLE)
ValueError: could not convert string to float: 'Married'


2023-07-11 09:56:17,890:INFO:Initializing create_model()
2023-07-11 09:56:17,890:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D76B8340>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D92A1D30>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:56:17,890:INFO:Checking exceptions
2023-07-11 09:56:17,890:INFO:Importing libraries
2023-07-11 09:56:17,890:INFO:Copying training dataset
2023-07-11 09:56:17,906:INFO:Defining folds
2023-07-11 09:56:17,906:INFO:Declaring metric variables
2023-07-11 09:56:17,906:INFO:Importing untrained model
2023-07-11 09:56:17,906:INFO:Random Forest Regressor Imported successfully
2023-07-11 09:56:17,906:INFO:Starting cross validation
2023-07-11 09:56:17,907:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:56:22,028:ERROR:create_model() for rf raised an exception or returned all 0.0:
2023-07-11 09:56:22,028:ERROR:Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 792, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
1 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\ensemble\_forest.py", line 388, in fit
    y = np.ascontiguousarray(y, dtype=DOUBLE)
ValueError: could not convert string to float: 'Divorced'

--------------------------------------------------------------------------------
9 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\ensemble\_forest.py", line 388, in fit
    y = np.ascontiguousarray(y, dtype=DOUBLE)
ValueError: could not convert string to float: 'Married'


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 808, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
1 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\ensemble\_forest.py", line 388, in fit
    y = np.ascontiguousarray(y, dtype=DOUBLE)
ValueError: could not convert string to float: 'Divorced'

--------------------------------------------------------------------------------
9 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\ensemble\_forest.py", line 388, in fit
    y = np.ascontiguousarray(y, dtype=DOUBLE)
ValueError: could not convert string to float: 'Married'


2023-07-11 09:56:23,060:INFO:Initializing Extra Trees Regressor
2023-07-11 09:56:23,060:INFO:Total runtime is 2.1094954053560895 minutes
2023-07-11 09:56:23,060:INFO:SubProcess create_model() called ==================================
2023-07-11 09:56:23,060:INFO:Initializing create_model()
2023-07-11 09:56:23,060:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D76B8340>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D92A1D30>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:56:23,060:INFO:Checking exceptions
2023-07-11 09:56:23,060:INFO:Importing libraries
2023-07-11 09:56:23,060:INFO:Copying training dataset
2023-07-11 09:56:23,060:INFO:Defining folds
2023-07-11 09:56:23,060:INFO:Declaring metric variables
2023-07-11 09:56:23,060:INFO:Importing untrained model
2023-07-11 09:56:23,060:INFO:Extra Trees Regressor Imported successfully
2023-07-11 09:56:23,060:INFO:Starting cross validation
2023-07-11 09:56:23,075:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:56:26,926:WARNING:create_model() for et raised an exception or returned all 0.0, trying without fit_kwargs:
2023-07-11 09:56:26,926:WARNING:Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 792, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
1 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\ensemble\_forest.py", line 388, in fit
    y = np.ascontiguousarray(y, dtype=DOUBLE)
ValueError: could not convert string to float: 'Divorced'

--------------------------------------------------------------------------------
9 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\ensemble\_forest.py", line 388, in fit
    y = np.ascontiguousarray(y, dtype=DOUBLE)
ValueError: could not convert string to float: 'Married'


2023-07-11 09:56:26,926:INFO:Initializing create_model()
2023-07-11 09:56:26,926:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D76B8340>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D92A1D30>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:56:26,926:INFO:Checking exceptions
2023-07-11 09:56:26,926:INFO:Importing libraries
2023-07-11 09:56:26,926:INFO:Copying training dataset
2023-07-11 09:56:26,936:INFO:Defining folds
2023-07-11 09:56:26,936:INFO:Declaring metric variables
2023-07-11 09:56:26,936:INFO:Importing untrained model
2023-07-11 09:56:26,936:INFO:Extra Trees Regressor Imported successfully
2023-07-11 09:56:26,936:INFO:Starting cross validation
2023-07-11 09:56:26,937:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:56:30,926:ERROR:create_model() for et raised an exception or returned all 0.0:
2023-07-11 09:56:30,926:ERROR:Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 792, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
1 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\ensemble\_forest.py", line 388, in fit
    y = np.ascontiguousarray(y, dtype=DOUBLE)
ValueError: could not convert string to float: 'Divorced'

--------------------------------------------------------------------------------
9 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\ensemble\_forest.py", line 388, in fit
    y = np.ascontiguousarray(y, dtype=DOUBLE)
ValueError: could not convert string to float: 'Married'


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 808, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
1 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\ensemble\_forest.py", line 388, in fit
    y = np.ascontiguousarray(y, dtype=DOUBLE)
ValueError: could not convert string to float: 'Divorced'

--------------------------------------------------------------------------------
9 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\ensemble\_forest.py", line 388, in fit
    y = np.ascontiguousarray(y, dtype=DOUBLE)
ValueError: could not convert string to float: 'Married'


2023-07-11 09:56:31,980:INFO:Initializing AdaBoost Regressor
2023-07-11 09:56:31,980:INFO:Total runtime is 2.258165192604065 minutes
2023-07-11 09:56:31,980:INFO:SubProcess create_model() called ==================================
2023-07-11 09:56:31,980:INFO:Initializing create_model()
2023-07-11 09:56:31,980:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D76B8340>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D92A1D30>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:56:31,980:INFO:Checking exceptions
2023-07-11 09:56:31,980:INFO:Importing libraries
2023-07-11 09:56:31,980:INFO:Copying training dataset
2023-07-11 09:56:31,980:INFO:Defining folds
2023-07-11 09:56:31,980:INFO:Declaring metric variables
2023-07-11 09:56:31,980:INFO:Importing untrained model
2023-07-11 09:56:31,980:INFO:AdaBoost Regressor Imported successfully
2023-07-11 09:56:31,980:INFO:Starting cross validation
2023-07-11 09:56:31,980:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:56:35,916:WARNING:create_model() for ada raised an exception or returned all 0.0, trying without fit_kwargs:
2023-07-11 09:56:35,916:WARNING:Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 792, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
1 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\ensemble\_weight_boosting.py", line 126, in fit
    X, y = self._validate_data(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1122, in check_X_y
    y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1147, in _check_y
    y = y.astype(np.float64)
ValueError: could not convert string to float: 'Divorced'

--------------------------------------------------------------------------------
9 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\ensemble\_weight_boosting.py", line 126, in fit
    X, y = self._validate_data(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1122, in check_X_y
    y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1147, in _check_y
    y = y.astype(np.float64)
ValueError: could not convert string to float: 'Married'


2023-07-11 09:56:35,916:INFO:Initializing create_model()
2023-07-11 09:56:35,916:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D76B8340>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D92A1D30>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:56:35,916:INFO:Checking exceptions
2023-07-11 09:56:35,916:INFO:Importing libraries
2023-07-11 09:56:35,916:INFO:Copying training dataset
2023-07-11 09:56:35,916:INFO:Defining folds
2023-07-11 09:56:35,916:INFO:Declaring metric variables
2023-07-11 09:56:35,916:INFO:Importing untrained model
2023-07-11 09:56:35,916:INFO:AdaBoost Regressor Imported successfully
2023-07-11 09:56:35,931:INFO:Starting cross validation
2023-07-11 09:56:35,932:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:56:40,217:ERROR:create_model() for ada raised an exception or returned all 0.0:
2023-07-11 09:56:40,217:ERROR:Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 792, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
1 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\ensemble\_weight_boosting.py", line 126, in fit
    X, y = self._validate_data(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1122, in check_X_y
    y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1147, in _check_y
    y = y.astype(np.float64)
ValueError: could not convert string to float: 'Divorced'

--------------------------------------------------------------------------------
9 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\ensemble\_weight_boosting.py", line 126, in fit
    X, y = self._validate_data(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1122, in check_X_y
    y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1147, in _check_y
    y = y.astype(np.float64)
ValueError: could not convert string to float: 'Married'


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 808, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
1 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\ensemble\_weight_boosting.py", line 126, in fit
    X, y = self._validate_data(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1122, in check_X_y
    y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1147, in _check_y
    y = y.astype(np.float64)
ValueError: could not convert string to float: 'Divorced'

--------------------------------------------------------------------------------
9 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\ensemble\_weight_boosting.py", line 126, in fit
    X, y = self._validate_data(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1122, in check_X_y
    y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1147, in _check_y
    y = y.astype(np.float64)
ValueError: could not convert string to float: 'Married'


2023-07-11 09:56:41,328:INFO:Initializing Gradient Boosting Regressor
2023-07-11 09:56:41,328:INFO:Total runtime is 2.4139703035354616 minutes
2023-07-11 09:56:41,328:INFO:SubProcess create_model() called ==================================
2023-07-11 09:56:41,329:INFO:Initializing create_model()
2023-07-11 09:56:41,329:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D76B8340>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D92A1D30>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:56:41,329:INFO:Checking exceptions
2023-07-11 09:56:41,329:INFO:Importing libraries
2023-07-11 09:56:41,329:INFO:Copying training dataset
2023-07-11 09:56:41,334:INFO:Defining folds
2023-07-11 09:56:41,334:INFO:Declaring metric variables
2023-07-11 09:56:41,334:INFO:Importing untrained model
2023-07-11 09:56:41,334:INFO:Gradient Boosting Regressor Imported successfully
2023-07-11 09:56:41,334:INFO:Starting cross validation
2023-07-11 09:56:41,336:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:56:45,211:WARNING:create_model() for gbr raised an exception or returned all 0.0, trying without fit_kwargs:
2023-07-11 09:56:45,211:WARNING:Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 792, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
1 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\ensemble\_gb.py", line 442, in fit
    y = self._validate_y(y)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\ensemble\_gb.py", line 1780, in _validate_y
    y = y.astype(DOUBLE)
ValueError: could not convert string to float: 'Divorced'

--------------------------------------------------------------------------------
9 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\ensemble\_gb.py", line 442, in fit
    y = self._validate_y(y)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\ensemble\_gb.py", line 1780, in _validate_y
    y = y.astype(DOUBLE)
ValueError: could not convert string to float: 'Married'


2023-07-11 09:56:45,211:INFO:Initializing create_model()
2023-07-11 09:56:45,211:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D76B8340>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D92A1D30>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:56:45,211:INFO:Checking exceptions
2023-07-11 09:56:45,211:INFO:Importing libraries
2023-07-11 09:56:45,211:INFO:Copying training dataset
2023-07-11 09:56:45,227:INFO:Defining folds
2023-07-11 09:56:45,227:INFO:Declaring metric variables
2023-07-11 09:56:45,227:INFO:Importing untrained model
2023-07-11 09:56:45,227:INFO:Gradient Boosting Regressor Imported successfully
2023-07-11 09:56:45,227:INFO:Starting cross validation
2023-07-11 09:56:45,227:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:56:49,444:ERROR:create_model() for gbr raised an exception or returned all 0.0:
2023-07-11 09:56:49,444:ERROR:Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 792, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
1 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\ensemble\_gb.py", line 442, in fit
    y = self._validate_y(y)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\ensemble\_gb.py", line 1780, in _validate_y
    y = y.astype(DOUBLE)
ValueError: could not convert string to float: 'Divorced'

--------------------------------------------------------------------------------
9 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\ensemble\_gb.py", line 442, in fit
    y = self._validate_y(y)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\ensemble\_gb.py", line 1780, in _validate_y
    y = y.astype(DOUBLE)
ValueError: could not convert string to float: 'Married'


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 808, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
1 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\ensemble\_gb.py", line 442, in fit
    y = self._validate_y(y)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\ensemble\_gb.py", line 1780, in _validate_y
    y = y.astype(DOUBLE)
ValueError: could not convert string to float: 'Divorced'

--------------------------------------------------------------------------------
9 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\ensemble\_gb.py", line 442, in fit
    y = self._validate_y(y)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\ensemble\_gb.py", line 1780, in _validate_y
    y = y.astype(DOUBLE)
ValueError: could not convert string to float: 'Married'


2023-07-11 09:56:50,444:INFO:Initializing Extreme Gradient Boosting
2023-07-11 09:56:50,444:INFO:Total runtime is 2.5658958077430727 minutes
2023-07-11 09:56:50,444:INFO:SubProcess create_model() called ==================================
2023-07-11 09:56:50,444:INFO:Initializing create_model()
2023-07-11 09:56:50,444:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D76B8340>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D92A1D30>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:56:50,444:INFO:Checking exceptions
2023-07-11 09:56:50,444:INFO:Importing libraries
2023-07-11 09:56:50,444:INFO:Copying training dataset
2023-07-11 09:56:50,444:INFO:Defining folds
2023-07-11 09:56:50,444:INFO:Declaring metric variables
2023-07-11 09:56:50,444:INFO:Importing untrained model
2023-07-11 09:56:50,444:INFO:Extreme Gradient Boosting Imported successfully
2023-07-11 09:56:50,444:INFO:Starting cross validation
2023-07-11 09:56:50,444:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:56:55,612:WARNING:create_model() for xgboost raised an exception or returned all 0.0, trying without fit_kwargs:
2023-07-11 09:56:55,628:WARNING:Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 792, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pandas\core\arrays\categorical.py", line 551, in astype
    new_cats = new_cats.astype(dtype=dtype, copy=copy)
ValueError: could not convert string to float: 'Divorced'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\core.py", line 620, in inner_f
    return func(**kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\sklearn.py", line 988, in fit
    train_dmatrix, evals = _wrap_evaluation_matrices(
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\sklearn.py", line 448, in _wrap_evaluation_matrices
    train_dmatrix = create_dmatrix(
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\sklearn.py", line 908, in _create_dmatrix
    return DMatrix(**kwargs, nthread=self.n_jobs)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\core.py", line 620, in inner_f
    return func(**kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\core.py", line 754, in __init__
    self.set_info(
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\core.py", line 620, in inner_f
    return func(**kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\core.py", line 819, in set_info
    self.set_label(label)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\core.py", line 950, in set_label
    dispatch_meta_backend(self, label, 'label', 'float')
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\data.py", line 1134, in dispatch_meta_backend
    _meta_from_pandas_series(data, name, dtype, handle)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\data.py", line 438, in _meta_from_pandas_series
    data = data.values.astype('float')
  File "C:\Users\didit\anaconda3\lib\site-packages\pandas\core\arrays\categorical.py", line 562, in astype
    raise ValueError(msg)
ValueError: Cannot cast object dtype to float64


2023-07-11 09:56:55,628:INFO:Initializing create_model()
2023-07-11 09:56:55,628:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D76B8340>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D92A1D30>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:56:55,628:INFO:Checking exceptions
2023-07-11 09:56:55,628:INFO:Importing libraries
2023-07-11 09:56:55,628:INFO:Copying training dataset
2023-07-11 09:56:55,628:INFO:Defining folds
2023-07-11 09:56:55,628:INFO:Declaring metric variables
2023-07-11 09:56:55,628:INFO:Importing untrained model
2023-07-11 09:56:55,628:INFO:Extreme Gradient Boosting Imported successfully
2023-07-11 09:56:55,628:INFO:Starting cross validation
2023-07-11 09:56:55,628:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:57:00,293:ERROR:create_model() for xgboost raised an exception or returned all 0.0:
2023-07-11 09:57:00,293:ERROR:Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 792, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pandas\core\arrays\categorical.py", line 551, in astype
    new_cats = new_cats.astype(dtype=dtype, copy=copy)
ValueError: could not convert string to float: 'Divorced'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\core.py", line 620, in inner_f
    return func(**kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\sklearn.py", line 988, in fit
    train_dmatrix, evals = _wrap_evaluation_matrices(
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\sklearn.py", line 448, in _wrap_evaluation_matrices
    train_dmatrix = create_dmatrix(
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\sklearn.py", line 908, in _create_dmatrix
    return DMatrix(**kwargs, nthread=self.n_jobs)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\core.py", line 620, in inner_f
    return func(**kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\core.py", line 754, in __init__
    self.set_info(
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\core.py", line 620, in inner_f
    return func(**kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\core.py", line 819, in set_info
    self.set_label(label)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\core.py", line 950, in set_label
    dispatch_meta_backend(self, label, 'label', 'float')
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\data.py", line 1134, in dispatch_meta_backend
    _meta_from_pandas_series(data, name, dtype, handle)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\data.py", line 438, in _meta_from_pandas_series
    data = data.values.astype('float')
  File "C:\Users\didit\anaconda3\lib\site-packages\pandas\core\arrays\categorical.py", line 562, in astype
    raise ValueError(msg)
ValueError: Cannot cast object dtype to float64


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 808, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pandas\core\arrays\categorical.py", line 551, in astype
    new_cats = new_cats.astype(dtype=dtype, copy=copy)
ValueError: could not convert string to float: 'Divorced'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\core.py", line 620, in inner_f
    return func(**kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\sklearn.py", line 988, in fit
    train_dmatrix, evals = _wrap_evaluation_matrices(
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\sklearn.py", line 448, in _wrap_evaluation_matrices
    train_dmatrix = create_dmatrix(
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\sklearn.py", line 908, in _create_dmatrix
    return DMatrix(**kwargs, nthread=self.n_jobs)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\core.py", line 620, in inner_f
    return func(**kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\core.py", line 754, in __init__
    self.set_info(
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\core.py", line 620, in inner_f
    return func(**kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\core.py", line 819, in set_info
    self.set_label(label)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\core.py", line 950, in set_label
    dispatch_meta_backend(self, label, 'label', 'float')
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\data.py", line 1134, in dispatch_meta_backend
    _meta_from_pandas_series(data, name, dtype, handle)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\data.py", line 438, in _meta_from_pandas_series
    data = data.values.astype('float')
  File "C:\Users\didit\anaconda3\lib\site-packages\pandas\core\arrays\categorical.py", line 562, in astype
    raise ValueError(msg)
ValueError: Cannot cast object dtype to float64


2023-07-11 09:57:01,260:INFO:Initializing Light Gradient Boosting Machine
2023-07-11 09:57:01,260:INFO:Total runtime is 2.7461666345596316 minutes
2023-07-11 09:57:01,260:INFO:SubProcess create_model() called ==================================
2023-07-11 09:57:01,260:INFO:Initializing create_model()
2023-07-11 09:57:01,260:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D76B8340>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D92A1D30>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:57:01,260:INFO:Checking exceptions
2023-07-11 09:57:01,260:INFO:Importing libraries
2023-07-11 09:57:01,260:INFO:Copying training dataset
2023-07-11 09:57:01,260:INFO:Defining folds
2023-07-11 09:57:01,260:INFO:Declaring metric variables
2023-07-11 09:57:01,260:INFO:Importing untrained model
2023-07-11 09:57:01,260:INFO:Light Gradient Boosting Machine Imported successfully
2023-07-11 09:57:01,260:INFO:Starting cross validation
2023-07-11 09:57:01,260:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:57:06,676:WARNING:create_model() for lightgbm raised an exception or returned all 0.0, trying without fit_kwargs:
2023-07-11 09:57:06,676:WARNING:Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 792, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\sklearn.py", line 895, in fit
    super().fit(X, y, sample_weight=sample_weight, init_score=init_score,
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\sklearn.py", line 748, in fit
    self._Booster = train(
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\engine.py", line 271, in train
    booster = Booster(params=params, train_set=train_set)
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 2605, in __init__
    train_set.construct()
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 1815, in construct
    self._lazy_init(self.data, label=self.label,
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 1557, in _lazy_init
    self.set_label(label)
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 2163, in set_label
    label = list_to_1d_numpy(_label_from_pandas(label), name='label')
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 186, in list_to_1d_numpy
    raise ValueError('Series.dtypes must be int, float or bool')
ValueError: Series.dtypes must be int, float or bool


2023-07-11 09:57:06,676:INFO:Initializing create_model()
2023-07-11 09:57:06,676:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D76B8340>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D92A1D30>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:57:06,676:INFO:Checking exceptions
2023-07-11 09:57:06,676:INFO:Importing libraries
2023-07-11 09:57:06,676:INFO:Copying training dataset
2023-07-11 09:57:06,676:INFO:Defining folds
2023-07-11 09:57:06,676:INFO:Declaring metric variables
2023-07-11 09:57:06,676:INFO:Importing untrained model
2023-07-11 09:57:06,676:INFO:Light Gradient Boosting Machine Imported successfully
2023-07-11 09:57:06,676:INFO:Starting cross validation
2023-07-11 09:57:06,676:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:57:11,429:ERROR:create_model() for lightgbm raised an exception or returned all 0.0:
2023-07-11 09:57:11,429:ERROR:Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 792, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\sklearn.py", line 895, in fit
    super().fit(X, y, sample_weight=sample_weight, init_score=init_score,
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\sklearn.py", line 748, in fit
    self._Booster = train(
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\engine.py", line 271, in train
    booster = Booster(params=params, train_set=train_set)
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 2605, in __init__
    train_set.construct()
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 1815, in construct
    self._lazy_init(self.data, label=self.label,
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 1557, in _lazy_init
    self.set_label(label)
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 2163, in set_label
    label = list_to_1d_numpy(_label_from_pandas(label), name='label')
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 186, in list_to_1d_numpy
    raise ValueError('Series.dtypes must be int, float or bool')
ValueError: Series.dtypes must be int, float or bool


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 808, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\sklearn.py", line 895, in fit
    super().fit(X, y, sample_weight=sample_weight, init_score=init_score,
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\sklearn.py", line 748, in fit
    self._Booster = train(
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\engine.py", line 271, in train
    booster = Booster(params=params, train_set=train_set)
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 2605, in __init__
    train_set.construct()
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 1815, in construct
    self._lazy_init(self.data, label=self.label,
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 1557, in _lazy_init
    self.set_label(label)
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 2163, in set_label
    label = list_to_1d_numpy(_label_from_pandas(label), name='label')
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 186, in list_to_1d_numpy
    raise ValueError('Series.dtypes must be int, float or bool')
ValueError: Series.dtypes must be int, float or bool


2023-07-11 09:57:12,491:INFO:Initializing Dummy Regressor
2023-07-11 09:57:12,491:INFO:Total runtime is 2.933355534076691 minutes
2023-07-11 09:57:12,491:INFO:SubProcess create_model() called ==================================
2023-07-11 09:57:12,491:INFO:Initializing create_model()
2023-07-11 09:57:12,491:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D76B8340>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D92A1D30>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:57:12,491:INFO:Checking exceptions
2023-07-11 09:57:12,491:INFO:Importing libraries
2023-07-11 09:57:12,491:INFO:Copying training dataset
2023-07-11 09:57:12,491:INFO:Defining folds
2023-07-11 09:57:12,491:INFO:Declaring metric variables
2023-07-11 09:57:12,491:INFO:Importing untrained model
2023-07-11 09:57:12,491:INFO:Dummy Regressor Imported successfully
2023-07-11 09:57:12,491:INFO:Starting cross validation
2023-07-11 09:57:12,491:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:57:16,644:WARNING:create_model() for dummy raised an exception or returned all 0.0, trying without fit_kwargs:
2023-07-11 09:57:16,644:WARNING:Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 792, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\dummy.py", line 554, in fit
    self.constant_ = np.average(y, axis=0, weights=sample_weight)
  File "<__array_function__ internals>", line 5, in average
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\lib\function_base.py", line 380, in average
    avg = a.mean(axis)
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\_methods.py", line 181, in _mean
    ret = um.true_divide(
TypeError: unsupported operand type(s) for /: 'str' and 'int'


2023-07-11 09:57:16,644:INFO:Initializing create_model()
2023-07-11 09:57:16,644:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D76B8340>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D92A1D30>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:57:16,644:INFO:Checking exceptions
2023-07-11 09:57:16,644:INFO:Importing libraries
2023-07-11 09:57:16,644:INFO:Copying training dataset
2023-07-11 09:57:16,660:INFO:Defining folds
2023-07-11 09:57:16,660:INFO:Declaring metric variables
2023-07-11 09:57:16,660:INFO:Importing untrained model
2023-07-11 09:57:16,660:INFO:Dummy Regressor Imported successfully
2023-07-11 09:57:16,660:INFO:Starting cross validation
2023-07-11 09:57:16,660:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:57:20,893:ERROR:create_model() for dummy raised an exception or returned all 0.0:
2023-07-11 09:57:20,909:ERROR:Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 792, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\dummy.py", line 554, in fit
    self.constant_ = np.average(y, axis=0, weights=sample_weight)
  File "<__array_function__ internals>", line 5, in average
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\lib\function_base.py", line 380, in average
    avg = a.mean(axis)
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\_methods.py", line 181, in _mean
    ret = um.true_divide(
TypeError: unsupported operand type(s) for /: 'str' and 'int'


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 808, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\dummy.py", line 554, in fit
    self.constant_ = np.average(y, axis=0, weights=sample_weight)
  File "<__array_function__ internals>", line 5, in average
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\lib\function_base.py", line 380, in average
    avg = a.mean(axis)
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\_methods.py", line 181, in _mean
    ret = um.true_divide(
TypeError: unsupported operand type(s) for /: 'str' and 'int'


2023-07-11 09:57:21,876:INFO:_master_model_container: 2
2023-07-11 09:57:21,876:INFO:_display_container: 2
2023-07-11 09:57:21,876:INFO:[]
2023-07-11 09:57:21,876:INFO:compare_models() successfully completed......................................
2023-07-11 09:57:21,926:INFO:Initializing save_model()
2023-07-11 09:57:21,926:INFO:save_model(model=[], model_name=best_model.pkl, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\didit\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['Age', 'Credit Utilization Ratio',
                                             'Payment History',
                                             'Number of Credit Accounts',
                                             'Loan Amount', 'Interest Rate',
                                             'Loan Term'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['Gender', 'Education Lev...
                                                                         'mapping': Female    0
Male      1
NaN      -1
dtype: int64}]))),
                ('onehot_encoding',
                 TransformerWrapper(include=['Education Level',
                                             'Employment Status',
                                             'Type of Loan'],
                                    transformer=OneHotEncoder(cols=['Education '
                                                                    'Level',
                                                                    'Employment '
                                                                    'Status',
                                                                    'Type of '
                                                                    'Loan'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames()))]), verbose=True, use_case=MLUsecase.REGRESSION, kwargs={})
2023-07-11 09:57:21,927:INFO:Adding model into prep_pipe
2023-07-11 09:57:21,934:INFO:best_model.pkl.pkl saved in current working directory
2023-07-11 09:57:21,954:INFO:Pipeline(memory=FastMemory(location=C:\Users\didit\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['Age', 'Credit Utilization Ratio',
                                             'Payment History',
                                             'Number of Credit Accounts',
                                             'Loan Amount', 'Interest Rate',
                                             'Loan Term'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['Gender', 'Education Lev...
NaN      -1
dtype: int64}]))),
                ('onehot_encoding',
                 TransformerWrapper(include=['Education Level',
                                             'Employment Status',
                                             'Type of Loan'],
                                    transformer=OneHotEncoder(cols=['Education '
                                                                    'Level',
                                                                    'Employment '
                                                                    'Status',
                                                                    'Type of '
                                                                    'Loan'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames())),
                ('trained_model', [])])
2023-07-11 09:57:21,954:INFO:save_model() successfully completed......................................
2023-07-11 09:57:43,045:INFO:PyCaret RegressionExperiment
2023-07-11 09:57:43,045:INFO:Logging name: reg-default-name
2023-07-11 09:57:43,045:INFO:ML Usecase: MLUsecase.REGRESSION
2023-07-11 09:57:43,045:INFO:version 3.0.2
2023-07-11 09:57:43,045:INFO:Initializing setup()
2023-07-11 09:57:43,045:INFO:self.USI: f379
2023-07-11 09:57:43,045:INFO:self._variable_keys: {'_ml_usecase', 'log_plots_param', 'USI', 'X_train', 'pipeline', 'gpu_param', 'exp_name_log', 'y', 'html_param', 'X', 'data', 'exp_id', 'y_train', 'target_param', 'X_test', 'fold_groups_param', 'n_jobs_param', 'idx', 'fold_generator', 'logging_param', 'memory', 'gpu_n_jobs_param', 'fold_shuffle_param', '_available_plots', 'seed', 'y_test', 'transform_target_param'}
2023-07-11 09:57:43,045:INFO:Checking environment
2023-07-11 09:57:43,045:INFO:python_version: 3.9.13
2023-07-11 09:57:43,045:INFO:python_build: ('main', 'Oct 13 2022 21:23:06')
2023-07-11 09:57:43,045:INFO:machine: AMD64
2023-07-11 09:57:43,045:INFO:platform: Windows-10-10.0.22621-SP0
2023-07-11 09:57:43,045:INFO:Memory: svmem(total=16893358080, available=1650860032, percent=90.2, used=15242498048, free=1650860032)
2023-07-11 09:57:43,045:INFO:Physical Core: 8
2023-07-11 09:57:43,045:INFO:Logical Core: 16
2023-07-11 09:57:43,045:INFO:Checking libraries
2023-07-11 09:57:43,045:INFO:System:
2023-07-11 09:57:43,045:INFO:    python: 3.9.13 (main, Oct 13 2022, 21:23:06) [MSC v.1916 64 bit (AMD64)]
2023-07-11 09:57:43,045:INFO:executable: C:\Users\didit\anaconda3\python.exe
2023-07-11 09:57:43,045:INFO:   machine: Windows-10-10.0.22621-SP0
2023-07-11 09:57:43,045:INFO:PyCaret required dependencies:
2023-07-11 09:57:43,045:INFO:                 pip: 23.0.1
2023-07-11 09:57:43,045:INFO:          setuptools: 67.8.0
2023-07-11 09:57:43,045:INFO:             pycaret: 3.0.2
2023-07-11 09:57:43,045:INFO:             IPython: 8.12.0
2023-07-11 09:57:43,045:INFO:          ipywidgets: 8.0.4
2023-07-11 09:57:43,045:INFO:                tqdm: 4.65.0
2023-07-11 09:57:43,045:INFO:               numpy: 1.21.5
2023-07-11 09:57:43,045:INFO:              pandas: 1.5.3
2023-07-11 09:57:43,045:INFO:              jinja2: 3.1.2
2023-07-11 09:57:43,045:INFO:               scipy: 1.10.1
2023-07-11 09:57:43,045:INFO:              joblib: 1.2.0
2023-07-11 09:57:43,045:INFO:             sklearn: 1.2.2
2023-07-11 09:57:43,045:INFO:                pyod: 1.0.9
2023-07-11 09:57:43,045:INFO:            imblearn: 0.10.1
2023-07-11 09:57:43,045:INFO:   category_encoders: 2.6.1
2023-07-11 09:57:43,045:INFO:            lightgbm: 3.3.5
2023-07-11 09:57:43,045:INFO:               numba: 0.57.0
2023-07-11 09:57:43,045:INFO:            requests: 2.29.0
2023-07-11 09:57:43,045:INFO:          matplotlib: 3.7.1
2023-07-11 09:57:43,045:INFO:          scikitplot: 0.3.7
2023-07-11 09:57:43,045:INFO:         yellowbrick: 1.5
2023-07-11 09:57:43,045:INFO:              plotly: 5.9.0
2023-07-11 09:57:43,045:INFO:             kaleido: 0.2.1
2023-07-11 09:57:43,045:INFO:         statsmodels: 0.13.5
2023-07-11 09:57:43,045:INFO:              sktime: 0.17.0
2023-07-11 09:57:43,045:INFO:               tbats: 1.1.3
2023-07-11 09:57:43,045:INFO:            pmdarima: 2.0.3
2023-07-11 09:57:43,045:INFO:              psutil: 5.9.0
2023-07-11 09:57:43,045:INFO:PyCaret optional dependencies:
2023-07-11 09:57:43,045:INFO:                shap: 0.41.0
2023-07-11 09:57:43,061:INFO:           interpret: Not installed
2023-07-11 09:57:43,061:INFO:                umap: Not installed
2023-07-11 09:57:43,061:INFO:    pandas_profiling: 4.3.1
2023-07-11 09:57:43,061:INFO:  explainerdashboard: Not installed
2023-07-11 09:57:43,061:INFO:             autoviz: Not installed
2023-07-11 09:57:43,061:INFO:           fairlearn: Not installed
2023-07-11 09:57:43,061:INFO:             xgboost: 1.7.6
2023-07-11 09:57:43,061:INFO:            catboost: Not installed
2023-07-11 09:57:43,061:INFO:              kmodes: Not installed
2023-07-11 09:57:43,061:INFO:             mlxtend: Not installed
2023-07-11 09:57:43,061:INFO:       statsforecast: Not installed
2023-07-11 09:57:43,061:INFO:        tune_sklearn: Not installed
2023-07-11 09:57:43,061:INFO:                 ray: Not installed
2023-07-11 09:57:43,061:INFO:            hyperopt: Not installed
2023-07-11 09:57:43,061:INFO:              optuna: Not installed
2023-07-11 09:57:43,061:INFO:               skopt: Not installed
2023-07-11 09:57:43,061:INFO:              mlflow: 2.4.2
2023-07-11 09:57:43,061:INFO:              gradio: Not installed
2023-07-11 09:57:43,061:INFO:             fastapi: 0.95.2
2023-07-11 09:57:43,061:INFO:             uvicorn: 0.22.0
2023-07-11 09:57:43,061:INFO:              m2cgen: Not installed
2023-07-11 09:57:43,061:INFO:           evidently: Not installed
2023-07-11 09:57:43,061:INFO:               fugue: Not installed
2023-07-11 09:57:43,061:INFO:           streamlit: 1.23.1
2023-07-11 09:57:43,061:INFO:             prophet: Not installed
2023-07-11 09:57:43,061:INFO:None
2023-07-11 09:57:43,061:INFO:Set up data.
2023-07-11 09:57:43,077:INFO:Set up train/test split.
2023-07-11 09:57:43,077:INFO:Set up index.
2023-07-11 09:57:43,077:INFO:Set up folding strategy.
2023-07-11 09:57:43,077:INFO:Assigning column types.
2023-07-11 09:57:43,077:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-07-11 09:57:43,077:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-07-11 09:57:43,092:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-07-11 09:57:43,092:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 09:57:43,145:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 09:57:43,172:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 09:57:43,172:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:57:43,172:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:57:43,172:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-07-11 09:57:43,172:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-07-11 09:57:43,182:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 09:57:43,214:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 09:57:43,245:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 09:57:43,245:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:57:43,245:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:57:43,261:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-07-11 09:57:43,261:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-07-11 09:57:43,261:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 09:57:43,308:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 09:57:43,343:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 09:57:43,344:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:57:43,346:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:57:43,350:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-07-11 09:57:43,350:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 09:57:43,382:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 09:57:43,429:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 09:57:43,429:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:57:43,429:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:57:43,429:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-07-11 09:57:43,429:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 09:57:43,476:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 09:57:43,511:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 09:57:43,512:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:57:43,514:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:57:43,517:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 09:57:43,563:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 09:57:43,583:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 09:57:43,583:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:57:43,583:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:57:43,583:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-07-11 09:57:43,646:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 09:57:43,678:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 09:57:43,678:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:57:43,678:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:57:43,751:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 09:57:43,773:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 09:57:43,782:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:57:43,782:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:57:43,782:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-07-11 09:57:43,829:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 09:57:43,861:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:57:43,861:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:57:43,930:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 09:57:43,966:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:57:43,966:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:57:43,966:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-07-11 09:57:44,061:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:57:44,061:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:57:44,183:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:57:44,183:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:57:44,183:INFO:Preparing preprocessing pipeline...
2023-07-11 09:57:44,183:INFO:Set up simple imputation.
2023-07-11 09:57:44,183:INFO:Set up encoding of ordinal features.
2023-07-11 09:57:44,183:INFO:Set up encoding of categorical features.
2023-07-11 09:57:44,183:INFO:Set up column name cleaning.
2023-07-11 09:57:44,261:INFO:Finished creating preprocessing pipeline.
2023-07-11 09:57:44,293:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\didit\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['Age', 'Payment History',
                                             'Number of Credit Accounts',
                                             'Loan Amount', 'Interest Rate',
                                             'Loan Term'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['Gender', 'Marital Status',
                                             'Education Level',
                                             'Empl...
dtype: int64}]))),
                ('onehot_encoding',
                 TransformerWrapper(include=['Marital Status',
                                             'Education Level',
                                             'Employment Status',
                                             'Type of Loan'],
                                    transformer=OneHotEncoder(cols=['Marital '
                                                                    'Status',
                                                                    'Education '
                                                                    'Level',
                                                                    'Employment '
                                                                    'Status',
                                                                    'Type of '
                                                                    'Loan'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames()))])
2023-07-11 09:57:44,293:INFO:Creating final display dataframe.
2023-07-11 09:57:44,478:INFO:Setup _display_container:                     Description                     Value
0                    Session id                      2206
1                        Target  Credit Utilization Ratio
2                   Target type                Regression
3           Original data shape                (1000, 12)
4        Transformed data shape                (1000, 21)
5   Transformed train set shape                 (700, 21)
6    Transformed test set shape                 (300, 21)
7              Ordinal features                         1
8              Numeric features                         6
9          Categorical features                         5
10                   Preprocess                      True
11              Imputation type                    simple
12           Numeric imputation                      mean
13       Categorical imputation                      mode
14     Maximum one-hot encoding                        25
15              Encoding method                      None
16               Fold Generator                     KFold
17                  Fold Number                        10
18                     CPU Jobs                        -1
19                      Use GPU                     False
20               Log Experiment                     False
21              Experiment Name          reg-default-name
22                          USI                      f379
2023-07-11 09:57:44,583:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:57:44,598:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:57:44,698:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 09:57:44,698:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 09:57:44,698:INFO:setup() successfully completed in 1.96s...............
2023-07-11 09:57:44,721:INFO:Initializing compare_models()
2023-07-11 09:57:44,721:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1DD292D00>, include=None, fold=None, round=4, cross_validation=True, sort=R2, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x000001E1DD292D00>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2023-07-11 09:57:44,722:INFO:Checking exceptions
2023-07-11 09:57:44,723:INFO:Preparing display monitor
2023-07-11 09:57:44,725:INFO:Initializing Linear Regression
2023-07-11 09:57:44,725:INFO:Total runtime is 0.0 minutes
2023-07-11 09:57:44,725:INFO:SubProcess create_model() called ==================================
2023-07-11 09:57:44,725:INFO:Initializing create_model()
2023-07-11 09:57:44,725:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1DD292D00>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D93698E0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:57:44,725:INFO:Checking exceptions
2023-07-11 09:57:44,725:INFO:Importing libraries
2023-07-11 09:57:44,725:INFO:Copying training dataset
2023-07-11 09:57:44,727:INFO:Defining folds
2023-07-11 09:57:44,727:INFO:Declaring metric variables
2023-07-11 09:57:44,727:INFO:Importing untrained model
2023-07-11 09:57:44,727:INFO:Linear Regression Imported successfully
2023-07-11 09:57:44,728:INFO:Starting cross validation
2023-07-11 09:57:44,728:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:57:48,446:INFO:Calculating mean and std
2023-07-11 09:57:48,446:INFO:Creating metrics dataframe
2023-07-11 09:57:49,317:INFO:Uploading results into container
2023-07-11 09:57:49,317:INFO:Uploading model into container now
2023-07-11 09:57:49,317:INFO:_master_model_container: 1
2023-07-11 09:57:49,317:INFO:_display_container: 2
2023-07-11 09:57:49,317:INFO:LinearRegression(n_jobs=-1)
2023-07-11 09:57:49,317:INFO:create_model() successfully completed......................................
2023-07-11 09:57:49,446:INFO:SubProcess create_model() end ==================================
2023-07-11 09:57:49,446:INFO:Creating metrics dataframe
2023-07-11 09:57:49,446:INFO:Initializing Lasso Regression
2023-07-11 09:57:49,446:INFO:Total runtime is 0.0786696712176005 minutes
2023-07-11 09:57:49,446:INFO:SubProcess create_model() called ==================================
2023-07-11 09:57:49,446:INFO:Initializing create_model()
2023-07-11 09:57:49,446:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1DD292D00>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D93698E0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:57:49,446:INFO:Checking exceptions
2023-07-11 09:57:49,446:INFO:Importing libraries
2023-07-11 09:57:49,446:INFO:Copying training dataset
2023-07-11 09:57:49,461:INFO:Defining folds
2023-07-11 09:57:49,461:INFO:Declaring metric variables
2023-07-11 09:57:49,461:INFO:Importing untrained model
2023-07-11 09:57:49,461:INFO:Lasso Regression Imported successfully
2023-07-11 09:57:49,461:INFO:Starting cross validation
2023-07-11 09:57:49,461:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:57:53,015:INFO:Calculating mean and std
2023-07-11 09:57:53,015:INFO:Creating metrics dataframe
2023-07-11 09:57:53,793:INFO:Uploading results into container
2023-07-11 09:57:53,793:INFO:Uploading model into container now
2023-07-11 09:57:53,793:INFO:_master_model_container: 2
2023-07-11 09:57:53,793:INFO:_display_container: 2
2023-07-11 09:57:53,793:INFO:Lasso(random_state=2206)
2023-07-11 09:57:53,793:INFO:create_model() successfully completed......................................
2023-07-11 09:57:53,922:INFO:SubProcess create_model() end ==================================
2023-07-11 09:57:53,922:INFO:Creating metrics dataframe
2023-07-11 09:57:53,931:INFO:Initializing Ridge Regression
2023-07-11 09:57:53,932:INFO:Total runtime is 0.15342525243759156 minutes
2023-07-11 09:57:53,932:INFO:SubProcess create_model() called ==================================
2023-07-11 09:57:53,932:INFO:Initializing create_model()
2023-07-11 09:57:53,933:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1DD292D00>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D93698E0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:57:53,933:INFO:Checking exceptions
2023-07-11 09:57:53,933:INFO:Importing libraries
2023-07-11 09:57:53,933:INFO:Copying training dataset
2023-07-11 09:57:53,938:INFO:Defining folds
2023-07-11 09:57:53,938:INFO:Declaring metric variables
2023-07-11 09:57:53,938:INFO:Importing untrained model
2023-07-11 09:57:53,939:INFO:Ridge Regression Imported successfully
2023-07-11 09:57:53,939:INFO:Starting cross validation
2023-07-11 09:57:53,940:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:57:57,568:INFO:Calculating mean and std
2023-07-11 09:57:57,568:INFO:Creating metrics dataframe
2023-07-11 09:57:58,293:INFO:Uploading results into container
2023-07-11 09:57:58,293:INFO:Uploading model into container now
2023-07-11 09:57:58,293:INFO:_master_model_container: 3
2023-07-11 09:57:58,293:INFO:_display_container: 2
2023-07-11 09:57:58,293:INFO:Ridge(random_state=2206)
2023-07-11 09:57:58,293:INFO:create_model() successfully completed......................................
2023-07-11 09:57:58,414:INFO:SubProcess create_model() end ==================================
2023-07-11 09:57:58,414:INFO:Creating metrics dataframe
2023-07-11 09:57:58,414:INFO:Initializing Elastic Net
2023-07-11 09:57:58,414:INFO:Total runtime is 0.2281412442525228 minutes
2023-07-11 09:57:58,414:INFO:SubProcess create_model() called ==================================
2023-07-11 09:57:58,414:INFO:Initializing create_model()
2023-07-11 09:57:58,414:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1DD292D00>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D93698E0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:57:58,414:INFO:Checking exceptions
2023-07-11 09:57:58,414:INFO:Importing libraries
2023-07-11 09:57:58,414:INFO:Copying training dataset
2023-07-11 09:57:58,430:INFO:Defining folds
2023-07-11 09:57:58,430:INFO:Declaring metric variables
2023-07-11 09:57:58,430:INFO:Importing untrained model
2023-07-11 09:57:58,430:INFO:Elastic Net Imported successfully
2023-07-11 09:57:58,430:INFO:Starting cross validation
2023-07-11 09:57:58,430:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:58:02,093:INFO:Calculating mean and std
2023-07-11 09:58:02,093:INFO:Creating metrics dataframe
2023-07-11 09:58:02,768:INFO:Uploading results into container
2023-07-11 09:58:02,768:INFO:Uploading model into container now
2023-07-11 09:58:02,768:INFO:_master_model_container: 4
2023-07-11 09:58:02,768:INFO:_display_container: 2
2023-07-11 09:58:02,768:INFO:ElasticNet(random_state=2206)
2023-07-11 09:58:02,768:INFO:create_model() successfully completed......................................
2023-07-11 09:58:02,893:INFO:SubProcess create_model() end ==================================
2023-07-11 09:58:02,893:INFO:Creating metrics dataframe
2023-07-11 09:58:02,918:INFO:Initializing Least Angle Regression
2023-07-11 09:58:02,918:INFO:Total runtime is 0.303214697043101 minutes
2023-07-11 09:58:02,918:INFO:SubProcess create_model() called ==================================
2023-07-11 09:58:02,918:INFO:Initializing create_model()
2023-07-11 09:58:02,918:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1DD292D00>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D93698E0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:58:02,918:INFO:Checking exceptions
2023-07-11 09:58:02,918:INFO:Importing libraries
2023-07-11 09:58:02,918:INFO:Copying training dataset
2023-07-11 09:58:02,918:INFO:Defining folds
2023-07-11 09:58:02,918:INFO:Declaring metric variables
2023-07-11 09:58:02,918:INFO:Importing untrained model
2023-07-11 09:58:02,918:INFO:Least Angle Regression Imported successfully
2023-07-11 09:58:02,918:INFO:Starting cross validation
2023-07-11 09:58:02,918:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:58:06,536:INFO:Calculating mean and std
2023-07-11 09:58:06,537:INFO:Creating metrics dataframe
2023-07-11 09:58:07,199:INFO:Uploading results into container
2023-07-11 09:58:07,199:INFO:Uploading model into container now
2023-07-11 09:58:07,199:INFO:_master_model_container: 5
2023-07-11 09:58:07,199:INFO:_display_container: 2
2023-07-11 09:58:07,199:INFO:Lars(random_state=2206)
2023-07-11 09:58:07,199:INFO:create_model() successfully completed......................................
2023-07-11 09:58:07,328:INFO:SubProcess create_model() end ==================================
2023-07-11 09:58:07,329:INFO:Creating metrics dataframe
2023-07-11 09:58:07,332:INFO:Initializing Lasso Least Angle Regression
2023-07-11 09:58:07,333:INFO:Total runtime is 0.3767924825350444 minutes
2023-07-11 09:58:07,333:INFO:SubProcess create_model() called ==================================
2023-07-11 09:58:07,333:INFO:Initializing create_model()
2023-07-11 09:58:07,333:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1DD292D00>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D93698E0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:58:07,333:INFO:Checking exceptions
2023-07-11 09:58:07,333:INFO:Importing libraries
2023-07-11 09:58:07,333:INFO:Copying training dataset
2023-07-11 09:58:07,337:INFO:Defining folds
2023-07-11 09:58:07,337:INFO:Declaring metric variables
2023-07-11 09:58:07,337:INFO:Importing untrained model
2023-07-11 09:58:07,337:INFO:Lasso Least Angle Regression Imported successfully
2023-07-11 09:58:07,337:INFO:Starting cross validation
2023-07-11 09:58:07,338:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:58:10,969:INFO:Calculating mean and std
2023-07-11 09:58:10,969:INFO:Creating metrics dataframe
2023-07-11 09:58:11,647:INFO:Uploading results into container
2023-07-11 09:58:11,647:INFO:Uploading model into container now
2023-07-11 09:58:11,647:INFO:_master_model_container: 6
2023-07-11 09:58:11,647:INFO:_display_container: 2
2023-07-11 09:58:11,647:INFO:LassoLars(random_state=2206)
2023-07-11 09:58:11,647:INFO:create_model() successfully completed......................................
2023-07-11 09:58:11,783:INFO:SubProcess create_model() end ==================================
2023-07-11 09:58:11,783:INFO:Creating metrics dataframe
2023-07-11 09:58:11,783:INFO:Initializing Orthogonal Matching Pursuit
2023-07-11 09:58:11,783:INFO:Total runtime is 0.4509522279103597 minutes
2023-07-11 09:58:11,783:INFO:SubProcess create_model() called ==================================
2023-07-11 09:58:11,783:INFO:Initializing create_model()
2023-07-11 09:58:11,783:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1DD292D00>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D93698E0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:58:11,783:INFO:Checking exceptions
2023-07-11 09:58:11,783:INFO:Importing libraries
2023-07-11 09:58:11,783:INFO:Copying training dataset
2023-07-11 09:58:11,783:INFO:Defining folds
2023-07-11 09:58:11,783:INFO:Declaring metric variables
2023-07-11 09:58:11,783:INFO:Importing untrained model
2023-07-11 09:58:11,783:INFO:Orthogonal Matching Pursuit Imported successfully
2023-07-11 09:58:11,783:INFO:Starting cross validation
2023-07-11 09:58:11,783:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:58:15,399:INFO:Calculating mean and std
2023-07-11 09:58:15,399:INFO:Creating metrics dataframe
2023-07-11 09:58:16,094:INFO:Uploading results into container
2023-07-11 09:58:16,094:INFO:Uploading model into container now
2023-07-11 09:58:16,094:INFO:_master_model_container: 7
2023-07-11 09:58:16,094:INFO:_display_container: 2
2023-07-11 09:58:16,094:INFO:OrthogonalMatchingPursuit()
2023-07-11 09:58:16,094:INFO:create_model() successfully completed......................................
2023-07-11 09:58:16,246:INFO:SubProcess create_model() end ==================================
2023-07-11 09:58:16,246:INFO:Creating metrics dataframe
2023-07-11 09:58:16,246:INFO:Initializing Bayesian Ridge
2023-07-11 09:58:16,246:INFO:Total runtime is 0.525342623392741 minutes
2023-07-11 09:58:16,246:INFO:SubProcess create_model() called ==================================
2023-07-11 09:58:16,246:INFO:Initializing create_model()
2023-07-11 09:58:16,246:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1DD292D00>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D93698E0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:58:16,246:INFO:Checking exceptions
2023-07-11 09:58:16,246:INFO:Importing libraries
2023-07-11 09:58:16,246:INFO:Copying training dataset
2023-07-11 09:58:16,246:INFO:Defining folds
2023-07-11 09:58:16,246:INFO:Declaring metric variables
2023-07-11 09:58:16,246:INFO:Importing untrained model
2023-07-11 09:58:16,246:INFO:Bayesian Ridge Imported successfully
2023-07-11 09:58:16,246:INFO:Starting cross validation
2023-07-11 09:58:16,262:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:58:19,935:INFO:Calculating mean and std
2023-07-11 09:58:19,935:INFO:Creating metrics dataframe
2023-07-11 09:58:20,647:INFO:Uploading results into container
2023-07-11 09:58:20,647:INFO:Uploading model into container now
2023-07-11 09:58:20,647:INFO:_master_model_container: 8
2023-07-11 09:58:20,647:INFO:_display_container: 2
2023-07-11 09:58:20,647:INFO:BayesianRidge()
2023-07-11 09:58:20,647:INFO:create_model() successfully completed......................................
2023-07-11 09:58:20,784:INFO:SubProcess create_model() end ==================================
2023-07-11 09:58:20,784:INFO:Creating metrics dataframe
2023-07-11 09:58:20,784:INFO:Initializing Passive Aggressive Regressor
2023-07-11 09:58:20,784:INFO:Total runtime is 0.6009700894355775 minutes
2023-07-11 09:58:20,784:INFO:SubProcess create_model() called ==================================
2023-07-11 09:58:20,784:INFO:Initializing create_model()
2023-07-11 09:58:20,784:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1DD292D00>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D93698E0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:58:20,784:INFO:Checking exceptions
2023-07-11 09:58:20,784:INFO:Importing libraries
2023-07-11 09:58:20,784:INFO:Copying training dataset
2023-07-11 09:58:20,784:INFO:Defining folds
2023-07-11 09:58:20,784:INFO:Declaring metric variables
2023-07-11 09:58:20,784:INFO:Importing untrained model
2023-07-11 09:58:20,784:INFO:Passive Aggressive Regressor Imported successfully
2023-07-11 09:58:20,784:INFO:Starting cross validation
2023-07-11 09:58:20,784:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:58:24,448:INFO:Calculating mean and std
2023-07-11 09:58:24,448:INFO:Creating metrics dataframe
2023-07-11 09:58:25,216:INFO:Uploading results into container
2023-07-11 09:58:25,216:INFO:Uploading model into container now
2023-07-11 09:58:25,216:INFO:_master_model_container: 9
2023-07-11 09:58:25,216:INFO:_display_container: 2
2023-07-11 09:58:25,216:INFO:PassiveAggressiveRegressor(random_state=2206)
2023-07-11 09:58:25,216:INFO:create_model() successfully completed......................................
2023-07-11 09:58:25,368:INFO:SubProcess create_model() end ==================================
2023-07-11 09:58:25,368:INFO:Creating metrics dataframe
2023-07-11 09:58:25,384:INFO:Initializing Huber Regressor
2023-07-11 09:58:25,384:INFO:Total runtime is 0.6776414195696514 minutes
2023-07-11 09:58:25,384:INFO:SubProcess create_model() called ==================================
2023-07-11 09:58:25,384:INFO:Initializing create_model()
2023-07-11 09:58:25,384:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1DD292D00>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D93698E0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:58:25,384:INFO:Checking exceptions
2023-07-11 09:58:25,384:INFO:Importing libraries
2023-07-11 09:58:25,384:INFO:Copying training dataset
2023-07-11 09:58:25,384:INFO:Defining folds
2023-07-11 09:58:25,384:INFO:Declaring metric variables
2023-07-11 09:58:25,384:INFO:Importing untrained model
2023-07-11 09:58:25,384:INFO:Huber Regressor Imported successfully
2023-07-11 09:58:25,384:INFO:Starting cross validation
2023-07-11 09:58:25,384:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:58:29,111:INFO:Calculating mean and std
2023-07-11 09:58:29,111:INFO:Creating metrics dataframe
2023-07-11 09:58:29,799:INFO:Uploading results into container
2023-07-11 09:58:29,799:INFO:Uploading model into container now
2023-07-11 09:58:29,799:INFO:_master_model_container: 10
2023-07-11 09:58:29,799:INFO:_display_container: 2
2023-07-11 09:58:29,799:INFO:HuberRegressor()
2023-07-11 09:58:29,799:INFO:create_model() successfully completed......................................
2023-07-11 09:58:29,935:INFO:SubProcess create_model() end ==================================
2023-07-11 09:58:29,935:INFO:Creating metrics dataframe
2023-07-11 09:58:29,949:INFO:Initializing K Neighbors Regressor
2023-07-11 09:58:29,949:INFO:Total runtime is 0.7537225246429444 minutes
2023-07-11 09:58:29,949:INFO:SubProcess create_model() called ==================================
2023-07-11 09:58:29,949:INFO:Initializing create_model()
2023-07-11 09:58:29,949:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1DD292D00>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D93698E0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:58:29,949:INFO:Checking exceptions
2023-07-11 09:58:29,949:INFO:Importing libraries
2023-07-11 09:58:29,949:INFO:Copying training dataset
2023-07-11 09:58:29,956:INFO:Defining folds
2023-07-11 09:58:29,956:INFO:Declaring metric variables
2023-07-11 09:58:29,956:INFO:Importing untrained model
2023-07-11 09:58:29,956:INFO:K Neighbors Regressor Imported successfully
2023-07-11 09:58:29,956:INFO:Starting cross validation
2023-07-11 09:58:29,957:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:58:33,740:INFO:Calculating mean and std
2023-07-11 09:58:33,741:INFO:Creating metrics dataframe
2023-07-11 09:58:34,447:INFO:Uploading results into container
2023-07-11 09:58:34,447:INFO:Uploading model into container now
2023-07-11 09:58:34,447:INFO:_master_model_container: 11
2023-07-11 09:58:34,447:INFO:_display_container: 2
2023-07-11 09:58:34,447:INFO:KNeighborsRegressor(n_jobs=-1)
2023-07-11 09:58:34,447:INFO:create_model() successfully completed......................................
2023-07-11 09:58:34,584:INFO:SubProcess create_model() end ==================================
2023-07-11 09:58:34,584:INFO:Creating metrics dataframe
2023-07-11 09:58:34,584:INFO:Initializing Decision Tree Regressor
2023-07-11 09:58:34,584:INFO:Total runtime is 0.8309706012407939 minutes
2023-07-11 09:58:34,584:INFO:SubProcess create_model() called ==================================
2023-07-11 09:58:34,584:INFO:Initializing create_model()
2023-07-11 09:58:34,584:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1DD292D00>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D93698E0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:58:34,584:INFO:Checking exceptions
2023-07-11 09:58:34,584:INFO:Importing libraries
2023-07-11 09:58:34,584:INFO:Copying training dataset
2023-07-11 09:58:34,584:INFO:Defining folds
2023-07-11 09:58:34,584:INFO:Declaring metric variables
2023-07-11 09:58:34,584:INFO:Importing untrained model
2023-07-11 09:58:34,584:INFO:Decision Tree Regressor Imported successfully
2023-07-11 09:58:34,584:INFO:Starting cross validation
2023-07-11 09:58:34,584:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:58:38,367:INFO:Calculating mean and std
2023-07-11 09:58:38,367:INFO:Creating metrics dataframe
2023-07-11 09:58:39,047:INFO:Uploading results into container
2023-07-11 09:58:39,047:INFO:Uploading model into container now
2023-07-11 09:58:39,047:INFO:_master_model_container: 12
2023-07-11 09:58:39,047:INFO:_display_container: 2
2023-07-11 09:58:39,047:INFO:DecisionTreeRegressor(random_state=2206)
2023-07-11 09:58:39,047:INFO:create_model() successfully completed......................................
2023-07-11 09:58:39,183:INFO:SubProcess create_model() end ==================================
2023-07-11 09:58:39,183:INFO:Creating metrics dataframe
2023-07-11 09:58:39,199:INFO:Initializing Random Forest Regressor
2023-07-11 09:58:39,199:INFO:Total runtime is 0.9078875263532004 minutes
2023-07-11 09:58:39,199:INFO:SubProcess create_model() called ==================================
2023-07-11 09:58:39,199:INFO:Initializing create_model()
2023-07-11 09:58:39,199:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1DD292D00>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D93698E0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:58:39,199:INFO:Checking exceptions
2023-07-11 09:58:39,199:INFO:Importing libraries
2023-07-11 09:58:39,199:INFO:Copying training dataset
2023-07-11 09:58:39,199:INFO:Defining folds
2023-07-11 09:58:39,199:INFO:Declaring metric variables
2023-07-11 09:58:39,199:INFO:Importing untrained model
2023-07-11 09:58:39,199:INFO:Random Forest Regressor Imported successfully
2023-07-11 09:58:39,199:INFO:Starting cross validation
2023-07-11 09:58:39,214:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:58:43,878:INFO:Calculating mean and std
2023-07-11 09:58:43,878:INFO:Creating metrics dataframe
2023-07-11 09:58:44,562:INFO:Uploading results into container
2023-07-11 09:58:44,563:INFO:Uploading model into container now
2023-07-11 09:58:44,564:INFO:_master_model_container: 13
2023-07-11 09:58:44,564:INFO:_display_container: 2
2023-07-11 09:58:44,565:INFO:RandomForestRegressor(n_jobs=-1, random_state=2206)
2023-07-11 09:58:44,565:INFO:create_model() successfully completed......................................
2023-07-11 09:58:44,695:INFO:SubProcess create_model() end ==================================
2023-07-11 09:58:44,695:INFO:Creating metrics dataframe
2023-07-11 09:58:44,725:INFO:Initializing Extra Trees Regressor
2023-07-11 09:58:44,725:INFO:Total runtime is 0.9999854922294618 minutes
2023-07-11 09:58:44,725:INFO:SubProcess create_model() called ==================================
2023-07-11 09:58:44,725:INFO:Initializing create_model()
2023-07-11 09:58:44,725:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1DD292D00>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D93698E0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:58:44,725:INFO:Checking exceptions
2023-07-11 09:58:44,725:INFO:Importing libraries
2023-07-11 09:58:44,725:INFO:Copying training dataset
2023-07-11 09:58:44,731:INFO:Defining folds
2023-07-11 09:58:44,731:INFO:Declaring metric variables
2023-07-11 09:58:44,732:INFO:Importing untrained model
2023-07-11 09:58:44,732:INFO:Extra Trees Regressor Imported successfully
2023-07-11 09:58:44,732:INFO:Starting cross validation
2023-07-11 09:58:44,733:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:58:49,232:INFO:Calculating mean and std
2023-07-11 09:58:49,232:INFO:Creating metrics dataframe
2023-07-11 09:58:49,911:INFO:Uploading results into container
2023-07-11 09:58:49,912:INFO:Uploading model into container now
2023-07-11 09:58:49,912:INFO:_master_model_container: 14
2023-07-11 09:58:49,913:INFO:_display_container: 2
2023-07-11 09:58:49,913:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=2206)
2023-07-11 09:58:49,913:INFO:create_model() successfully completed......................................
2023-07-11 09:58:50,031:INFO:SubProcess create_model() end ==================================
2023-07-11 09:58:50,031:INFO:Creating metrics dataframe
2023-07-11 09:58:50,047:INFO:Initializing AdaBoost Regressor
2023-07-11 09:58:50,047:INFO:Total runtime is 1.0886936783790588 minutes
2023-07-11 09:58:50,047:INFO:SubProcess create_model() called ==================================
2023-07-11 09:58:50,047:INFO:Initializing create_model()
2023-07-11 09:58:50,047:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1DD292D00>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D93698E0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:58:50,047:INFO:Checking exceptions
2023-07-11 09:58:50,047:INFO:Importing libraries
2023-07-11 09:58:50,047:INFO:Copying training dataset
2023-07-11 09:58:50,063:INFO:Defining folds
2023-07-11 09:58:50,063:INFO:Declaring metric variables
2023-07-11 09:58:50,063:INFO:Importing untrained model
2023-07-11 09:58:50,063:INFO:AdaBoost Regressor Imported successfully
2023-07-11 09:58:50,063:INFO:Starting cross validation
2023-07-11 09:58:50,063:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:58:54,031:INFO:Calculating mean and std
2023-07-11 09:58:54,031:INFO:Creating metrics dataframe
2023-07-11 09:58:54,753:INFO:Uploading results into container
2023-07-11 09:58:54,753:INFO:Uploading model into container now
2023-07-11 09:58:54,753:INFO:_master_model_container: 15
2023-07-11 09:58:54,753:INFO:_display_container: 2
2023-07-11 09:58:54,753:INFO:AdaBoostRegressor(random_state=2206)
2023-07-11 09:58:54,753:INFO:create_model() successfully completed......................................
2023-07-11 09:58:54,896:INFO:SubProcess create_model() end ==================================
2023-07-11 09:58:54,896:INFO:Creating metrics dataframe
2023-07-11 09:58:54,916:INFO:Initializing Gradient Boosting Regressor
2023-07-11 09:58:54,916:INFO:Total runtime is 1.1698441624641418 minutes
2023-07-11 09:58:54,916:INFO:SubProcess create_model() called ==================================
2023-07-11 09:58:54,916:INFO:Initializing create_model()
2023-07-11 09:58:54,916:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1DD292D00>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D93698E0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:58:54,916:INFO:Checking exceptions
2023-07-11 09:58:54,916:INFO:Importing libraries
2023-07-11 09:58:54,916:INFO:Copying training dataset
2023-07-11 09:58:54,919:INFO:Defining folds
2023-07-11 09:58:54,919:INFO:Declaring metric variables
2023-07-11 09:58:54,919:INFO:Importing untrained model
2023-07-11 09:58:54,919:INFO:Gradient Boosting Regressor Imported successfully
2023-07-11 09:58:54,919:INFO:Starting cross validation
2023-07-11 09:58:54,919:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:58:59,231:INFO:Calculating mean and std
2023-07-11 09:58:59,231:INFO:Creating metrics dataframe
2023-07-11 09:58:59,919:INFO:Uploading results into container
2023-07-11 09:58:59,934:INFO:Uploading model into container now
2023-07-11 09:58:59,935:INFO:_master_model_container: 16
2023-07-11 09:58:59,935:INFO:_display_container: 2
2023-07-11 09:58:59,935:INFO:GradientBoostingRegressor(random_state=2206)
2023-07-11 09:58:59,935:INFO:create_model() successfully completed......................................
2023-07-11 09:59:00,063:INFO:SubProcess create_model() end ==================================
2023-07-11 09:59:00,063:INFO:Creating metrics dataframe
2023-07-11 09:59:00,063:INFO:Initializing Extreme Gradient Boosting
2023-07-11 09:59:00,063:INFO:Total runtime is 1.2556296308835346 minutes
2023-07-11 09:59:00,063:INFO:SubProcess create_model() called ==================================
2023-07-11 09:59:00,063:INFO:Initializing create_model()
2023-07-11 09:59:00,063:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1DD292D00>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D93698E0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:59:00,063:INFO:Checking exceptions
2023-07-11 09:59:00,063:INFO:Importing libraries
2023-07-11 09:59:00,063:INFO:Copying training dataset
2023-07-11 09:59:00,063:INFO:Defining folds
2023-07-11 09:59:00,063:INFO:Declaring metric variables
2023-07-11 09:59:00,063:INFO:Importing untrained model
2023-07-11 09:59:00,063:INFO:Extreme Gradient Boosting Imported successfully
2023-07-11 09:59:00,063:INFO:Starting cross validation
2023-07-11 09:59:00,079:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:59:04,332:INFO:Calculating mean and std
2023-07-11 09:59:04,333:INFO:Creating metrics dataframe
2023-07-11 09:59:05,016:INFO:Uploading results into container
2023-07-11 09:59:05,016:INFO:Uploading model into container now
2023-07-11 09:59:05,016:INFO:_master_model_container: 17
2023-07-11 09:59:05,016:INFO:_display_container: 2
2023-07-11 09:59:05,016:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             n_estimators=100, n_jobs=-1, num_parallel_tree=None,
             predictor=None, random_state=2206, ...)
2023-07-11 09:59:05,016:INFO:create_model() successfully completed......................................
2023-07-11 09:59:05,166:INFO:SubProcess create_model() end ==================================
2023-07-11 09:59:05,166:INFO:Creating metrics dataframe
2023-07-11 09:59:05,169:INFO:Initializing Light Gradient Boosting Machine
2023-07-11 09:59:05,169:INFO:Total runtime is 1.3407312512397764 minutes
2023-07-11 09:59:05,169:INFO:SubProcess create_model() called ==================================
2023-07-11 09:59:05,169:INFO:Initializing create_model()
2023-07-11 09:59:05,169:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1DD292D00>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D93698E0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:59:05,169:INFO:Checking exceptions
2023-07-11 09:59:05,169:INFO:Importing libraries
2023-07-11 09:59:05,169:INFO:Copying training dataset
2023-07-11 09:59:05,185:INFO:Defining folds
2023-07-11 09:59:05,185:INFO:Declaring metric variables
2023-07-11 09:59:05,185:INFO:Importing untrained model
2023-07-11 09:59:05,185:INFO:Light Gradient Boosting Machine Imported successfully
2023-07-11 09:59:05,185:INFO:Starting cross validation
2023-07-11 09:59:05,185:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:59:09,119:INFO:Calculating mean and std
2023-07-11 09:59:09,119:INFO:Creating metrics dataframe
2023-07-11 09:59:09,895:INFO:Uploading results into container
2023-07-11 09:59:09,895:INFO:Uploading model into container now
2023-07-11 09:59:09,895:INFO:_master_model_container: 18
2023-07-11 09:59:09,895:INFO:_display_container: 2
2023-07-11 09:59:09,895:INFO:LGBMRegressor(random_state=2206)
2023-07-11 09:59:09,895:INFO:create_model() successfully completed......................................
2023-07-11 09:59:10,020:INFO:SubProcess create_model() end ==================================
2023-07-11 09:59:10,020:INFO:Creating metrics dataframe
2023-07-11 09:59:10,032:INFO:Initializing Dummy Regressor
2023-07-11 09:59:10,032:INFO:Total runtime is 1.421777009963989 minutes
2023-07-11 09:59:10,032:INFO:SubProcess create_model() called ==================================
2023-07-11 09:59:10,032:INFO:Initializing create_model()
2023-07-11 09:59:10,032:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1DD292D00>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D93698E0>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:59:10,032:INFO:Checking exceptions
2023-07-11 09:59:10,032:INFO:Importing libraries
2023-07-11 09:59:10,032:INFO:Copying training dataset
2023-07-11 09:59:10,032:INFO:Defining folds
2023-07-11 09:59:10,032:INFO:Declaring metric variables
2023-07-11 09:59:10,032:INFO:Importing untrained model
2023-07-11 09:59:10,048:INFO:Dummy Regressor Imported successfully
2023-07-11 09:59:10,048:INFO:Starting cross validation
2023-07-11 09:59:10,048:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 09:59:13,942:INFO:Calculating mean and std
2023-07-11 09:59:13,942:INFO:Creating metrics dataframe
2023-07-11 09:59:14,648:INFO:Uploading results into container
2023-07-11 09:59:14,648:INFO:Uploading model into container now
2023-07-11 09:59:14,648:INFO:_master_model_container: 19
2023-07-11 09:59:14,648:INFO:_display_container: 2
2023-07-11 09:59:14,648:INFO:DummyRegressor()
2023-07-11 09:59:14,648:INFO:create_model() successfully completed......................................
2023-07-11 09:59:14,785:INFO:SubProcess create_model() end ==================================
2023-07-11 09:59:14,785:INFO:Creating metrics dataframe
2023-07-11 09:59:14,800:INFO:Initializing create_model()
2023-07-11 09:59:14,800:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1DD292D00>, estimator=DummyRegressor(), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-07-11 09:59:14,800:INFO:Checking exceptions
2023-07-11 09:59:14,800:INFO:Importing libraries
2023-07-11 09:59:14,800:INFO:Copying training dataset
2023-07-11 09:59:14,800:INFO:Defining folds
2023-07-11 09:59:14,800:INFO:Declaring metric variables
2023-07-11 09:59:14,800:INFO:Importing untrained model
2023-07-11 09:59:14,800:INFO:Declaring custom model
2023-07-11 09:59:14,800:INFO:Dummy Regressor Imported successfully
2023-07-11 09:59:14,800:INFO:Cross validation set to False
2023-07-11 09:59:14,800:INFO:Fitting Model
2023-07-11 09:59:15,246:INFO:DummyRegressor()
2023-07-11 09:59:15,246:INFO:create_model() successfully completed......................................
2023-07-11 09:59:15,400:INFO:_master_model_container: 19
2023-07-11 09:59:15,400:INFO:_display_container: 2
2023-07-11 09:59:15,416:INFO:DummyRegressor()
2023-07-11 09:59:15,416:INFO:compare_models() successfully completed......................................
2023-07-11 10:00:38,199:INFO:PyCaret RegressionExperiment
2023-07-11 10:00:38,199:INFO:Logging name: reg-default-name
2023-07-11 10:00:38,199:INFO:ML Usecase: MLUsecase.REGRESSION
2023-07-11 10:00:38,199:INFO:version 3.0.2
2023-07-11 10:00:38,199:INFO:Initializing setup()
2023-07-11 10:00:38,199:INFO:self.USI: 2861
2023-07-11 10:00:38,199:INFO:self._variable_keys: {'_ml_usecase', 'log_plots_param', 'USI', 'X_train', 'pipeline', 'gpu_param', 'exp_name_log', 'y', 'html_param', 'X', 'data', 'exp_id', 'y_train', 'target_param', 'X_test', 'fold_groups_param', 'n_jobs_param', 'idx', 'fold_generator', 'logging_param', 'memory', 'gpu_n_jobs_param', 'fold_shuffle_param', '_available_plots', 'seed', 'y_test', 'transform_target_param'}
2023-07-11 10:00:38,199:INFO:Checking environment
2023-07-11 10:00:38,199:INFO:python_version: 3.9.13
2023-07-11 10:00:38,199:INFO:python_build: ('main', 'Oct 13 2022 21:23:06')
2023-07-11 10:00:38,199:INFO:machine: AMD64
2023-07-11 10:00:38,199:INFO:platform: Windows-10-10.0.22621-SP0
2023-07-11 10:00:38,199:INFO:Memory: svmem(total=16893358080, available=2786058240, percent=83.5, used=14107299840, free=2786058240)
2023-07-11 10:00:38,199:INFO:Physical Core: 8
2023-07-11 10:00:38,199:INFO:Logical Core: 16
2023-07-11 10:00:38,199:INFO:Checking libraries
2023-07-11 10:00:38,199:INFO:System:
2023-07-11 10:00:38,199:INFO:    python: 3.9.13 (main, Oct 13 2022, 21:23:06) [MSC v.1916 64 bit (AMD64)]
2023-07-11 10:00:38,208:INFO:executable: C:\Users\didit\anaconda3\python.exe
2023-07-11 10:00:38,208:INFO:   machine: Windows-10-10.0.22621-SP0
2023-07-11 10:00:38,208:INFO:PyCaret required dependencies:
2023-07-11 10:00:38,208:INFO:                 pip: 23.0.1
2023-07-11 10:00:38,208:INFO:          setuptools: 67.8.0
2023-07-11 10:00:38,208:INFO:             pycaret: 3.0.2
2023-07-11 10:00:38,208:INFO:             IPython: 8.12.0
2023-07-11 10:00:38,208:INFO:          ipywidgets: 8.0.4
2023-07-11 10:00:38,208:INFO:                tqdm: 4.65.0
2023-07-11 10:00:38,208:INFO:               numpy: 1.21.5
2023-07-11 10:00:38,208:INFO:              pandas: 1.5.3
2023-07-11 10:00:38,208:INFO:              jinja2: 3.1.2
2023-07-11 10:00:38,208:INFO:               scipy: 1.10.1
2023-07-11 10:00:38,208:INFO:              joblib: 1.2.0
2023-07-11 10:00:38,208:INFO:             sklearn: 1.2.2
2023-07-11 10:00:38,208:INFO:                pyod: 1.0.9
2023-07-11 10:00:38,208:INFO:            imblearn: 0.10.1
2023-07-11 10:00:38,208:INFO:   category_encoders: 2.6.1
2023-07-11 10:00:38,208:INFO:            lightgbm: 3.3.5
2023-07-11 10:00:38,208:INFO:               numba: 0.57.0
2023-07-11 10:00:38,208:INFO:            requests: 2.29.0
2023-07-11 10:00:38,208:INFO:          matplotlib: 3.7.1
2023-07-11 10:00:38,208:INFO:          scikitplot: 0.3.7
2023-07-11 10:00:38,208:INFO:         yellowbrick: 1.5
2023-07-11 10:00:38,208:INFO:              plotly: 5.9.0
2023-07-11 10:00:38,208:INFO:             kaleido: 0.2.1
2023-07-11 10:00:38,208:INFO:         statsmodels: 0.13.5
2023-07-11 10:00:38,208:INFO:              sktime: 0.17.0
2023-07-11 10:00:38,208:INFO:               tbats: 1.1.3
2023-07-11 10:00:38,208:INFO:            pmdarima: 2.0.3
2023-07-11 10:00:38,208:INFO:              psutil: 5.9.0
2023-07-11 10:00:38,208:INFO:PyCaret optional dependencies:
2023-07-11 10:00:38,208:INFO:                shap: 0.41.0
2023-07-11 10:00:38,208:INFO:           interpret: Not installed
2023-07-11 10:00:38,208:INFO:                umap: Not installed
2023-07-11 10:00:38,208:INFO:    pandas_profiling: 4.3.1
2023-07-11 10:00:38,208:INFO:  explainerdashboard: Not installed
2023-07-11 10:00:38,208:INFO:             autoviz: Not installed
2023-07-11 10:00:38,208:INFO:           fairlearn: Not installed
2023-07-11 10:00:38,208:INFO:             xgboost: 1.7.6
2023-07-11 10:00:38,208:INFO:            catboost: Not installed
2023-07-11 10:00:38,208:INFO:              kmodes: Not installed
2023-07-11 10:00:38,208:INFO:             mlxtend: Not installed
2023-07-11 10:00:38,208:INFO:       statsforecast: Not installed
2023-07-11 10:00:38,208:INFO:        tune_sklearn: Not installed
2023-07-11 10:00:38,208:INFO:                 ray: Not installed
2023-07-11 10:00:38,208:INFO:            hyperopt: Not installed
2023-07-11 10:00:38,208:INFO:              optuna: Not installed
2023-07-11 10:00:38,208:INFO:               skopt: Not installed
2023-07-11 10:00:38,208:INFO:              mlflow: 2.4.2
2023-07-11 10:00:38,208:INFO:              gradio: Not installed
2023-07-11 10:00:38,208:INFO:             fastapi: 0.95.2
2023-07-11 10:00:38,208:INFO:             uvicorn: 0.22.0
2023-07-11 10:00:38,208:INFO:              m2cgen: Not installed
2023-07-11 10:00:38,208:INFO:           evidently: Not installed
2023-07-11 10:00:38,208:INFO:               fugue: Not installed
2023-07-11 10:00:38,208:INFO:           streamlit: 1.23.1
2023-07-11 10:00:38,208:INFO:             prophet: Not installed
2023-07-11 10:00:38,208:INFO:None
2023-07-11 10:00:38,208:INFO:Set up data.
2023-07-11 10:00:38,208:INFO:Set up train/test split.
2023-07-11 10:00:38,215:INFO:Set up index.
2023-07-11 10:00:38,215:INFO:Set up folding strategy.
2023-07-11 10:00:38,215:INFO:Assigning column types.
2023-07-11 10:00:38,215:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-07-11 10:00:38,215:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-07-11 10:00:38,215:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-07-11 10:00:38,215:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 10:00:38,263:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 10:00:38,298:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 10:00:38,298:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 10:00:38,300:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 10:00:38,300:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-07-11 10:00:38,303:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-07-11 10:00:38,306:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 10:00:38,353:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 10:00:38,384:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 10:00:38,384:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 10:00:38,384:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 10:00:38,384:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-07-11 10:00:38,384:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-07-11 10:00:38,384:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 10:00:38,431:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 10:00:38,464:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 10:00:38,464:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 10:00:38,464:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 10:00:38,472:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-07-11 10:00:38,476:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 10:00:38,509:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 10:00:38,552:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 10:00:38,552:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 10:00:38,552:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 10:00:38,552:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-07-11 10:00:38,552:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 10:00:38,598:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 10:00:38,630:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 10:00:38,630:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 10:00:38,630:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 10:00:38,649:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 10:00:38,697:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 10:00:38,721:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 10:00:38,721:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 10:00:38,721:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 10:00:38,721:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-07-11 10:00:38,768:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 10:00:38,810:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 10:00:38,810:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 10:00:38,815:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 10:00:38,855:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 10:00:38,903:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 10:00:38,903:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 10:00:38,903:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 10:00:38,903:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-07-11 10:00:38,951:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 10:00:38,982:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 10:00:38,982:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 10:00:39,046:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 10:00:39,088:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 10:00:39,088:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 10:00:39,088:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-07-11 10:00:39,182:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 10:00:39,182:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 10:00:39,285:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 10:00:39,287:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 10:00:39,288:INFO:Preparing preprocessing pipeline...
2023-07-11 10:00:39,288:INFO:Set up simple imputation.
2023-07-11 10:00:39,290:INFO:Set up encoding of ordinal features.
2023-07-11 10:00:39,291:INFO:Set up encoding of categorical features.
2023-07-11 10:00:58,255:INFO:PyCaret RegressionExperiment
2023-07-11 10:00:58,255:INFO:Logging name: reg-default-name
2023-07-11 10:00:58,255:INFO:ML Usecase: MLUsecase.REGRESSION
2023-07-11 10:00:58,255:INFO:version 3.0.2
2023-07-11 10:00:58,255:INFO:Initializing setup()
2023-07-11 10:00:58,255:INFO:self.USI: 6eac
2023-07-11 10:00:58,255:INFO:self._variable_keys: {'_ml_usecase', 'log_plots_param', 'USI', 'X_train', 'pipeline', 'gpu_param', 'exp_name_log', 'y', 'html_param', 'X', 'data', 'exp_id', 'y_train', 'target_param', 'X_test', 'fold_groups_param', 'n_jobs_param', 'idx', 'fold_generator', 'logging_param', 'memory', 'gpu_n_jobs_param', 'fold_shuffle_param', '_available_plots', 'seed', 'y_test', 'transform_target_param'}
2023-07-11 10:00:58,255:INFO:Checking environment
2023-07-11 10:00:58,255:INFO:python_version: 3.9.13
2023-07-11 10:00:58,255:INFO:python_build: ('main', 'Oct 13 2022 21:23:06')
2023-07-11 10:00:58,255:INFO:machine: AMD64
2023-07-11 10:00:58,255:INFO:platform: Windows-10-10.0.22621-SP0
2023-07-11 10:00:58,255:INFO:Memory: svmem(total=16893358080, available=2947571712, percent=82.6, used=13945786368, free=2947571712)
2023-07-11 10:00:58,255:INFO:Physical Core: 8
2023-07-11 10:00:58,255:INFO:Logical Core: 16
2023-07-11 10:00:58,255:INFO:Checking libraries
2023-07-11 10:00:58,255:INFO:System:
2023-07-11 10:00:58,255:INFO:    python: 3.9.13 (main, Oct 13 2022, 21:23:06) [MSC v.1916 64 bit (AMD64)]
2023-07-11 10:00:58,255:INFO:executable: C:\Users\didit\anaconda3\python.exe
2023-07-11 10:00:58,255:INFO:   machine: Windows-10-10.0.22621-SP0
2023-07-11 10:00:58,255:INFO:PyCaret required dependencies:
2023-07-11 10:00:58,255:INFO:                 pip: 23.0.1
2023-07-11 10:00:58,255:INFO:          setuptools: 67.8.0
2023-07-11 10:00:58,255:INFO:             pycaret: 3.0.2
2023-07-11 10:00:58,255:INFO:             IPython: 8.12.0
2023-07-11 10:00:58,255:INFO:          ipywidgets: 8.0.4
2023-07-11 10:00:58,255:INFO:                tqdm: 4.65.0
2023-07-11 10:00:58,255:INFO:               numpy: 1.21.5
2023-07-11 10:00:58,255:INFO:              pandas: 1.5.3
2023-07-11 10:00:58,255:INFO:              jinja2: 3.1.2
2023-07-11 10:00:58,255:INFO:               scipy: 1.10.1
2023-07-11 10:00:58,255:INFO:              joblib: 1.2.0
2023-07-11 10:00:58,255:INFO:             sklearn: 1.2.2
2023-07-11 10:00:58,255:INFO:                pyod: 1.0.9
2023-07-11 10:00:58,255:INFO:            imblearn: 0.10.1
2023-07-11 10:00:58,255:INFO:   category_encoders: 2.6.1
2023-07-11 10:00:58,255:INFO:            lightgbm: 3.3.5
2023-07-11 10:00:58,255:INFO:               numba: 0.57.0
2023-07-11 10:00:58,255:INFO:            requests: 2.29.0
2023-07-11 10:00:58,255:INFO:          matplotlib: 3.7.1
2023-07-11 10:00:58,255:INFO:          scikitplot: 0.3.7
2023-07-11 10:00:58,255:INFO:         yellowbrick: 1.5
2023-07-11 10:00:58,255:INFO:              plotly: 5.9.0
2023-07-11 10:00:58,255:INFO:             kaleido: 0.2.1
2023-07-11 10:00:58,255:INFO:         statsmodels: 0.13.5
2023-07-11 10:00:58,255:INFO:              sktime: 0.17.0
2023-07-11 10:00:58,255:INFO:               tbats: 1.1.3
2023-07-11 10:00:58,255:INFO:            pmdarima: 2.0.3
2023-07-11 10:00:58,255:INFO:              psutil: 5.9.0
2023-07-11 10:00:58,255:INFO:PyCaret optional dependencies:
2023-07-11 10:00:58,255:INFO:                shap: 0.41.0
2023-07-11 10:00:58,255:INFO:           interpret: Not installed
2023-07-11 10:00:58,255:INFO:                umap: Not installed
2023-07-11 10:00:58,255:INFO:    pandas_profiling: 4.3.1
2023-07-11 10:00:58,255:INFO:  explainerdashboard: Not installed
2023-07-11 10:00:58,255:INFO:             autoviz: Not installed
2023-07-11 10:00:58,255:INFO:           fairlearn: Not installed
2023-07-11 10:00:58,255:INFO:             xgboost: 1.7.6
2023-07-11 10:00:58,255:INFO:            catboost: Not installed
2023-07-11 10:00:58,255:INFO:              kmodes: Not installed
2023-07-11 10:00:58,255:INFO:             mlxtend: Not installed
2023-07-11 10:00:58,255:INFO:       statsforecast: Not installed
2023-07-11 10:00:58,255:INFO:        tune_sklearn: Not installed
2023-07-11 10:00:58,255:INFO:                 ray: Not installed
2023-07-11 10:00:58,255:INFO:            hyperopt: Not installed
2023-07-11 10:00:58,255:INFO:              optuna: Not installed
2023-07-11 10:00:58,255:INFO:               skopt: Not installed
2023-07-11 10:00:58,255:INFO:              mlflow: 2.4.2
2023-07-11 10:00:58,255:INFO:              gradio: Not installed
2023-07-11 10:00:58,255:INFO:             fastapi: 0.95.2
2023-07-11 10:00:58,255:INFO:             uvicorn: 0.22.0
2023-07-11 10:00:58,255:INFO:              m2cgen: Not installed
2023-07-11 10:00:58,255:INFO:           evidently: Not installed
2023-07-11 10:00:58,255:INFO:               fugue: Not installed
2023-07-11 10:00:58,255:INFO:           streamlit: 1.23.1
2023-07-11 10:00:58,255:INFO:             prophet: Not installed
2023-07-11 10:00:58,255:INFO:None
2023-07-11 10:00:58,255:INFO:Set up data.
2023-07-11 10:01:10,879:INFO:PyCaret RegressionExperiment
2023-07-11 10:01:10,879:INFO:Logging name: reg-default-name
2023-07-11 10:01:10,879:INFO:ML Usecase: MLUsecase.REGRESSION
2023-07-11 10:01:10,879:INFO:version 3.0.2
2023-07-11 10:01:10,879:INFO:Initializing setup()
2023-07-11 10:01:10,880:INFO:self.USI: 9eb4
2023-07-11 10:01:10,880:INFO:self._variable_keys: {'_ml_usecase', 'log_plots_param', 'USI', 'X_train', 'pipeline', 'gpu_param', 'exp_name_log', 'y', 'html_param', 'X', 'data', 'exp_id', 'y_train', 'target_param', 'X_test', 'fold_groups_param', 'n_jobs_param', 'idx', 'fold_generator', 'logging_param', 'memory', 'gpu_n_jobs_param', 'fold_shuffle_param', '_available_plots', 'seed', 'y_test', 'transform_target_param'}
2023-07-11 10:01:10,880:INFO:Checking environment
2023-07-11 10:01:10,881:INFO:python_version: 3.9.13
2023-07-11 10:01:10,881:INFO:python_build: ('main', 'Oct 13 2022 21:23:06')
2023-07-11 10:01:10,881:INFO:machine: AMD64
2023-07-11 10:01:10,881:INFO:platform: Windows-10-10.0.22621-SP0
2023-07-11 10:01:10,881:INFO:Memory: svmem(total=16893358080, available=3109306368, percent=81.6, used=13784051712, free=3109306368)
2023-07-11 10:01:10,881:INFO:Physical Core: 8
2023-07-11 10:01:10,881:INFO:Logical Core: 16
2023-07-11 10:01:10,881:INFO:Checking libraries
2023-07-11 10:01:10,882:INFO:System:
2023-07-11 10:01:10,882:INFO:    python: 3.9.13 (main, Oct 13 2022, 21:23:06) [MSC v.1916 64 bit (AMD64)]
2023-07-11 10:01:10,882:INFO:executable: C:\Users\didit\anaconda3\python.exe
2023-07-11 10:01:10,882:INFO:   machine: Windows-10-10.0.22621-SP0
2023-07-11 10:01:10,882:INFO:PyCaret required dependencies:
2023-07-11 10:01:10,882:INFO:                 pip: 23.0.1
2023-07-11 10:01:10,882:INFO:          setuptools: 67.8.0
2023-07-11 10:01:10,883:INFO:             pycaret: 3.0.2
2023-07-11 10:01:10,883:INFO:             IPython: 8.12.0
2023-07-11 10:01:10,883:INFO:          ipywidgets: 8.0.4
2023-07-11 10:01:10,883:INFO:                tqdm: 4.65.0
2023-07-11 10:01:10,883:INFO:               numpy: 1.21.5
2023-07-11 10:01:10,883:INFO:              pandas: 1.5.3
2023-07-11 10:01:10,883:INFO:              jinja2: 3.1.2
2023-07-11 10:01:10,883:INFO:               scipy: 1.10.1
2023-07-11 10:01:10,883:INFO:              joblib: 1.2.0
2023-07-11 10:01:10,884:INFO:             sklearn: 1.2.2
2023-07-11 10:01:10,884:INFO:                pyod: 1.0.9
2023-07-11 10:01:10,884:INFO:            imblearn: 0.10.1
2023-07-11 10:01:10,884:INFO:   category_encoders: 2.6.1
2023-07-11 10:01:10,884:INFO:            lightgbm: 3.3.5
2023-07-11 10:01:10,884:INFO:               numba: 0.57.0
2023-07-11 10:01:10,884:INFO:            requests: 2.29.0
2023-07-11 10:01:10,885:INFO:          matplotlib: 3.7.1
2023-07-11 10:01:10,885:INFO:          scikitplot: 0.3.7
2023-07-11 10:01:10,885:INFO:         yellowbrick: 1.5
2023-07-11 10:01:10,885:INFO:              plotly: 5.9.0
2023-07-11 10:01:10,885:INFO:             kaleido: 0.2.1
2023-07-11 10:01:10,885:INFO:         statsmodels: 0.13.5
2023-07-11 10:01:10,885:INFO:              sktime: 0.17.0
2023-07-11 10:01:10,886:INFO:               tbats: 1.1.3
2023-07-11 10:01:10,886:INFO:            pmdarima: 2.0.3
2023-07-11 10:01:10,886:INFO:              psutil: 5.9.0
2023-07-11 10:01:10,886:INFO:PyCaret optional dependencies:
2023-07-11 10:01:10,886:INFO:                shap: 0.41.0
2023-07-11 10:01:10,886:INFO:           interpret: Not installed
2023-07-11 10:01:10,886:INFO:                umap: Not installed
2023-07-11 10:01:10,886:INFO:    pandas_profiling: 4.3.1
2023-07-11 10:01:10,886:INFO:  explainerdashboard: Not installed
2023-07-11 10:01:10,887:INFO:             autoviz: Not installed
2023-07-11 10:01:10,887:INFO:           fairlearn: Not installed
2023-07-11 10:01:10,887:INFO:             xgboost: 1.7.6
2023-07-11 10:01:10,887:INFO:            catboost: Not installed
2023-07-11 10:01:10,887:INFO:              kmodes: Not installed
2023-07-11 10:01:10,887:INFO:             mlxtend: Not installed
2023-07-11 10:01:10,887:INFO:       statsforecast: Not installed
2023-07-11 10:01:10,888:INFO:        tune_sklearn: Not installed
2023-07-11 10:01:10,888:INFO:                 ray: Not installed
2023-07-11 10:01:10,888:INFO:            hyperopt: Not installed
2023-07-11 10:01:10,888:INFO:              optuna: Not installed
2023-07-11 10:01:10,888:INFO:               skopt: Not installed
2023-07-11 10:01:10,888:INFO:              mlflow: 2.4.2
2023-07-11 10:01:10,888:INFO:              gradio: Not installed
2023-07-11 10:01:10,888:INFO:             fastapi: 0.95.2
2023-07-11 10:01:10,888:INFO:             uvicorn: 0.22.0
2023-07-11 10:01:10,889:INFO:              m2cgen: Not installed
2023-07-11 10:01:10,889:INFO:           evidently: Not installed
2023-07-11 10:01:10,889:INFO:               fugue: Not installed
2023-07-11 10:01:10,889:INFO:           streamlit: 1.23.1
2023-07-11 10:01:10,889:INFO:             prophet: Not installed
2023-07-11 10:01:10,889:INFO:None
2023-07-11 10:01:10,889:INFO:Set up data.
2023-07-11 10:01:10,890:INFO:Set up train/test split.
2023-07-11 10:01:10,890:INFO:Set up index.
2023-07-11 10:01:10,890:INFO:Set up folding strategy.
2023-07-11 10:01:10,890:INFO:Assigning column types.
2023-07-11 10:01:10,905:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-07-11 10:01:10,905:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-07-11 10:01:10,905:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-07-11 10:01:10,905:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 10:01:10,952:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 10:01:10,984:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 10:01:10,984:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 10:01:10,984:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 10:01:10,984:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-07-11 10:01:10,984:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-07-11 10:01:11,000:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 10:01:11,031:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 10:01:11,084:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 10:01:11,084:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 10:01:11,086:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 10:01:11,086:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-07-11 10:01:11,088:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-07-11 10:01:11,088:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 10:01:11,120:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 10:01:11,167:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 10:01:11,167:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 10:01:11,167:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 10:01:11,167:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-07-11 10:01:11,175:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 10:01:11,214:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 10:01:11,255:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 10:01:11,255:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 10:01:11,255:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 10:01:11,255:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-07-11 10:01:11,255:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 10:01:11,322:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 10:01:11,353:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 10:01:11,353:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 10:01:11,353:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 10:01:11,375:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 10:01:11,400:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 10:01:11,448:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 10:01:11,449:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 10:01:11,450:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 10:01:11,451:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-07-11 10:01:11,496:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 10:01:11,522:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 10:01:11,522:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 10:01:11,522:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 10:01:11,576:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 10:01:11,616:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 10:01:11,616:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 10:01:11,616:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 10:01:11,616:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-07-11 10:01:11,667:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 10:01:11,689:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 10:01:11,704:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 10:01:11,751:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 10:01:11,783:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 10:01:11,783:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 10:01:11,783:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-07-11 10:01:11,877:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 10:01:11,879:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 10:01:11,952:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 10:01:11,952:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 10:01:11,952:INFO:Preparing preprocessing pipeline...
2023-07-11 10:01:11,952:INFO:Set up simple imputation.
2023-07-11 10:01:11,976:INFO:Set up encoding of categorical features.
2023-07-11 10:01:21,152:INFO:PyCaret RegressionExperiment
2023-07-11 10:01:21,168:INFO:Logging name: reg-default-name
2023-07-11 10:01:21,168:INFO:ML Usecase: MLUsecase.REGRESSION
2023-07-11 10:01:21,168:INFO:version 3.0.2
2023-07-11 10:01:21,168:INFO:Initializing setup()
2023-07-11 10:01:21,168:INFO:self.USI: b0aa
2023-07-11 10:01:21,168:INFO:self._variable_keys: {'_ml_usecase', 'log_plots_param', 'USI', 'X_train', 'pipeline', 'gpu_param', 'exp_name_log', 'y', 'html_param', 'X', 'data', 'exp_id', 'y_train', 'target_param', 'X_test', 'fold_groups_param', 'n_jobs_param', 'idx', 'fold_generator', 'logging_param', 'memory', 'gpu_n_jobs_param', 'fold_shuffle_param', '_available_plots', 'seed', 'y_test', 'transform_target_param'}
2023-07-11 10:01:21,168:INFO:Checking environment
2023-07-11 10:01:21,168:INFO:python_version: 3.9.13
2023-07-11 10:01:21,168:INFO:python_build: ('main', 'Oct 13 2022 21:23:06')
2023-07-11 10:01:21,168:INFO:machine: AMD64
2023-07-11 10:01:21,168:INFO:platform: Windows-10-10.0.22621-SP0
2023-07-11 10:01:21,168:INFO:Memory: svmem(total=16893358080, available=3096715264, percent=81.7, used=13796642816, free=3096715264)
2023-07-11 10:01:21,168:INFO:Physical Core: 8
2023-07-11 10:01:21,168:INFO:Logical Core: 16
2023-07-11 10:01:21,168:INFO:Checking libraries
2023-07-11 10:01:21,168:INFO:System:
2023-07-11 10:01:21,168:INFO:    python: 3.9.13 (main, Oct 13 2022, 21:23:06) [MSC v.1916 64 bit (AMD64)]
2023-07-11 10:01:21,168:INFO:executable: C:\Users\didit\anaconda3\python.exe
2023-07-11 10:01:21,168:INFO:   machine: Windows-10-10.0.22621-SP0
2023-07-11 10:01:21,168:INFO:PyCaret required dependencies:
2023-07-11 10:01:21,168:INFO:                 pip: 23.0.1
2023-07-11 10:01:21,168:INFO:          setuptools: 67.8.0
2023-07-11 10:01:21,168:INFO:             pycaret: 3.0.2
2023-07-11 10:01:21,168:INFO:             IPython: 8.12.0
2023-07-11 10:01:21,168:INFO:          ipywidgets: 8.0.4
2023-07-11 10:01:21,168:INFO:                tqdm: 4.65.0
2023-07-11 10:01:21,168:INFO:               numpy: 1.21.5
2023-07-11 10:01:21,168:INFO:              pandas: 1.5.3
2023-07-11 10:01:21,168:INFO:              jinja2: 3.1.2
2023-07-11 10:01:21,168:INFO:               scipy: 1.10.1
2023-07-11 10:01:21,168:INFO:              joblib: 1.2.0
2023-07-11 10:01:21,168:INFO:             sklearn: 1.2.2
2023-07-11 10:01:21,168:INFO:                pyod: 1.0.9
2023-07-11 10:01:21,168:INFO:            imblearn: 0.10.1
2023-07-11 10:01:21,168:INFO:   category_encoders: 2.6.1
2023-07-11 10:01:21,168:INFO:            lightgbm: 3.3.5
2023-07-11 10:01:21,168:INFO:               numba: 0.57.0
2023-07-11 10:01:21,168:INFO:            requests: 2.29.0
2023-07-11 10:01:21,168:INFO:          matplotlib: 3.7.1
2023-07-11 10:01:21,168:INFO:          scikitplot: 0.3.7
2023-07-11 10:01:21,168:INFO:         yellowbrick: 1.5
2023-07-11 10:01:21,168:INFO:              plotly: 5.9.0
2023-07-11 10:01:21,168:INFO:             kaleido: 0.2.1
2023-07-11 10:01:21,168:INFO:         statsmodels: 0.13.5
2023-07-11 10:01:21,168:INFO:              sktime: 0.17.0
2023-07-11 10:01:21,168:INFO:               tbats: 1.1.3
2023-07-11 10:01:21,168:INFO:            pmdarima: 2.0.3
2023-07-11 10:01:21,168:INFO:              psutil: 5.9.0
2023-07-11 10:01:21,168:INFO:PyCaret optional dependencies:
2023-07-11 10:01:21,168:INFO:                shap: 0.41.0
2023-07-11 10:01:21,168:INFO:           interpret: Not installed
2023-07-11 10:01:21,168:INFO:                umap: Not installed
2023-07-11 10:01:21,168:INFO:    pandas_profiling: 4.3.1
2023-07-11 10:01:21,168:INFO:  explainerdashboard: Not installed
2023-07-11 10:01:21,168:INFO:             autoviz: Not installed
2023-07-11 10:01:21,168:INFO:           fairlearn: Not installed
2023-07-11 10:01:21,168:INFO:             xgboost: 1.7.6
2023-07-11 10:01:21,168:INFO:            catboost: Not installed
2023-07-11 10:01:21,168:INFO:              kmodes: Not installed
2023-07-11 10:01:21,168:INFO:             mlxtend: Not installed
2023-07-11 10:01:21,168:INFO:       statsforecast: Not installed
2023-07-11 10:01:21,168:INFO:        tune_sklearn: Not installed
2023-07-11 10:01:21,168:INFO:                 ray: Not installed
2023-07-11 10:01:21,168:INFO:            hyperopt: Not installed
2023-07-11 10:01:21,168:INFO:              optuna: Not installed
2023-07-11 10:01:21,168:INFO:               skopt: Not installed
2023-07-11 10:01:21,168:INFO:              mlflow: 2.4.2
2023-07-11 10:01:21,168:INFO:              gradio: Not installed
2023-07-11 10:01:21,168:INFO:             fastapi: 0.95.2
2023-07-11 10:01:21,168:INFO:             uvicorn: 0.22.0
2023-07-11 10:01:21,168:INFO:              m2cgen: Not installed
2023-07-11 10:01:21,168:INFO:           evidently: Not installed
2023-07-11 10:01:21,168:INFO:               fugue: Not installed
2023-07-11 10:01:21,168:INFO:           streamlit: 1.23.1
2023-07-11 10:01:21,168:INFO:             prophet: Not installed
2023-07-11 10:01:21,168:INFO:None
2023-07-11 10:01:21,168:INFO:Set up data.
2023-07-11 10:02:30,657:INFO:PyCaret RegressionExperiment
2023-07-11 10:02:30,657:INFO:Logging name: reg-default-name
2023-07-11 10:02:30,657:INFO:ML Usecase: MLUsecase.REGRESSION
2023-07-11 10:02:30,658:INFO:version 3.0.2
2023-07-11 10:02:30,658:INFO:Initializing setup()
2023-07-11 10:02:30,658:INFO:self.USI: d036
2023-07-11 10:02:30,658:INFO:self._variable_keys: {'_ml_usecase', 'log_plots_param', 'USI', 'X_train', 'pipeline', 'gpu_param', 'exp_name_log', 'y', 'html_param', 'X', 'data', 'exp_id', 'y_train', 'target_param', 'X_test', 'fold_groups_param', 'n_jobs_param', 'idx', 'fold_generator', 'logging_param', 'memory', 'gpu_n_jobs_param', 'fold_shuffle_param', '_available_plots', 'seed', 'y_test', 'transform_target_param'}
2023-07-11 10:02:30,658:INFO:Checking environment
2023-07-11 10:02:30,659:INFO:python_version: 3.9.13
2023-07-11 10:02:30,659:INFO:python_build: ('main', 'Oct 13 2022 21:23:06')
2023-07-11 10:02:30,659:INFO:machine: AMD64
2023-07-11 10:02:30,659:INFO:platform: Windows-10-10.0.22621-SP0
2023-07-11 10:02:30,659:INFO:Memory: svmem(total=16893358080, available=2971000832, percent=82.4, used=13922357248, free=2971000832)
2023-07-11 10:02:30,660:INFO:Physical Core: 8
2023-07-11 10:02:30,660:INFO:Logical Core: 16
2023-07-11 10:02:30,660:INFO:Checking libraries
2023-07-11 10:02:30,660:INFO:System:
2023-07-11 10:02:30,660:INFO:    python: 3.9.13 (main, Oct 13 2022, 21:23:06) [MSC v.1916 64 bit (AMD64)]
2023-07-11 10:02:30,660:INFO:executable: C:\Users\didit\anaconda3\python.exe
2023-07-11 10:02:30,661:INFO:   machine: Windows-10-10.0.22621-SP0
2023-07-11 10:02:30,661:INFO:PyCaret required dependencies:
2023-07-11 10:02:30,661:INFO:                 pip: 23.0.1
2023-07-11 10:02:30,661:INFO:          setuptools: 67.8.0
2023-07-11 10:02:30,662:INFO:             pycaret: 3.0.2
2023-07-11 10:02:30,662:INFO:             IPython: 8.12.0
2023-07-11 10:02:30,662:INFO:          ipywidgets: 8.0.4
2023-07-11 10:02:30,662:INFO:                tqdm: 4.65.0
2023-07-11 10:02:30,662:INFO:               numpy: 1.21.5
2023-07-11 10:02:30,662:INFO:              pandas: 1.5.3
2023-07-11 10:02:30,663:INFO:              jinja2: 3.1.2
2023-07-11 10:02:30,663:INFO:               scipy: 1.10.1
2023-07-11 10:02:30,663:INFO:              joblib: 1.2.0
2023-07-11 10:02:30,663:INFO:             sklearn: 1.2.2
2023-07-11 10:02:30,663:INFO:                pyod: 1.0.9
2023-07-11 10:02:30,663:INFO:            imblearn: 0.10.1
2023-07-11 10:02:30,664:INFO:   category_encoders: 2.6.1
2023-07-11 10:02:30,664:INFO:            lightgbm: 3.3.5
2023-07-11 10:02:30,664:INFO:               numba: 0.57.0
2023-07-11 10:02:30,664:INFO:            requests: 2.29.0
2023-07-11 10:02:30,664:INFO:          matplotlib: 3.7.1
2023-07-11 10:02:30,664:INFO:          scikitplot: 0.3.7
2023-07-11 10:02:30,664:INFO:         yellowbrick: 1.5
2023-07-11 10:02:30,664:INFO:              plotly: 5.9.0
2023-07-11 10:02:30,664:INFO:             kaleido: 0.2.1
2023-07-11 10:02:30,665:INFO:         statsmodels: 0.13.5
2023-07-11 10:02:30,665:INFO:              sktime: 0.17.0
2023-07-11 10:02:30,665:INFO:               tbats: 1.1.3
2023-07-11 10:02:30,665:INFO:            pmdarima: 2.0.3
2023-07-11 10:02:30,665:INFO:              psutil: 5.9.0
2023-07-11 10:02:30,665:INFO:PyCaret optional dependencies:
2023-07-11 10:02:30,665:INFO:                shap: 0.41.0
2023-07-11 10:02:30,665:INFO:           interpret: Not installed
2023-07-11 10:02:30,666:INFO:                umap: Not installed
2023-07-11 10:02:30,666:INFO:    pandas_profiling: 4.3.1
2023-07-11 10:02:30,666:INFO:  explainerdashboard: Not installed
2023-07-11 10:02:30,666:INFO:             autoviz: Not installed
2023-07-11 10:02:30,666:INFO:           fairlearn: Not installed
2023-07-11 10:02:30,666:INFO:             xgboost: 1.7.6
2023-07-11 10:02:30,666:INFO:            catboost: Not installed
2023-07-11 10:02:30,667:INFO:              kmodes: Not installed
2023-07-11 10:02:30,667:INFO:             mlxtend: Not installed
2023-07-11 10:02:30,667:INFO:       statsforecast: Not installed
2023-07-11 10:02:30,667:INFO:        tune_sklearn: Not installed
2023-07-11 10:02:30,667:INFO:                 ray: Not installed
2023-07-11 10:02:30,667:INFO:            hyperopt: Not installed
2023-07-11 10:02:30,667:INFO:              optuna: Not installed
2023-07-11 10:02:30,667:INFO:               skopt: Not installed
2023-07-11 10:02:30,667:INFO:              mlflow: 2.4.2
2023-07-11 10:02:30,668:INFO:              gradio: Not installed
2023-07-11 10:02:30,668:INFO:             fastapi: 0.95.2
2023-07-11 10:02:30,668:INFO:             uvicorn: 0.22.0
2023-07-11 10:02:30,668:INFO:              m2cgen: Not installed
2023-07-11 10:02:30,668:INFO:           evidently: Not installed
2023-07-11 10:02:30,668:INFO:               fugue: Not installed
2023-07-11 10:02:30,668:INFO:           streamlit: 1.23.1
2023-07-11 10:02:30,668:INFO:             prophet: Not installed
2023-07-11 10:02:30,668:INFO:None
2023-07-11 10:02:30,669:INFO:Set up data.
2023-07-11 10:02:30,674:INFO:Set up train/test split.
2023-07-11 10:02:30,678:INFO:Set up index.
2023-07-11 10:02:30,678:INFO:Set up folding strategy.
2023-07-11 10:02:30,678:INFO:Assigning column types.
2023-07-11 10:02:30,680:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-07-11 10:02:30,680:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-07-11 10:02:30,685:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-07-11 10:02:30,688:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 10:02:30,722:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 10:02:30,753:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 10:02:30,768:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 10:02:30,768:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 10:02:30,768:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-07-11 10:02:30,768:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-07-11 10:02:30,768:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 10:02:30,816:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 10:02:30,853:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 10:02:30,854:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 10:02:30,856:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 10:02:30,856:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-07-11 10:02:30,857:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-07-11 10:02:30,857:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 10:02:30,907:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 10:02:30,938:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 10:02:30,938:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 10:02:30,954:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 10:02:30,954:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-07-11 10:02:30,954:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 10:02:31,001:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 10:02:31,033:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 10:02:31,033:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 10:02:31,033:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 10:02:31,033:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-07-11 10:02:31,058:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 10:02:31,091:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 10:02:31,122:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 10:02:31,122:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 10:02:31,122:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 10:02:31,138:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 10:02:31,177:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 10:02:31,209:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 10:02:31,209:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 10:02:31,209:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 10:02:31,209:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-07-11 10:02:31,275:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 10:02:31,307:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 10:02:31,307:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 10:02:31,309:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 10:02:31,355:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 10:02:31,386:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 10:02:31,386:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 10:02:31,400:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 10:02:31,400:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-07-11 10:02:31,456:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 10:02:31,490:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 10:02:31,504:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 10:02:31,553:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 10:02:31,585:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 10:02:31,585:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 10:02:31,585:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-07-11 10:02:31,678:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 10:02:31,681:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 10:02:31,771:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 10:02:31,771:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 10:02:31,786:INFO:Preparing preprocessing pipeline...
2023-07-11 10:02:31,786:INFO:Set up simple imputation.
2023-07-11 10:02:31,786:INFO:Set up encoding of categorical features.
2023-07-11 10:02:41,801:INFO:PyCaret RegressionExperiment
2023-07-11 10:02:41,801:INFO:Logging name: reg-default-name
2023-07-11 10:02:41,801:INFO:ML Usecase: MLUsecase.REGRESSION
2023-07-11 10:02:41,801:INFO:version 3.0.2
2023-07-11 10:02:41,801:INFO:Initializing setup()
2023-07-11 10:02:41,801:INFO:self.USI: c83b
2023-07-11 10:02:41,801:INFO:self._variable_keys: {'_ml_usecase', 'log_plots_param', 'USI', 'X_train', 'pipeline', 'gpu_param', 'exp_name_log', 'y', 'html_param', 'X', 'data', 'exp_id', 'y_train', 'target_param', 'X_test', 'fold_groups_param', 'n_jobs_param', 'idx', 'fold_generator', 'logging_param', 'memory', 'gpu_n_jobs_param', 'fold_shuffle_param', '_available_plots', 'seed', 'y_test', 'transform_target_param'}
2023-07-11 10:02:41,801:INFO:Checking environment
2023-07-11 10:02:41,801:INFO:python_version: 3.9.13
2023-07-11 10:02:41,801:INFO:python_build: ('main', 'Oct 13 2022 21:23:06')
2023-07-11 10:02:41,801:INFO:machine: AMD64
2023-07-11 10:02:41,801:INFO:platform: Windows-10-10.0.22621-SP0
2023-07-11 10:02:41,801:INFO:Memory: svmem(total=16893358080, available=3142467584, percent=81.4, used=13750890496, free=3142467584)
2023-07-11 10:02:41,801:INFO:Physical Core: 8
2023-07-11 10:02:41,801:INFO:Logical Core: 16
2023-07-11 10:02:41,801:INFO:Checking libraries
2023-07-11 10:02:41,801:INFO:System:
2023-07-11 10:02:41,801:INFO:    python: 3.9.13 (main, Oct 13 2022, 21:23:06) [MSC v.1916 64 bit (AMD64)]
2023-07-11 10:02:41,801:INFO:executable: C:\Users\didit\anaconda3\python.exe
2023-07-11 10:02:41,801:INFO:   machine: Windows-10-10.0.22621-SP0
2023-07-11 10:02:41,801:INFO:PyCaret required dependencies:
2023-07-11 10:02:41,801:INFO:                 pip: 23.0.1
2023-07-11 10:02:41,801:INFO:          setuptools: 67.8.0
2023-07-11 10:02:41,801:INFO:             pycaret: 3.0.2
2023-07-11 10:02:41,801:INFO:             IPython: 8.12.0
2023-07-11 10:02:41,801:INFO:          ipywidgets: 8.0.4
2023-07-11 10:02:41,801:INFO:                tqdm: 4.65.0
2023-07-11 10:02:41,801:INFO:               numpy: 1.21.5
2023-07-11 10:02:41,801:INFO:              pandas: 1.5.3
2023-07-11 10:02:41,801:INFO:              jinja2: 3.1.2
2023-07-11 10:02:41,801:INFO:               scipy: 1.10.1
2023-07-11 10:02:41,816:INFO:              joblib: 1.2.0
2023-07-11 10:02:41,816:INFO:             sklearn: 1.2.2
2023-07-11 10:02:41,816:INFO:                pyod: 1.0.9
2023-07-11 10:02:41,816:INFO:            imblearn: 0.10.1
2023-07-11 10:02:41,816:INFO:   category_encoders: 2.6.1
2023-07-11 10:02:41,816:INFO:            lightgbm: 3.3.5
2023-07-11 10:02:41,816:INFO:               numba: 0.57.0
2023-07-11 10:02:41,816:INFO:            requests: 2.29.0
2023-07-11 10:02:41,816:INFO:          matplotlib: 3.7.1
2023-07-11 10:02:41,818:INFO:          scikitplot: 0.3.7
2023-07-11 10:02:41,818:INFO:         yellowbrick: 1.5
2023-07-11 10:02:41,818:INFO:              plotly: 5.9.0
2023-07-11 10:02:41,818:INFO:             kaleido: 0.2.1
2023-07-11 10:02:41,818:INFO:         statsmodels: 0.13.5
2023-07-11 10:02:41,818:INFO:              sktime: 0.17.0
2023-07-11 10:02:41,818:INFO:               tbats: 1.1.3
2023-07-11 10:02:41,818:INFO:            pmdarima: 2.0.3
2023-07-11 10:02:41,818:INFO:              psutil: 5.9.0
2023-07-11 10:02:41,818:INFO:PyCaret optional dependencies:
2023-07-11 10:02:41,818:INFO:                shap: 0.41.0
2023-07-11 10:02:41,818:INFO:           interpret: Not installed
2023-07-11 10:02:41,818:INFO:                umap: Not installed
2023-07-11 10:02:41,818:INFO:    pandas_profiling: 4.3.1
2023-07-11 10:02:41,818:INFO:  explainerdashboard: Not installed
2023-07-11 10:02:41,818:INFO:             autoviz: Not installed
2023-07-11 10:02:41,818:INFO:           fairlearn: Not installed
2023-07-11 10:02:41,818:INFO:             xgboost: 1.7.6
2023-07-11 10:02:41,818:INFO:            catboost: Not installed
2023-07-11 10:02:41,818:INFO:              kmodes: Not installed
2023-07-11 10:02:41,818:INFO:             mlxtend: Not installed
2023-07-11 10:02:41,818:INFO:       statsforecast: Not installed
2023-07-11 10:02:41,818:INFO:        tune_sklearn: Not installed
2023-07-11 10:02:41,818:INFO:                 ray: Not installed
2023-07-11 10:02:41,818:INFO:            hyperopt: Not installed
2023-07-11 10:02:41,818:INFO:              optuna: Not installed
2023-07-11 10:02:41,818:INFO:               skopt: Not installed
2023-07-11 10:02:41,818:INFO:              mlflow: 2.4.2
2023-07-11 10:02:41,818:INFO:              gradio: Not installed
2023-07-11 10:02:41,818:INFO:             fastapi: 0.95.2
2023-07-11 10:02:41,818:INFO:             uvicorn: 0.22.0
2023-07-11 10:02:41,818:INFO:              m2cgen: Not installed
2023-07-11 10:02:41,818:INFO:           evidently: Not installed
2023-07-11 10:02:41,818:INFO:               fugue: Not installed
2023-07-11 10:02:41,818:INFO:           streamlit: 1.23.1
2023-07-11 10:02:41,818:INFO:             prophet: Not installed
2023-07-11 10:02:41,818:INFO:None
2023-07-11 10:02:41,818:INFO:Set up data.
2023-07-11 10:02:41,818:INFO:Set up train/test split.
2023-07-11 10:02:41,818:INFO:Set up index.
2023-07-11 10:02:41,832:INFO:Set up folding strategy.
2023-07-11 10:02:41,832:INFO:Assigning column types.
2023-07-11 10:02:41,832:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-07-11 10:02:41,832:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-07-11 10:02:41,832:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-07-11 10:02:41,832:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 10:02:41,888:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 10:02:41,923:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 10:02:41,923:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 10:02:41,923:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 10:02:41,923:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-07-11 10:02:41,938:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-07-11 10:02:41,938:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 10:02:41,985:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 10:02:42,018:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 10:02:42,018:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 10:02:42,018:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 10:02:42,018:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-07-11 10:02:42,033:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-07-11 10:02:42,033:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 10:02:42,084:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 10:02:42,123:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 10:02:42,123:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 10:02:42,123:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 10:02:42,123:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-07-11 10:02:42,123:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 10:02:42,170:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 10:02:42,201:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 10:02:42,201:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 10:02:42,201:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 10:02:42,201:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-07-11 10:02:42,217:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 10:02:42,260:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 10:02:42,291:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 10:02:42,291:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 10:02:42,291:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 10:02:42,291:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 10:02:42,338:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 10:02:42,370:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 10:02:42,370:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 10:02:42,370:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 10:02:42,370:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-07-11 10:02:42,418:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 10:02:42,458:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 10:02:42,458:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 10:02:42,458:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 10:02:42,508:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 10:02:42,539:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 10:02:42,539:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 10:02:42,555:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 10:02:42,555:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-07-11 10:02:42,602:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 10:02:42,618:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 10:02:42,633:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 10:02:42,688:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 10:02:42,718:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 10:02:42,722:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 10:02:42,722:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-07-11 10:02:42,800:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 10:02:42,800:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 10:02:42,895:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 10:02:42,897:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 10:02:42,898:INFO:Preparing preprocessing pipeline...
2023-07-11 10:02:42,898:INFO:Set up simple imputation.
2023-07-11 10:02:42,902:INFO:Set up encoding of ordinal features.
2023-07-11 10:02:42,903:INFO:Set up encoding of categorical features.
2023-07-11 10:02:49,483:INFO:PyCaret RegressionExperiment
2023-07-11 10:02:49,483:INFO:Logging name: reg-default-name
2023-07-11 10:02:49,484:INFO:ML Usecase: MLUsecase.REGRESSION
2023-07-11 10:02:49,484:INFO:version 3.0.2
2023-07-11 10:02:49,484:INFO:Initializing setup()
2023-07-11 10:02:49,484:INFO:self.USI: 40a1
2023-07-11 10:02:49,484:INFO:self._variable_keys: {'_ml_usecase', 'log_plots_param', 'USI', 'X_train', 'pipeline', 'gpu_param', 'exp_name_log', 'y', 'html_param', 'X', 'data', 'exp_id', 'y_train', 'target_param', 'X_test', 'fold_groups_param', 'n_jobs_param', 'idx', 'fold_generator', 'logging_param', 'memory', 'gpu_n_jobs_param', 'fold_shuffle_param', '_available_plots', 'seed', 'y_test', 'transform_target_param'}
2023-07-11 10:02:49,484:INFO:Checking environment
2023-07-11 10:02:49,484:INFO:python_version: 3.9.13
2023-07-11 10:02:49,484:INFO:python_build: ('main', 'Oct 13 2022 21:23:06')
2023-07-11 10:02:49,484:INFO:machine: AMD64
2023-07-11 10:02:49,484:INFO:platform: Windows-10-10.0.22621-SP0
2023-07-11 10:02:49,484:INFO:Memory: svmem(total=16893358080, available=3217428480, percent=81.0, used=13675929600, free=3217428480)
2023-07-11 10:02:49,484:INFO:Physical Core: 8
2023-07-11 10:02:49,484:INFO:Logical Core: 16
2023-07-11 10:02:49,484:INFO:Checking libraries
2023-07-11 10:02:49,484:INFO:System:
2023-07-11 10:02:49,485:INFO:    python: 3.9.13 (main, Oct 13 2022, 21:23:06) [MSC v.1916 64 bit (AMD64)]
2023-07-11 10:02:49,485:INFO:executable: C:\Users\didit\anaconda3\python.exe
2023-07-11 10:02:49,485:INFO:   machine: Windows-10-10.0.22621-SP0
2023-07-11 10:02:49,485:INFO:PyCaret required dependencies:
2023-07-11 10:02:49,485:INFO:                 pip: 23.0.1
2023-07-11 10:02:49,485:INFO:          setuptools: 67.8.0
2023-07-11 10:02:49,485:INFO:             pycaret: 3.0.2
2023-07-11 10:02:49,485:INFO:             IPython: 8.12.0
2023-07-11 10:02:49,485:INFO:          ipywidgets: 8.0.4
2023-07-11 10:02:49,485:INFO:                tqdm: 4.65.0
2023-07-11 10:02:49,485:INFO:               numpy: 1.21.5
2023-07-11 10:02:49,485:INFO:              pandas: 1.5.3
2023-07-11 10:02:49,485:INFO:              jinja2: 3.1.2
2023-07-11 10:02:49,485:INFO:               scipy: 1.10.1
2023-07-11 10:02:49,485:INFO:              joblib: 1.2.0
2023-07-11 10:02:49,485:INFO:             sklearn: 1.2.2
2023-07-11 10:02:49,485:INFO:                pyod: 1.0.9
2023-07-11 10:02:49,485:INFO:            imblearn: 0.10.1
2023-07-11 10:02:49,486:INFO:   category_encoders: 2.6.1
2023-07-11 10:02:49,486:INFO:            lightgbm: 3.3.5
2023-07-11 10:02:49,486:INFO:               numba: 0.57.0
2023-07-11 10:02:49,486:INFO:            requests: 2.29.0
2023-07-11 10:02:49,486:INFO:          matplotlib: 3.7.1
2023-07-11 10:02:49,486:INFO:          scikitplot: 0.3.7
2023-07-11 10:02:49,486:INFO:         yellowbrick: 1.5
2023-07-11 10:02:49,486:INFO:              plotly: 5.9.0
2023-07-11 10:02:49,486:INFO:             kaleido: 0.2.1
2023-07-11 10:02:49,486:INFO:         statsmodels: 0.13.5
2023-07-11 10:02:49,486:INFO:              sktime: 0.17.0
2023-07-11 10:02:49,486:INFO:               tbats: 1.1.3
2023-07-11 10:02:49,486:INFO:            pmdarima: 2.0.3
2023-07-11 10:02:49,486:INFO:              psutil: 5.9.0
2023-07-11 10:02:49,486:INFO:PyCaret optional dependencies:
2023-07-11 10:02:49,486:INFO:                shap: 0.41.0
2023-07-11 10:02:49,486:INFO:           interpret: Not installed
2023-07-11 10:02:49,486:INFO:                umap: Not installed
2023-07-11 10:02:49,486:INFO:    pandas_profiling: 4.3.1
2023-07-11 10:02:49,487:INFO:  explainerdashboard: Not installed
2023-07-11 10:02:49,487:INFO:             autoviz: Not installed
2023-07-11 10:02:49,487:INFO:           fairlearn: Not installed
2023-07-11 10:02:49,487:INFO:             xgboost: 1.7.6
2023-07-11 10:02:49,487:INFO:            catboost: Not installed
2023-07-11 10:02:49,487:INFO:              kmodes: Not installed
2023-07-11 10:02:49,487:INFO:             mlxtend: Not installed
2023-07-11 10:02:49,487:INFO:       statsforecast: Not installed
2023-07-11 10:02:49,487:INFO:        tune_sklearn: Not installed
2023-07-11 10:02:49,487:INFO:                 ray: Not installed
2023-07-11 10:02:49,487:INFO:            hyperopt: Not installed
2023-07-11 10:02:49,487:INFO:              optuna: Not installed
2023-07-11 10:02:49,487:INFO:               skopt: Not installed
2023-07-11 10:02:49,487:INFO:              mlflow: 2.4.2
2023-07-11 10:02:49,487:INFO:              gradio: Not installed
2023-07-11 10:02:49,487:INFO:             fastapi: 0.95.2
2023-07-11 10:02:49,487:INFO:             uvicorn: 0.22.0
2023-07-11 10:02:49,487:INFO:              m2cgen: Not installed
2023-07-11 10:02:49,487:INFO:           evidently: Not installed
2023-07-11 10:02:49,487:INFO:               fugue: Not installed
2023-07-11 10:02:49,488:INFO:           streamlit: 1.23.1
2023-07-11 10:02:49,488:INFO:             prophet: Not installed
2023-07-11 10:02:49,488:INFO:None
2023-07-11 10:02:49,488:INFO:Set up data.
2023-07-11 10:03:46,455:INFO:PyCaret RegressionExperiment
2023-07-11 10:03:46,455:INFO:Logging name: reg-default-name
2023-07-11 10:03:46,455:INFO:ML Usecase: MLUsecase.REGRESSION
2023-07-11 10:03:46,455:INFO:version 3.0.2
2023-07-11 10:03:46,455:INFO:Initializing setup()
2023-07-11 10:03:46,455:INFO:self.USI: 4681
2023-07-11 10:03:46,455:INFO:self._variable_keys: {'_ml_usecase', 'log_plots_param', 'USI', 'X_train', 'pipeline', 'gpu_param', 'exp_name_log', 'y', 'html_param', 'X', 'data', 'exp_id', 'y_train', 'target_param', 'X_test', 'fold_groups_param', 'n_jobs_param', 'idx', 'fold_generator', 'logging_param', 'memory', 'gpu_n_jobs_param', 'fold_shuffle_param', '_available_plots', 'seed', 'y_test', 'transform_target_param'}
2023-07-11 10:03:46,455:INFO:Checking environment
2023-07-11 10:03:46,455:INFO:python_version: 3.9.13
2023-07-11 10:03:46,455:INFO:python_build: ('main', 'Oct 13 2022 21:23:06')
2023-07-11 10:03:46,455:INFO:machine: AMD64
2023-07-11 10:03:46,455:INFO:platform: Windows-10-10.0.22621-SP0
2023-07-11 10:03:46,455:INFO:Memory: svmem(total=16893358080, available=3566223360, percent=78.9, used=13327134720, free=3566223360)
2023-07-11 10:03:46,455:INFO:Physical Core: 8
2023-07-11 10:03:46,455:INFO:Logical Core: 16
2023-07-11 10:03:46,457:INFO:Checking libraries
2023-07-11 10:03:46,457:INFO:System:
2023-07-11 10:03:46,457:INFO:    python: 3.9.13 (main, Oct 13 2022, 21:23:06) [MSC v.1916 64 bit (AMD64)]
2023-07-11 10:03:46,457:INFO:executable: C:\Users\didit\anaconda3\python.exe
2023-07-11 10:03:46,457:INFO:   machine: Windows-10-10.0.22621-SP0
2023-07-11 10:03:46,457:INFO:PyCaret required dependencies:
2023-07-11 10:03:46,457:INFO:                 pip: 23.0.1
2023-07-11 10:03:46,457:INFO:          setuptools: 67.8.0
2023-07-11 10:03:46,457:INFO:             pycaret: 3.0.2
2023-07-11 10:03:46,457:INFO:             IPython: 8.12.0
2023-07-11 10:03:46,457:INFO:          ipywidgets: 8.0.4
2023-07-11 10:03:46,457:INFO:                tqdm: 4.65.0
2023-07-11 10:03:46,457:INFO:               numpy: 1.21.5
2023-07-11 10:03:46,457:INFO:              pandas: 1.5.3
2023-07-11 10:03:46,457:INFO:              jinja2: 3.1.2
2023-07-11 10:03:46,457:INFO:               scipy: 1.10.1
2023-07-11 10:03:46,457:INFO:              joblib: 1.2.0
2023-07-11 10:03:46,457:INFO:             sklearn: 1.2.2
2023-07-11 10:03:46,457:INFO:                pyod: 1.0.9
2023-07-11 10:03:46,457:INFO:            imblearn: 0.10.1
2023-07-11 10:03:46,457:INFO:   category_encoders: 2.6.1
2023-07-11 10:03:46,457:INFO:            lightgbm: 3.3.5
2023-07-11 10:03:46,457:INFO:               numba: 0.57.0
2023-07-11 10:03:46,457:INFO:            requests: 2.29.0
2023-07-11 10:03:46,457:INFO:          matplotlib: 3.7.1
2023-07-11 10:03:46,457:INFO:          scikitplot: 0.3.7
2023-07-11 10:03:46,457:INFO:         yellowbrick: 1.5
2023-07-11 10:03:46,457:INFO:              plotly: 5.9.0
2023-07-11 10:03:46,457:INFO:             kaleido: 0.2.1
2023-07-11 10:03:46,457:INFO:         statsmodels: 0.13.5
2023-07-11 10:03:46,457:INFO:              sktime: 0.17.0
2023-07-11 10:03:46,457:INFO:               tbats: 1.1.3
2023-07-11 10:03:46,457:INFO:            pmdarima: 2.0.3
2023-07-11 10:03:46,457:INFO:              psutil: 5.9.0
2023-07-11 10:03:46,457:INFO:PyCaret optional dependencies:
2023-07-11 10:03:46,457:INFO:                shap: 0.41.0
2023-07-11 10:03:46,457:INFO:           interpret: Not installed
2023-07-11 10:03:46,457:INFO:                umap: Not installed
2023-07-11 10:03:46,457:INFO:    pandas_profiling: 4.3.1
2023-07-11 10:03:46,457:INFO:  explainerdashboard: Not installed
2023-07-11 10:03:46,457:INFO:             autoviz: Not installed
2023-07-11 10:03:46,457:INFO:           fairlearn: Not installed
2023-07-11 10:03:46,457:INFO:             xgboost: 1.7.6
2023-07-11 10:03:46,458:INFO:            catboost: Not installed
2023-07-11 10:03:46,458:INFO:              kmodes: Not installed
2023-07-11 10:03:46,458:INFO:             mlxtend: Not installed
2023-07-11 10:03:46,458:INFO:       statsforecast: Not installed
2023-07-11 10:03:46,458:INFO:        tune_sklearn: Not installed
2023-07-11 10:03:46,458:INFO:                 ray: Not installed
2023-07-11 10:03:46,458:INFO:            hyperopt: Not installed
2023-07-11 10:03:46,458:INFO:              optuna: Not installed
2023-07-11 10:03:46,458:INFO:               skopt: Not installed
2023-07-11 10:03:46,458:INFO:              mlflow: 2.4.2
2023-07-11 10:03:46,458:INFO:              gradio: Not installed
2023-07-11 10:03:46,458:INFO:             fastapi: 0.95.2
2023-07-11 10:03:46,458:INFO:             uvicorn: 0.22.0
2023-07-11 10:03:46,458:INFO:              m2cgen: Not installed
2023-07-11 10:03:46,458:INFO:           evidently: Not installed
2023-07-11 10:03:46,458:INFO:               fugue: Not installed
2023-07-11 10:03:46,458:INFO:           streamlit: 1.23.1
2023-07-11 10:03:46,458:INFO:             prophet: Not installed
2023-07-11 10:03:46,458:INFO:None
2023-07-11 10:03:46,458:INFO:Set up data.
2023-07-11 10:03:46,459:INFO:Set up train/test split.
2023-07-11 10:03:46,475:INFO:Set up index.
2023-07-11 10:03:46,475:INFO:Set up folding strategy.
2023-07-11 10:03:46,475:INFO:Assigning column types.
2023-07-11 10:03:46,477:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-07-11 10:03:46,477:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-07-11 10:03:46,484:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-07-11 10:03:46,488:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 10:03:46,526:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 10:03:46,573:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 10:03:46,573:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 10:03:46,573:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 10:03:46,573:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-07-11 10:03:46,573:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-07-11 10:03:46,573:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 10:03:46,620:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 10:03:46,658:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 10:03:46,658:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 10:03:46,659:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 10:03:46,660:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-07-11 10:03:46,663:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-07-11 10:03:46,666:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 10:03:46,707:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 10:03:46,739:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 10:03:46,739:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 10:03:46,739:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 10:03:46,739:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-07-11 10:03:46,739:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 10:03:46,786:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 10:03:46,817:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 10:03:46,817:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 10:03:46,817:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 10:03:46,817:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-07-11 10:03:46,833:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 10:03:46,881:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 10:03:46,908:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 10:03:46,908:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 10:03:46,908:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 10:03:46,908:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 10:03:46,955:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 10:03:46,987:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 10:03:46,987:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 10:03:46,987:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 10:03:46,987:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-07-11 10:03:47,034:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 10:03:47,079:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 10:03:47,079:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 10:03:47,081:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 10:03:47,125:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 10:03:47,168:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 10:03:47,168:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 10:03:47,168:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 10:03:47,168:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-07-11 10:03:47,219:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 10:03:47,251:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 10:03:47,259:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 10:03:47,308:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 10:03:47,355:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 10:03:47,355:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 10:03:47,355:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-07-11 10:03:47,442:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 10:03:47,444:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 10:03:47,531:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 10:03:47,533:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 10:03:47,534:INFO:Preparing preprocessing pipeline...
2023-07-11 10:03:47,534:INFO:Set up simple imputation.
2023-07-11 10:03:47,535:INFO:Set up column name cleaning.
2023-07-11 10:03:47,552:INFO:Finished creating preprocessing pipeline.
2023-07-11 10:03:47,555:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\didit\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=[], transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames()))])
2023-07-11 10:03:47,555:INFO:Creating final display dataframe.
2023-07-11 10:03:47,613:INFO:Setup _display_container:                     Description                                              Value
0                    Session id                                               1762
1                        Target  school;sex;age;address;famsize;Pstatus;Medu;Fe...
2                   Target type                                         Regression
3           Original data shape                                           (395, 1)
4        Transformed data shape                                           (395, 1)
5   Transformed train set shape                                           (276, 1)
6    Transformed test set shape                                           (119, 1)
7                    Preprocess                                               True
8               Imputation type                                             simple
9            Numeric imputation                                               mean
10       Categorical imputation                                               mode
11               Fold Generator                                              KFold
12                  Fold Number                                                 10
13                     CPU Jobs                                                 -1
14                      Use GPU                                              False
15               Log Experiment                                              False
16              Experiment Name                                   reg-default-name
17                          USI                                               4681
2023-07-11 10:03:47,691:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 10:03:47,691:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 10:03:47,785:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 10:03:47,785:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 10:03:47,785:INFO:setup() successfully completed in 1.68s...............
2023-07-11 10:03:47,785:INFO:Initializing compare_models()
2023-07-11 10:03:47,785:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1DD3B4EE0>, include=None, fold=None, round=4, cross_validation=True, sort=R2, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x000001E1DD3B4EE0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2023-07-11 10:03:47,785:INFO:Checking exceptions
2023-07-11 10:03:47,798:INFO:Preparing display monitor
2023-07-11 10:03:47,799:INFO:Initializing Linear Regression
2023-07-11 10:03:47,799:INFO:Total runtime is 0.0 minutes
2023-07-11 10:03:47,799:INFO:SubProcess create_model() called ==================================
2023-07-11 10:03:47,799:INFO:Initializing create_model()
2023-07-11 10:03:47,799:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1DD3B4EE0>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D98BB070>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 10:03:47,799:INFO:Checking exceptions
2023-07-11 10:03:47,799:INFO:Importing libraries
2023-07-11 10:03:47,799:INFO:Copying training dataset
2023-07-11 10:03:47,802:INFO:Defining folds
2023-07-11 10:03:47,802:INFO:Declaring metric variables
2023-07-11 10:03:47,802:INFO:Importing untrained model
2023-07-11 10:03:47,802:INFO:Linear Regression Imported successfully
2023-07-11 10:03:47,802:INFO:Starting cross validation
2023-07-11 10:03:47,802:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 10:03:51,819:WARNING:create_model() for lr raised an exception or returned all 0.0, trying without fit_kwargs:
2023-07-11 10:03:51,819:WARNING:Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 792, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_base.py", line 648, in fit
    X, y = self._validate_data(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1106, in check_X_y
    X = check_array(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 778, in check_array
    dtype_orig = np.result_type(*dtypes_orig)
  File "<__array_function__ internals>", line 5, in result_type
ValueError: at least one array or dtype is required


2023-07-11 10:03:51,819:INFO:Initializing create_model()
2023-07-11 10:03:51,819:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1DD3B4EE0>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D98BB070>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 10:03:51,819:INFO:Checking exceptions
2023-07-11 10:03:51,819:INFO:Importing libraries
2023-07-11 10:03:51,819:INFO:Copying training dataset
2023-07-11 10:03:51,819:INFO:Defining folds
2023-07-11 10:03:51,819:INFO:Declaring metric variables
2023-07-11 10:03:51,819:INFO:Importing untrained model
2023-07-11 10:03:51,819:INFO:Linear Regression Imported successfully
2023-07-11 10:03:51,819:INFO:Starting cross validation
2023-07-11 10:03:51,819:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 10:03:56,250:ERROR:create_model() for lr raised an exception or returned all 0.0:
2023-07-11 10:03:56,250:ERROR:Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 792, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_base.py", line 648, in fit
    X, y = self._validate_data(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1106, in check_X_y
    X = check_array(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 778, in check_array
    dtype_orig = np.result_type(*dtypes_orig)
  File "<__array_function__ internals>", line 5, in result_type
ValueError: at least one array or dtype is required


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 808, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_base.py", line 648, in fit
    X, y = self._validate_data(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1106, in check_X_y
    X = check_array(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 778, in check_array
    dtype_orig = np.result_type(*dtypes_orig)
  File "<__array_function__ internals>", line 5, in result_type
ValueError: at least one array or dtype is required


2023-07-11 10:03:57,453:INFO:Initializing Lasso Regression
2023-07-11 10:03:57,453:INFO:Total runtime is 0.1608950893084208 minutes
2023-07-11 10:03:57,453:INFO:SubProcess create_model() called ==================================
2023-07-11 10:03:57,454:INFO:Initializing create_model()
2023-07-11 10:03:57,454:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1DD3B4EE0>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D98BB070>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 10:03:57,454:INFO:Checking exceptions
2023-07-11 10:03:57,454:INFO:Importing libraries
2023-07-11 10:03:57,454:INFO:Copying training dataset
2023-07-11 10:03:57,456:INFO:Defining folds
2023-07-11 10:03:57,456:INFO:Declaring metric variables
2023-07-11 10:03:57,456:INFO:Importing untrained model
2023-07-11 10:03:57,456:INFO:Lasso Regression Imported successfully
2023-07-11 10:03:57,456:INFO:Starting cross validation
2023-07-11 10:03:57,457:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 10:04:01,636:WARNING:create_model() for lasso raised an exception or returned all 0.0, trying without fit_kwargs:
2023-07-11 10:04:01,636:WARNING:Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 792, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py", line 908, in fit
    X, y = self._validate_data(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1106, in check_X_y
    X = check_array(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 778, in check_array
    dtype_orig = np.result_type(*dtypes_orig)
  File "<__array_function__ internals>", line 5, in result_type
ValueError: at least one array or dtype is required


2023-07-11 10:04:01,636:INFO:Initializing create_model()
2023-07-11 10:04:01,636:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1DD3B4EE0>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D98BB070>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 10:04:01,636:INFO:Checking exceptions
2023-07-11 10:04:01,636:INFO:Importing libraries
2023-07-11 10:04:01,636:INFO:Copying training dataset
2023-07-11 10:04:01,636:INFO:Defining folds
2023-07-11 10:04:01,636:INFO:Declaring metric variables
2023-07-11 10:04:01,636:INFO:Importing untrained model
2023-07-11 10:04:01,636:INFO:Lasso Regression Imported successfully
2023-07-11 10:04:01,636:INFO:Starting cross validation
2023-07-11 10:04:01,636:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 10:04:06,109:ERROR:create_model() for lasso raised an exception or returned all 0.0:
2023-07-11 10:04:06,109:ERROR:Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 792, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py", line 908, in fit
    X, y = self._validate_data(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1106, in check_X_y
    X = check_array(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 778, in check_array
    dtype_orig = np.result_type(*dtypes_orig)
  File "<__array_function__ internals>", line 5, in result_type
ValueError: at least one array or dtype is required


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 808, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py", line 908, in fit
    X, y = self._validate_data(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1106, in check_X_y
    X = check_array(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 778, in check_array
    dtype_orig = np.result_type(*dtypes_orig)
  File "<__array_function__ internals>", line 5, in result_type
ValueError: at least one array or dtype is required


2023-07-11 10:04:07,171:INFO:Initializing Ridge Regression
2023-07-11 10:04:07,171:INFO:Total runtime is 0.3228639483451843 minutes
2023-07-11 10:04:07,171:INFO:SubProcess create_model() called ==================================
2023-07-11 10:04:07,171:INFO:Initializing create_model()
2023-07-11 10:04:07,171:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1DD3B4EE0>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D98BB070>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 10:04:07,171:INFO:Checking exceptions
2023-07-11 10:04:07,171:INFO:Importing libraries
2023-07-11 10:04:07,171:INFO:Copying training dataset
2023-07-11 10:04:07,171:INFO:Defining folds
2023-07-11 10:04:07,171:INFO:Declaring metric variables
2023-07-11 10:04:07,171:INFO:Importing untrained model
2023-07-11 10:04:07,171:INFO:Ridge Regression Imported successfully
2023-07-11 10:04:07,171:INFO:Starting cross validation
2023-07-11 10:04:07,171:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 10:04:11,325:WARNING:create_model() for ridge raised an exception or returned all 0.0, trying without fit_kwargs:
2023-07-11 10:04:11,325:WARNING:Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 792, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py", line 1126, in fit
    X, y = self._validate_data(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1106, in check_X_y
    X = check_array(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 778, in check_array
    dtype_orig = np.result_type(*dtypes_orig)
  File "<__array_function__ internals>", line 5, in result_type
ValueError: at least one array or dtype is required


2023-07-11 10:04:11,325:INFO:Initializing create_model()
2023-07-11 10:04:11,325:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1DD3B4EE0>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D98BB070>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 10:04:11,325:INFO:Checking exceptions
2023-07-11 10:04:11,325:INFO:Importing libraries
2023-07-11 10:04:11,325:INFO:Copying training dataset
2023-07-11 10:04:11,325:INFO:Defining folds
2023-07-11 10:04:11,325:INFO:Declaring metric variables
2023-07-11 10:04:11,325:INFO:Importing untrained model
2023-07-11 10:04:11,325:INFO:Ridge Regression Imported successfully
2023-07-11 10:04:11,325:INFO:Starting cross validation
2023-07-11 10:04:11,325:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 10:04:15,908:ERROR:create_model() for ridge raised an exception or returned all 0.0:
2023-07-11 10:04:15,908:ERROR:Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 792, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py", line 1126, in fit
    X, y = self._validate_data(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1106, in check_X_y
    X = check_array(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 778, in check_array
    dtype_orig = np.result_type(*dtypes_orig)
  File "<__array_function__ internals>", line 5, in result_type
ValueError: at least one array or dtype is required


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 808, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py", line 1126, in fit
    X, y = self._validate_data(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1106, in check_X_y
    X = check_array(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 778, in check_array
    dtype_orig = np.result_type(*dtypes_orig)
  File "<__array_function__ internals>", line 5, in result_type
ValueError: at least one array or dtype is required


2023-07-11 10:04:16,940:INFO:Initializing Elastic Net
2023-07-11 10:04:16,940:INFO:Total runtime is 0.48568337361017855 minutes
2023-07-11 10:04:16,940:INFO:SubProcess create_model() called ==================================
2023-07-11 10:04:16,940:INFO:Initializing create_model()
2023-07-11 10:04:16,940:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1DD3B4EE0>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D98BB070>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 10:04:16,940:INFO:Checking exceptions
2023-07-11 10:04:16,940:INFO:Importing libraries
2023-07-11 10:04:16,940:INFO:Copying training dataset
2023-07-11 10:04:16,940:INFO:Defining folds
2023-07-11 10:04:16,940:INFO:Declaring metric variables
2023-07-11 10:04:16,940:INFO:Importing untrained model
2023-07-11 10:04:16,940:INFO:Elastic Net Imported successfully
2023-07-11 10:04:16,940:INFO:Starting cross validation
2023-07-11 10:04:16,940:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 10:04:21,124:WARNING:create_model() for en raised an exception or returned all 0.0, trying without fit_kwargs:
2023-07-11 10:04:21,124:WARNING:Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 792, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py", line 908, in fit
    X, y = self._validate_data(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1106, in check_X_y
    X = check_array(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 778, in check_array
    dtype_orig = np.result_type(*dtypes_orig)
  File "<__array_function__ internals>", line 5, in result_type
ValueError: at least one array or dtype is required


2023-07-11 10:04:21,124:INFO:Initializing create_model()
2023-07-11 10:04:21,124:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1DD3B4EE0>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D98BB070>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 10:04:21,124:INFO:Checking exceptions
2023-07-11 10:04:21,124:INFO:Importing libraries
2023-07-11 10:04:21,124:INFO:Copying training dataset
2023-07-11 10:04:21,124:INFO:Defining folds
2023-07-11 10:04:21,124:INFO:Declaring metric variables
2023-07-11 10:04:21,124:INFO:Importing untrained model
2023-07-11 10:04:21,124:INFO:Elastic Net Imported successfully
2023-07-11 10:04:21,124:INFO:Starting cross validation
2023-07-11 10:04:21,124:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 10:04:25,805:ERROR:create_model() for en raised an exception or returned all 0.0:
2023-07-11 10:04:25,805:ERROR:Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 792, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py", line 908, in fit
    X, y = self._validate_data(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1106, in check_X_y
    X = check_array(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 778, in check_array
    dtype_orig = np.result_type(*dtypes_orig)
  File "<__array_function__ internals>", line 5, in result_type
ValueError: at least one array or dtype is required


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 808, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py", line 908, in fit
    X, y = self._validate_data(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1106, in check_X_y
    X = check_array(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 778, in check_array
    dtype_orig = np.result_type(*dtypes_orig)
  File "<__array_function__ internals>", line 5, in result_type
ValueError: at least one array or dtype is required


2023-07-11 10:04:26,937:INFO:Initializing Least Angle Regression
2023-07-11 10:04:26,937:INFO:Total runtime is 0.6522910118103027 minutes
2023-07-11 10:04:26,937:INFO:SubProcess create_model() called ==================================
2023-07-11 10:04:26,937:INFO:Initializing create_model()
2023-07-11 10:04:26,937:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1DD3B4EE0>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D98BB070>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 10:04:26,937:INFO:Checking exceptions
2023-07-11 10:04:26,937:INFO:Importing libraries
2023-07-11 10:04:26,937:INFO:Copying training dataset
2023-07-11 10:04:26,941:INFO:Defining folds
2023-07-11 10:04:26,941:INFO:Declaring metric variables
2023-07-11 10:04:26,941:INFO:Importing untrained model
2023-07-11 10:04:26,941:INFO:Least Angle Regression Imported successfully
2023-07-11 10:04:26,941:INFO:Starting cross validation
2023-07-11 10:04:26,941:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 10:04:31,126:WARNING:create_model() for lar raised an exception or returned all 0.0, trying without fit_kwargs:
2023-07-11 10:04:31,126:WARNING:Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 792, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py", line 1125, in fit
    X, y = self._validate_data(X, y, y_numeric=True, multi_output=True)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1106, in check_X_y
    X = check_array(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 778, in check_array
    dtype_orig = np.result_type(*dtypes_orig)
  File "<__array_function__ internals>", line 5, in result_type
ValueError: at least one array or dtype is required


2023-07-11 10:04:31,126:INFO:Initializing create_model()
2023-07-11 10:04:31,126:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1DD3B4EE0>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D98BB070>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 10:04:31,126:INFO:Checking exceptions
2023-07-11 10:04:31,126:INFO:Importing libraries
2023-07-11 10:04:31,126:INFO:Copying training dataset
2023-07-11 10:04:31,126:INFO:Defining folds
2023-07-11 10:04:31,126:INFO:Declaring metric variables
2023-07-11 10:04:31,126:INFO:Importing untrained model
2023-07-11 10:04:31,126:INFO:Least Angle Regression Imported successfully
2023-07-11 10:04:31,126:INFO:Starting cross validation
2023-07-11 10:04:31,126:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 10:04:35,726:ERROR:create_model() for lar raised an exception or returned all 0.0:
2023-07-11 10:04:35,726:ERROR:Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 792, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py", line 1125, in fit
    X, y = self._validate_data(X, y, y_numeric=True, multi_output=True)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1106, in check_X_y
    X = check_array(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 778, in check_array
    dtype_orig = np.result_type(*dtypes_orig)
  File "<__array_function__ internals>", line 5, in result_type
ValueError: at least one array or dtype is required


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 808, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py", line 1125, in fit
    X, y = self._validate_data(X, y, y_numeric=True, multi_output=True)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1106, in check_X_y
    X = check_array(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 778, in check_array
    dtype_orig = np.result_type(*dtypes_orig)
  File "<__array_function__ internals>", line 5, in result_type
ValueError: at least one array or dtype is required


2023-07-11 10:04:36,820:INFO:Initializing Lasso Least Angle Regression
2023-07-11 10:04:36,820:INFO:Total runtime is 0.8170013745625814 minutes
2023-07-11 10:04:36,820:INFO:SubProcess create_model() called ==================================
2023-07-11 10:04:36,820:INFO:Initializing create_model()
2023-07-11 10:04:36,820:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1DD3B4EE0>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D98BB070>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 10:04:36,820:INFO:Checking exceptions
2023-07-11 10:04:36,820:INFO:Importing libraries
2023-07-11 10:04:36,820:INFO:Copying training dataset
2023-07-11 10:04:36,820:INFO:Defining folds
2023-07-11 10:04:36,820:INFO:Declaring metric variables
2023-07-11 10:04:36,820:INFO:Importing untrained model
2023-07-11 10:04:36,820:INFO:Lasso Least Angle Regression Imported successfully
2023-07-11 10:04:36,820:INFO:Starting cross validation
2023-07-11 10:04:36,820:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 10:04:41,109:WARNING:create_model() for llar raised an exception or returned all 0.0, trying without fit_kwargs:
2023-07-11 10:04:41,109:WARNING:Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 792, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py", line 1125, in fit
    X, y = self._validate_data(X, y, y_numeric=True, multi_output=True)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1106, in check_X_y
    X = check_array(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 778, in check_array
    dtype_orig = np.result_type(*dtypes_orig)
  File "<__array_function__ internals>", line 5, in result_type
ValueError: at least one array or dtype is required


2023-07-11 10:04:41,109:INFO:Initializing create_model()
2023-07-11 10:04:41,109:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1DD3B4EE0>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D98BB070>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 10:04:41,109:INFO:Checking exceptions
2023-07-11 10:04:41,109:INFO:Importing libraries
2023-07-11 10:04:41,109:INFO:Copying training dataset
2023-07-11 10:04:41,109:INFO:Defining folds
2023-07-11 10:04:41,109:INFO:Declaring metric variables
2023-07-11 10:04:41,109:INFO:Importing untrained model
2023-07-11 10:04:41,109:INFO:Lasso Least Angle Regression Imported successfully
2023-07-11 10:04:41,109:INFO:Starting cross validation
2023-07-11 10:04:41,109:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 10:04:45,741:ERROR:create_model() for llar raised an exception or returned all 0.0:
2023-07-11 10:04:45,741:ERROR:Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 792, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py", line 1125, in fit
    X, y = self._validate_data(X, y, y_numeric=True, multi_output=True)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1106, in check_X_y
    X = check_array(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 778, in check_array
    dtype_orig = np.result_type(*dtypes_orig)
  File "<__array_function__ internals>", line 5, in result_type
ValueError: at least one array or dtype is required


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 808, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py", line 1125, in fit
    X, y = self._validate_data(X, y, y_numeric=True, multi_output=True)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1106, in check_X_y
    X = check_array(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 778, in check_array
    dtype_orig = np.result_type(*dtypes_orig)
  File "<__array_function__ internals>", line 5, in result_type
ValueError: at least one array or dtype is required


2023-07-11 10:04:46,894:INFO:Initializing Orthogonal Matching Pursuit
2023-07-11 10:04:46,894:INFO:Total runtime is 0.9849120299021403 minutes
2023-07-11 10:04:46,894:INFO:SubProcess create_model() called ==================================
2023-07-11 10:04:46,894:INFO:Initializing create_model()
2023-07-11 10:04:46,894:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1DD3B4EE0>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D98BB070>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 10:04:46,894:INFO:Checking exceptions
2023-07-11 10:04:46,894:INFO:Importing libraries
2023-07-11 10:04:46,894:INFO:Copying training dataset
2023-07-11 10:04:46,894:INFO:Defining folds
2023-07-11 10:04:46,894:INFO:Declaring metric variables
2023-07-11 10:04:46,894:INFO:Importing untrained model
2023-07-11 10:04:46,894:INFO:Orthogonal Matching Pursuit Imported successfully
2023-07-11 10:04:46,894:INFO:Starting cross validation
2023-07-11 10:04:46,894:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 10:04:51,061:WARNING:create_model() for omp raised an exception or returned all 0.0, trying without fit_kwargs:
2023-07-11 10:04:51,061:WARNING:Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 792, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_omp.py", line 741, in fit
    X, y = self._validate_data(X, y, multi_output=True, y_numeric=True)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1106, in check_X_y
    X = check_array(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 778, in check_array
    dtype_orig = np.result_type(*dtypes_orig)
  File "<__array_function__ internals>", line 5, in result_type
ValueError: at least one array or dtype is required


2023-07-11 10:04:51,061:INFO:Initializing create_model()
2023-07-11 10:04:51,061:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1DD3B4EE0>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D98BB070>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 10:04:51,061:INFO:Checking exceptions
2023-07-11 10:04:51,061:INFO:Importing libraries
2023-07-11 10:04:51,061:INFO:Copying training dataset
2023-07-11 10:04:51,061:INFO:Defining folds
2023-07-11 10:04:51,061:INFO:Declaring metric variables
2023-07-11 10:04:51,061:INFO:Importing untrained model
2023-07-11 10:04:51,061:INFO:Orthogonal Matching Pursuit Imported successfully
2023-07-11 10:04:51,061:INFO:Starting cross validation
2023-07-11 10:04:51,061:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 10:04:55,676:ERROR:create_model() for omp raised an exception or returned all 0.0:
2023-07-11 10:04:55,677:ERROR:Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 792, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_omp.py", line 741, in fit
    X, y = self._validate_data(X, y, multi_output=True, y_numeric=True)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1106, in check_X_y
    X = check_array(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 778, in check_array
    dtype_orig = np.result_type(*dtypes_orig)
  File "<__array_function__ internals>", line 5, in result_type
ValueError: at least one array or dtype is required


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 808, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_omp.py", line 741, in fit
    X, y = self._validate_data(X, y, multi_output=True, y_numeric=True)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1106, in check_X_y
    X = check_array(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 778, in check_array
    dtype_orig = np.result_type(*dtypes_orig)
  File "<__array_function__ internals>", line 5, in result_type
ValueError: at least one array or dtype is required


2023-07-11 10:04:56,726:INFO:Initializing Bayesian Ridge
2023-07-11 10:04:56,726:INFO:Total runtime is 1.1487771352132161 minutes
2023-07-11 10:04:56,726:INFO:SubProcess create_model() called ==================================
2023-07-11 10:04:56,726:INFO:Initializing create_model()
2023-07-11 10:04:56,726:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1DD3B4EE0>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D98BB070>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 10:04:56,726:INFO:Checking exceptions
2023-07-11 10:04:56,726:INFO:Importing libraries
2023-07-11 10:04:56,726:INFO:Copying training dataset
2023-07-11 10:04:56,726:INFO:Defining folds
2023-07-11 10:04:56,726:INFO:Declaring metric variables
2023-07-11 10:04:56,726:INFO:Importing untrained model
2023-07-11 10:04:56,726:INFO:Bayesian Ridge Imported successfully
2023-07-11 10:04:56,726:INFO:Starting cross validation
2023-07-11 10:04:56,726:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 10:05:01,043:WARNING:create_model() for br raised an exception or returned all 0.0, trying without fit_kwargs:
2023-07-11 10:05:01,043:WARNING:Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 792, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_bayes.py", line 231, in fit
    X, y = self._validate_data(X, y, dtype=[np.float64, np.float32], y_numeric=True)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1106, in check_X_y
    X = check_array(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 778, in check_array
    dtype_orig = np.result_type(*dtypes_orig)
  File "<__array_function__ internals>", line 5, in result_type
ValueError: at least one array or dtype is required


2023-07-11 10:05:01,043:INFO:Initializing create_model()
2023-07-11 10:05:01,043:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1DD3B4EE0>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D98BB070>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 10:05:01,043:INFO:Checking exceptions
2023-07-11 10:05:01,043:INFO:Importing libraries
2023-07-11 10:05:01,043:INFO:Copying training dataset
2023-07-11 10:05:01,043:INFO:Defining folds
2023-07-11 10:05:01,043:INFO:Declaring metric variables
2023-07-11 10:05:01,043:INFO:Importing untrained model
2023-07-11 10:05:01,043:INFO:Bayesian Ridge Imported successfully
2023-07-11 10:05:01,043:INFO:Starting cross validation
2023-07-11 10:05:01,043:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 10:05:05,808:ERROR:create_model() for br raised an exception or returned all 0.0:
2023-07-11 10:05:05,808:ERROR:Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 792, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_bayes.py", line 231, in fit
    X, y = self._validate_data(X, y, dtype=[np.float64, np.float32], y_numeric=True)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1106, in check_X_y
    X = check_array(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 778, in check_array
    dtype_orig = np.result_type(*dtypes_orig)
  File "<__array_function__ internals>", line 5, in result_type
ValueError: at least one array or dtype is required


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 808, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_bayes.py", line 231, in fit
    X, y = self._validate_data(X, y, dtype=[np.float64, np.float32], y_numeric=True)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1106, in check_X_y
    X = check_array(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 778, in check_array
    dtype_orig = np.result_type(*dtypes_orig)
  File "<__array_function__ internals>", line 5, in result_type
ValueError: at least one array or dtype is required


2023-07-11 10:05:06,906:INFO:Initializing Passive Aggressive Regressor
2023-07-11 10:05:06,906:INFO:Total runtime is 1.3184358755747476 minutes
2023-07-11 10:05:06,907:INFO:SubProcess create_model() called ==================================
2023-07-11 10:05:06,907:INFO:Initializing create_model()
2023-07-11 10:05:06,907:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1DD3B4EE0>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D98BB070>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 10:05:06,907:INFO:Checking exceptions
2023-07-11 10:05:06,907:INFO:Importing libraries
2023-07-11 10:05:06,907:INFO:Copying training dataset
2023-07-11 10:05:06,909:INFO:Defining folds
2023-07-11 10:05:06,909:INFO:Declaring metric variables
2023-07-11 10:05:06,909:INFO:Importing untrained model
2023-07-11 10:05:06,909:INFO:Passive Aggressive Regressor Imported successfully
2023-07-11 10:05:06,909:INFO:Starting cross validation
2023-07-11 10:05:06,910:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 10:05:11,293:WARNING:create_model() for par raised an exception or returned all 0.0, trying without fit_kwargs:
2023-07-11 10:05:11,293:WARNING:Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 792, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_passive_aggressive.py", line 567, in fit
    return self._fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1530, in _fit
    self._partial_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1435, in _partial_fit
    X, y = self._validate_data(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1106, in check_X_y
    X = check_array(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 778, in check_array
    dtype_orig = np.result_type(*dtypes_orig)
  File "<__array_function__ internals>", line 5, in result_type
ValueError: at least one array or dtype is required


2023-07-11 10:05:11,293:INFO:Initializing create_model()
2023-07-11 10:05:11,293:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1DD3B4EE0>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D98BB070>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 10:05:11,293:INFO:Checking exceptions
2023-07-11 10:05:11,293:INFO:Importing libraries
2023-07-11 10:05:11,293:INFO:Copying training dataset
2023-07-11 10:05:11,293:INFO:Defining folds
2023-07-11 10:05:11,293:INFO:Declaring metric variables
2023-07-11 10:05:11,293:INFO:Importing untrained model
2023-07-11 10:05:11,293:INFO:Passive Aggressive Regressor Imported successfully
2023-07-11 10:05:11,293:INFO:Starting cross validation
2023-07-11 10:05:11,293:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 10:05:16,070:ERROR:create_model() for par raised an exception or returned all 0.0:
2023-07-11 10:05:16,077:ERROR:Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 792, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_passive_aggressive.py", line 567, in fit
    return self._fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1530, in _fit
    self._partial_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1435, in _partial_fit
    X, y = self._validate_data(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1106, in check_X_y
    X = check_array(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 778, in check_array
    dtype_orig = np.result_type(*dtypes_orig)
  File "<__array_function__ internals>", line 5, in result_type
ValueError: at least one array or dtype is required


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 808, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_passive_aggressive.py", line 567, in fit
    return self._fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1530, in _fit
    self._partial_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1435, in _partial_fit
    X, y = self._validate_data(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1106, in check_X_y
    X = check_array(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 778, in check_array
    dtype_orig = np.result_type(*dtypes_orig)
  File "<__array_function__ internals>", line 5, in result_type
ValueError: at least one array or dtype is required


2023-07-11 10:05:17,110:INFO:Initializing Huber Regressor
2023-07-11 10:05:17,110:INFO:Total runtime is 1.4885103185971578 minutes
2023-07-11 10:05:17,110:INFO:SubProcess create_model() called ==================================
2023-07-11 10:05:17,110:INFO:Initializing create_model()
2023-07-11 10:05:17,110:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1DD3B4EE0>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D98BB070>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 10:05:17,110:INFO:Checking exceptions
2023-07-11 10:05:17,110:INFO:Importing libraries
2023-07-11 10:05:17,110:INFO:Copying training dataset
2023-07-11 10:05:17,110:INFO:Defining folds
2023-07-11 10:05:17,110:INFO:Declaring metric variables
2023-07-11 10:05:17,110:INFO:Importing untrained model
2023-07-11 10:05:17,110:INFO:Huber Regressor Imported successfully
2023-07-11 10:05:17,110:INFO:Starting cross validation
2023-07-11 10:05:17,110:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 10:05:21,589:WARNING:create_model() for huber raised an exception or returned all 0.0, trying without fit_kwargs:
2023-07-11 10:05:21,589:WARNING:Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 792, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py", line 297, in fit
    X, y = self._validate_data(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1106, in check_X_y
    X = check_array(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 778, in check_array
    dtype_orig = np.result_type(*dtypes_orig)
  File "<__array_function__ internals>", line 5, in result_type
ValueError: at least one array or dtype is required


2023-07-11 10:05:21,589:INFO:Initializing create_model()
2023-07-11 10:05:21,589:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1DD3B4EE0>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D98BB070>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 10:05:21,589:INFO:Checking exceptions
2023-07-11 10:05:21,589:INFO:Importing libraries
2023-07-11 10:05:21,589:INFO:Copying training dataset
2023-07-11 10:05:21,605:INFO:Defining folds
2023-07-11 10:05:21,605:INFO:Declaring metric variables
2023-07-11 10:05:21,605:INFO:Importing untrained model
2023-07-11 10:05:21,605:INFO:Huber Regressor Imported successfully
2023-07-11 10:05:21,605:INFO:Starting cross validation
2023-07-11 10:05:21,605:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 10:05:26,237:ERROR:create_model() for huber raised an exception or returned all 0.0:
2023-07-11 10:05:26,237:ERROR:Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 792, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py", line 297, in fit
    X, y = self._validate_data(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1106, in check_X_y
    X = check_array(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 778, in check_array
    dtype_orig = np.result_type(*dtypes_orig)
  File "<__array_function__ internals>", line 5, in result_type
ValueError: at least one array or dtype is required


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 808, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py", line 297, in fit
    X, y = self._validate_data(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1106, in check_X_y
    X = check_array(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 778, in check_array
    dtype_orig = np.result_type(*dtypes_orig)
  File "<__array_function__ internals>", line 5, in result_type
ValueError: at least one array or dtype is required


2023-07-11 10:05:27,338:INFO:Initializing K Neighbors Regressor
2023-07-11 10:05:27,338:INFO:Total runtime is 1.6589810927708943 minutes
2023-07-11 10:05:27,338:INFO:SubProcess create_model() called ==================================
2023-07-11 10:05:27,338:INFO:Initializing create_model()
2023-07-11 10:05:27,338:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1DD3B4EE0>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D98BB070>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 10:05:27,338:INFO:Checking exceptions
2023-07-11 10:05:27,338:INFO:Importing libraries
2023-07-11 10:05:27,338:INFO:Copying training dataset
2023-07-11 10:05:27,338:INFO:Defining folds
2023-07-11 10:05:27,338:INFO:Declaring metric variables
2023-07-11 10:05:27,338:INFO:Importing untrained model
2023-07-11 10:05:27,338:INFO:K Neighbors Regressor Imported successfully
2023-07-11 10:05:27,338:INFO:Starting cross validation
2023-07-11 10:05:27,343:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 10:05:31,620:WARNING:create_model() for knn raised an exception or returned all 0.0, trying without fit_kwargs:
2023-07-11 10:05:31,620:WARNING:Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 792, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\neighbors\_regression.py", line 217, in fit
    return self._fit(X, y)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\neighbors\_base.py", line 454, in _fit
    X, y = self._validate_data(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1106, in check_X_y
    X = check_array(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 778, in check_array
    dtype_orig = np.result_type(*dtypes_orig)
  File "<__array_function__ internals>", line 5, in result_type
ValueError: at least one array or dtype is required


2023-07-11 10:05:31,620:INFO:Initializing create_model()
2023-07-11 10:05:31,620:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1DD3B4EE0>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D98BB070>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 10:05:31,620:INFO:Checking exceptions
2023-07-11 10:05:31,620:INFO:Importing libraries
2023-07-11 10:05:31,620:INFO:Copying training dataset
2023-07-11 10:05:31,620:INFO:Defining folds
2023-07-11 10:05:31,620:INFO:Declaring metric variables
2023-07-11 10:05:31,620:INFO:Importing untrained model
2023-07-11 10:05:31,620:INFO:K Neighbors Regressor Imported successfully
2023-07-11 10:05:31,620:INFO:Starting cross validation
2023-07-11 10:05:31,620:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 10:05:36,284:ERROR:create_model() for knn raised an exception or returned all 0.0:
2023-07-11 10:05:36,285:ERROR:Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 792, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\neighbors\_regression.py", line 217, in fit
    return self._fit(X, y)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\neighbors\_base.py", line 454, in _fit
    X, y = self._validate_data(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1106, in check_X_y
    X = check_array(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 778, in check_array
    dtype_orig = np.result_type(*dtypes_orig)
  File "<__array_function__ internals>", line 5, in result_type
ValueError: at least one array or dtype is required


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 808, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\neighbors\_regression.py", line 217, in fit
    return self._fit(X, y)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\neighbors\_base.py", line 454, in _fit
    X, y = self._validate_data(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1106, in check_X_y
    X = check_array(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 778, in check_array
    dtype_orig = np.result_type(*dtypes_orig)
  File "<__array_function__ internals>", line 5, in result_type
ValueError: at least one array or dtype is required


2023-07-11 10:05:37,495:INFO:Initializing Decision Tree Regressor
2023-07-11 10:05:37,495:INFO:Total runtime is 1.8282538771629333 minutes
2023-07-11 10:05:37,495:INFO:SubProcess create_model() called ==================================
2023-07-11 10:05:37,495:INFO:Initializing create_model()
2023-07-11 10:05:37,495:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1DD3B4EE0>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D98BB070>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 10:05:37,495:INFO:Checking exceptions
2023-07-11 10:05:37,495:INFO:Importing libraries
2023-07-11 10:05:37,495:INFO:Copying training dataset
2023-07-11 10:05:37,498:INFO:Defining folds
2023-07-11 10:05:37,498:INFO:Declaring metric variables
2023-07-11 10:05:37,498:INFO:Importing untrained model
2023-07-11 10:05:37,498:INFO:Decision Tree Regressor Imported successfully
2023-07-11 10:05:37,499:INFO:Starting cross validation
2023-07-11 10:05:37,499:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 10:05:41,606:WARNING:create_model() for dt raised an exception or returned all 0.0, trying without fit_kwargs:
2023-07-11 10:05:41,606:WARNING:Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 792, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\tree\_classes.py", line 1247, in fit
    super().fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\tree\_classes.py", line 186, in fit
    X, y = self._validate_data(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 579, in _validate_data
    X = check_array(X, input_name="X", **check_X_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 778, in check_array
    dtype_orig = np.result_type(*dtypes_orig)
  File "<__array_function__ internals>", line 5, in result_type
ValueError: at least one array or dtype is required


2023-07-11 10:05:41,606:INFO:Initializing create_model()
2023-07-11 10:05:41,606:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1DD3B4EE0>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D98BB070>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 10:05:41,606:INFO:Checking exceptions
2023-07-11 10:05:41,606:INFO:Importing libraries
2023-07-11 10:05:41,606:INFO:Copying training dataset
2023-07-11 10:05:41,606:INFO:Defining folds
2023-07-11 10:05:41,606:INFO:Declaring metric variables
2023-07-11 10:05:41,606:INFO:Importing untrained model
2023-07-11 10:05:41,606:INFO:Decision Tree Regressor Imported successfully
2023-07-11 10:05:41,606:INFO:Starting cross validation
2023-07-11 10:05:41,606:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 10:05:46,206:ERROR:create_model() for dt raised an exception or returned all 0.0:
2023-07-11 10:05:46,206:ERROR:Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 792, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\tree\_classes.py", line 1247, in fit
    super().fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\tree\_classes.py", line 186, in fit
    X, y = self._validate_data(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 579, in _validate_data
    X = check_array(X, input_name="X", **check_X_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 778, in check_array
    dtype_orig = np.result_type(*dtypes_orig)
  File "<__array_function__ internals>", line 5, in result_type
ValueError: at least one array or dtype is required


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 808, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\tree\_classes.py", line 1247, in fit
    super().fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\tree\_classes.py", line 186, in fit
    X, y = self._validate_data(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 579, in _validate_data
    X = check_array(X, input_name="X", **check_X_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 778, in check_array
    dtype_orig = np.result_type(*dtypes_orig)
  File "<__array_function__ internals>", line 5, in result_type
ValueError: at least one array or dtype is required


2023-07-11 10:05:47,237:INFO:Initializing Random Forest Regressor
2023-07-11 10:05:47,237:INFO:Total runtime is 1.9906280676523844 minutes
2023-07-11 10:05:47,237:INFO:SubProcess create_model() called ==================================
2023-07-11 10:05:47,237:INFO:Initializing create_model()
2023-07-11 10:05:47,237:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1DD3B4EE0>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D98BB070>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 10:05:47,237:INFO:Checking exceptions
2023-07-11 10:05:47,237:INFO:Importing libraries
2023-07-11 10:05:47,237:INFO:Copying training dataset
2023-07-11 10:05:47,254:INFO:Defining folds
2023-07-11 10:05:47,254:INFO:Declaring metric variables
2023-07-11 10:05:47,254:INFO:Importing untrained model
2023-07-11 10:05:47,254:INFO:Random Forest Regressor Imported successfully
2023-07-11 10:05:47,254:INFO:Starting cross validation
2023-07-11 10:05:47,255:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 10:05:51,528:WARNING:create_model() for rf raised an exception or returned all 0.0, trying without fit_kwargs:
2023-07-11 10:05:51,528:WARNING:Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 792, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\ensemble\_forest.py", line 345, in fit
    X, y = self._validate_data(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1106, in check_X_y
    X = check_array(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 778, in check_array
    dtype_orig = np.result_type(*dtypes_orig)
  File "<__array_function__ internals>", line 5, in result_type
ValueError: at least one array or dtype is required


2023-07-11 10:05:51,528:INFO:Initializing create_model()
2023-07-11 10:05:51,528:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1DD3B4EE0>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D98BB070>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 10:05:51,528:INFO:Checking exceptions
2023-07-11 10:05:51,528:INFO:Importing libraries
2023-07-11 10:05:51,528:INFO:Copying training dataset
2023-07-11 10:05:51,528:INFO:Defining folds
2023-07-11 10:05:51,528:INFO:Declaring metric variables
2023-07-11 10:05:51,544:INFO:Importing untrained model
2023-07-11 10:05:51,544:INFO:Random Forest Regressor Imported successfully
2023-07-11 10:05:51,544:INFO:Starting cross validation
2023-07-11 10:05:51,544:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 10:05:56,142:ERROR:create_model() for rf raised an exception or returned all 0.0:
2023-07-11 10:05:56,142:ERROR:Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 792, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\ensemble\_forest.py", line 345, in fit
    X, y = self._validate_data(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1106, in check_X_y
    X = check_array(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 778, in check_array
    dtype_orig = np.result_type(*dtypes_orig)
  File "<__array_function__ internals>", line 5, in result_type
ValueError: at least one array or dtype is required


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 808, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\ensemble\_forest.py", line 345, in fit
    X, y = self._validate_data(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1106, in check_X_y
    X = check_array(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 778, in check_array
    dtype_orig = np.result_type(*dtypes_orig)
  File "<__array_function__ internals>", line 5, in result_type
ValueError: at least one array or dtype is required


2023-07-11 10:05:57,236:INFO:Initializing Extra Trees Regressor
2023-07-11 10:05:57,236:INFO:Total runtime is 2.1572770953178404 minutes
2023-07-11 10:05:57,236:INFO:SubProcess create_model() called ==================================
2023-07-11 10:05:57,236:INFO:Initializing create_model()
2023-07-11 10:05:57,236:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1DD3B4EE0>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D98BB070>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 10:05:57,236:INFO:Checking exceptions
2023-07-11 10:05:57,236:INFO:Importing libraries
2023-07-11 10:05:57,236:INFO:Copying training dataset
2023-07-11 10:05:57,252:INFO:Defining folds
2023-07-11 10:05:57,252:INFO:Declaring metric variables
2023-07-11 10:05:57,253:INFO:Importing untrained model
2023-07-11 10:05:57,253:INFO:Extra Trees Regressor Imported successfully
2023-07-11 10:05:57,253:INFO:Starting cross validation
2023-07-11 10:05:57,253:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 10:06:01,610:WARNING:create_model() for et raised an exception or returned all 0.0, trying without fit_kwargs:
2023-07-11 10:06:01,611:WARNING:Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 792, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\ensemble\_forest.py", line 345, in fit
    X, y = self._validate_data(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1106, in check_X_y
    X = check_array(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 778, in check_array
    dtype_orig = np.result_type(*dtypes_orig)
  File "<__array_function__ internals>", line 5, in result_type
ValueError: at least one array or dtype is required


2023-07-11 10:06:01,611:INFO:Initializing create_model()
2023-07-11 10:06:01,611:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1DD3B4EE0>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D98BB070>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 10:06:01,611:INFO:Checking exceptions
2023-07-11 10:06:01,611:INFO:Importing libraries
2023-07-11 10:06:01,611:INFO:Copying training dataset
2023-07-11 10:06:01,614:INFO:Defining folds
2023-07-11 10:06:01,614:INFO:Declaring metric variables
2023-07-11 10:06:01,614:INFO:Importing untrained model
2023-07-11 10:06:01,614:INFO:Extra Trees Regressor Imported successfully
2023-07-11 10:06:01,615:INFO:Starting cross validation
2023-07-11 10:06:01,615:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 10:06:06,483:ERROR:create_model() for et raised an exception or returned all 0.0:
2023-07-11 10:06:06,483:ERROR:Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 792, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\ensemble\_forest.py", line 345, in fit
    X, y = self._validate_data(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1106, in check_X_y
    X = check_array(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 778, in check_array
    dtype_orig = np.result_type(*dtypes_orig)
  File "<__array_function__ internals>", line 5, in result_type
ValueError: at least one array or dtype is required


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 808, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\ensemble\_forest.py", line 345, in fit
    X, y = self._validate_data(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1106, in check_X_y
    X = check_array(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 778, in check_array
    dtype_orig = np.result_type(*dtypes_orig)
  File "<__array_function__ internals>", line 5, in result_type
ValueError: at least one array or dtype is required


2023-07-11 10:06:07,652:INFO:Initializing AdaBoost Regressor
2023-07-11 10:06:07,652:INFO:Total runtime is 2.330883502960205 minutes
2023-07-11 10:06:07,652:INFO:SubProcess create_model() called ==================================
2023-07-11 10:06:07,652:INFO:Initializing create_model()
2023-07-11 10:06:07,652:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1DD3B4EE0>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D98BB070>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 10:06:07,652:INFO:Checking exceptions
2023-07-11 10:06:07,652:INFO:Importing libraries
2023-07-11 10:06:07,652:INFO:Copying training dataset
2023-07-11 10:06:07,652:INFO:Defining folds
2023-07-11 10:06:07,652:INFO:Declaring metric variables
2023-07-11 10:06:07,652:INFO:Importing untrained model
2023-07-11 10:06:07,652:INFO:AdaBoost Regressor Imported successfully
2023-07-11 10:06:07,652:INFO:Starting cross validation
2023-07-11 10:06:07,652:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 10:06:12,430:WARNING:create_model() for ada raised an exception or returned all 0.0, trying without fit_kwargs:
2023-07-11 10:06:12,430:WARNING:Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 792, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\ensemble\_weight_boosting.py", line 126, in fit
    X, y = self._validate_data(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1106, in check_X_y
    X = check_array(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 778, in check_array
    dtype_orig = np.result_type(*dtypes_orig)
  File "<__array_function__ internals>", line 5, in result_type
ValueError: at least one array or dtype is required


2023-07-11 10:06:12,430:INFO:Initializing create_model()
2023-07-11 10:06:12,430:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1DD3B4EE0>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D98BB070>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 10:06:12,430:INFO:Checking exceptions
2023-07-11 10:06:12,430:INFO:Importing libraries
2023-07-11 10:06:12,430:INFO:Copying training dataset
2023-07-11 10:06:12,430:INFO:Defining folds
2023-07-11 10:06:12,430:INFO:Declaring metric variables
2023-07-11 10:06:12,430:INFO:Importing untrained model
2023-07-11 10:06:12,430:INFO:AdaBoost Regressor Imported successfully
2023-07-11 10:06:12,430:INFO:Starting cross validation
2023-07-11 10:06:12,430:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 10:06:17,044:WARNING:Summarize dataset:   0%|                                                                         | 0/5 [00:00<?, ?it/s]
2023-07-11 10:06:17,079:WARNING:Summarize dataset:   0%|                                             | 0/38 [00:00<?, ?it/s, Describe variable:address]
2023-07-11 10:06:17,145:WARNING:Summarize dataset:   3%|9                                    | 1/38 [00:00<00:03, 10.02it/s, Describe variable:famsize]
2023-07-11 10:06:17,145:WARNING:Summarize dataset:   5%|##1                                      | 2/38 [00:00<00:01, 20.04it/s, Describe variable:sex]
2023-07-11 10:06:17,145:WARNING:Summarize dataset:   8%|##9                                  | 3/38 [00:00<00:01, 30.06it/s, Describe variable:Pstatus]
2023-07-11 10:06:17,145:WARNING:Summarize dataset:  11%|####                                  | 4/38 [00:00<00:00, 40.08it/s, Describe variable:school]
2023-07-11 10:06:17,164:WARNING:Summarize dataset:  13%|#####2                                  | 5/38 [00:00<00:00, 42.12it/s, Describe variable:Fjob]
2023-07-11 10:06:17,164:WARNING:Summarize dataset:  16%|######3                                 | 6/38 [00:00<00:00, 50.54it/s, Describe variable:Fjob]
2023-07-11 10:06:17,164:WARNING:Summarize dataset:  16%|######3                                 | 6/38 [00:00<00:00, 50.54it/s, Describe variable:Mjob]
2023-07-11 10:06:17,196:WARNING:Summarize dataset:  18%|#######                               | 7/38 [00:00<00:00, 50.54it/s, Describe variable:famsup]
2023-07-11 10:06:17,196:WARNING:Summarize dataset:  21%|#######3                           | 8/38 [00:00<00:00, 50.54it/s, Describe variable:schoolsup]
2023-07-11 10:06:17,216:WARNING:Summarize dataset:  24%|########5                           | 9/38 [00:00<00:00, 50.54it/s, Describe variable:guardian]
2023-07-11 10:06:17,220:WARNING:Summarize dataset:  26%|##########2                            | 10/38 [00:00<00:00, 50.54it/s, Describe variable:paid]
2023-07-11 10:06:17,247:WARNING:Summarize dataset:  29%|###########2                           | 11/38 [00:00<00:00, 50.54it/s, Describe variable:Medu]
2023-07-11 10:06:17,280:WARNING:Summarize dataset:  32%|############3                          | 12/38 [00:00<00:00, 51.16it/s, Describe variable:Medu]
2023-07-11 10:06:17,290:WARNING:Summarize dataset:  32%|###########                        | 12/38 [00:00<00:00, 51.16it/s, Describe variable:internet]
2023-07-11 10:06:17,305:WARNING:Summarize dataset:  34%|############6                        | 13/38 [00:00<00:00, 51.16it/s, Describe variable:higher]
2023-07-11 10:06:17,306:WARNING:Summarize dataset:  37%|#############2                      | 14/38 [00:00<00:00, 51.16it/s, Describe variable:nursery]
2023-07-11 10:06:17,312:WARNING:Summarize dataset:  39%|#############                    | 15/38 [00:00<00:00, 51.16it/s, Describe variable:traveltime]
2023-07-11 10:06:17,329:WARNING:Summarize dataset:  42%|#############8                   | 16/38 [00:00<00:00, 51.16it/s, Describe variable:activities]
2023-07-11 10:06:17,329:WARNING:Summarize dataset:  45%|################5                    | 17/38 [00:00<00:00, 51.16it/s, Describe variable:reason]
2023-07-11 10:06:17,329:WARNING:Summarize dataset:  47%|#################5                   | 18/38 [00:00<00:00, 51.16it/s, Describe variable:famrel]
2023-07-11 10:06:17,329:WARNING:Summarize dataset:  50%|###################5                   | 19/38 [00:00<00:00, 51.16it/s, Describe variable:Fedu]
2023-07-11 10:06:17,345:WARNING:Summarize dataset:  53%|#################8                | 20/38 [00:00<00:00, 51.16it/s, Describe variable:studytime]
2023-07-11 10:06:17,345:WARNING:Summarize dataset:  55%|###################3               | 21/38 [00:00<00:00, 51.16it/s, Describe variable:romantic]
2023-07-11 10:06:17,345:WARNING:Summarize dataset:  58%|######################5                | 22/38 [00:00<00:00, 51.16it/s, Describe variable:Walc]
2023-07-11 10:06:17,345:WARNING:Summarize dataset:  61%|#######################               | 23/38 [00:00<00:00, 51.16it/s, Describe variable:goout]
2023-07-11 10:06:17,360:WARNING:Summarize dataset:  63%|#######################3             | 24/38 [00:00<00:00, 51.16it/s, Describe variable:health]
2023-07-11 10:06:17,360:WARNING:Summarize dataset:  66%|#########################6             | 25/38 [00:00<00:00, 51.16it/s, Describe variable:Dalc]
2023-07-11 10:06:17,360:WARNING:Summarize dataset:  68%|#######################9           | 26/38 [00:00<00:00, 51.16it/s, Describe variable:freetime]
2023-07-11 10:06:17,360:WARNING:Summarize dataset:  71%|########################8          | 27/38 [00:00<00:00, 51.16it/s, Describe variable:failures]
2023-07-11 10:06:17,360:WARNING:Summarize dataset:  74%|#############################4          | 28/38 [00:00<00:00, 51.16it/s, Describe variable:age]
2023-07-11 10:06:17,360:WARNING:Summarize dataset:  76%|##########################7        | 29/38 [00:00<00:00, 51.16it/s, Describe variable:absences]
2023-07-11 10:06:17,360:WARNING:Summarize dataset:  79%|################################3        | 30/38 [00:00<00:00, 51.16it/s, Describe variable:G2]
2023-07-11 10:06:17,360:WARNING:Summarize dataset:  82%|#################################4       | 31/38 [00:00<00:00, 51.16it/s, Describe variable:G3]
2023-07-11 10:06:17,360:WARNING:Summarize dataset:  84%|##################################5      | 32/38 [00:00<00:00, 51.16it/s, Describe variable:G1]
2023-07-11 10:06:17,360:WARNING:Summarize dataset:  87%|#####################################3     | 33/38 [00:00<00:00, 51.16it/s, Get variable types]
2023-07-11 10:06:17,360:WARNING:Summarize dataset:  85%|#############################7     | 34/40 [00:00<00:00, 51.16it/s, Calculate auto correlation]
2023-07-11 10:06:19,978:WARNING:Summarize dataset:  88%|##############################6    | 35/40 [00:02<00:00, 10.68it/s, Calculate auto correlation]
2023-07-11 10:06:19,978:WARNING:Summarize dataset:  88%|#####################################6     | 35/40 [00:02<00:00, 10.68it/s, Get scatter matrix]
2023-07-11 10:06:19,978:WARNING:Summarize dataset:  54%|########################2                    | 35/65 [00:02<00:02, 10.68it/s, scatter age, age]
2023-07-11 10:06:20,097:WARNING:Summarize dataset:  55%|######################1                 | 36/65 [00:03<00:02, 10.68it/s, scatter absences, age]
2023-07-11 10:06:20,212:WARNING:Summarize dataset:  57%|##########################1                   | 37/65 [00:03<00:02, 10.68it/s, scatter G1, age]
2023-07-11 10:06:20,330:WARNING:Summarize dataset:  58%|##########################8                   | 38/65 [00:03<00:02, 10.68it/s, scatter G2, age]
2023-07-11 10:06:20,446:WARNING:Summarize dataset:  60%|###########################6                  | 39/65 [00:03<00:02, 10.21it/s, scatter G2, age]
2023-07-11 10:06:20,446:WARNING:Summarize dataset:  60%|###########################6                  | 39/65 [00:03<00:02, 10.21it/s, scatter G3, age]
2023-07-11 10:06:20,571:WARNING:Summarize dataset:  62%|########################6               | 40/65 [00:03<00:02, 10.21it/s, scatter age, absences]
2023-07-11 10:06:20,682:WARNING:Summarize dataset:  63%|######################             | 41/65 [00:03<00:02, 10.21it/s, scatter absences, absences]
2023-07-11 10:06:20,793:WARNING:Summarize dataset:  65%|######################6            | 42/65 [00:03<00:02,  9.95it/s, scatter absences, absences]
2023-07-11 10:06:20,793:WARNING:Summarize dataset:  65%|##########################4              | 42/65 [00:03<00:02,  9.95it/s, scatter G1, absences]
2023-07-11 10:06:20,908:WARNING:Summarize dataset:  66%|###########################1             | 43/65 [00:03<00:02,  9.95it/s, scatter G2, absences]
2023-07-11 10:06:21,031:WARNING:Summarize dataset:  68%|###########################7             | 44/65 [00:03<00:02,  9.73it/s, scatter G2, absences]
2023-07-11 10:06:21,031:WARNING:Summarize dataset:  68%|###########################7             | 44/65 [00:03<00:02,  9.73it/s, scatter G3, absences]
2023-07-11 10:06:21,163:WARNING:Summarize dataset:  69%|###############################8              | 45/65 [00:04<00:02,  9.73it/s, scatter age, G1]
2023-07-11 10:06:21,296:WARNING:Summarize dataset:  71%|################################5             | 46/65 [00:04<00:02,  9.33it/s, scatter age, G1]
2023-07-11 10:06:21,296:WARNING:Summarize dataset:  71%|#############################            | 46/65 [00:04<00:02,  9.33it/s, scatter absences, G1]
2023-07-11 10:06:21,429:WARNING:Summarize dataset:  72%|#################################9             | 47/65 [00:04<00:01,  9.33it/s, scatter G1, G1]
2023-07-11 10:06:21,563:WARNING:Summarize dataset:  74%|##################################7            | 48/65 [00:04<00:01,  8.95it/s, scatter G1, G1]
2023-07-11 10:06:21,563:WARNING:Summarize dataset:  74%|##################################7            | 48/65 [00:04<00:01,  8.95it/s, scatter G2, G1]
2023-07-11 10:06:21,680:WARNING:Summarize dataset:  75%|###################################4           | 49/65 [00:04<00:01,  8.95it/s, scatter G3, G1]
2023-07-11 10:06:21,813:WARNING:Summarize dataset:  77%|####################################1          | 50/65 [00:04<00:01,  8.74it/s, scatter G3, G1]
2023-07-11 10:06:21,813:WARNING:Summarize dataset:  77%|###################################3          | 50/65 [00:04<00:01,  8.74it/s, scatter age, G2]
2023-07-11 10:06:21,930:WARNING:Summarize dataset:  78%|####################################          | 51/65 [00:04<00:01,  8.72it/s, scatter age, G2]
2023-07-11 10:06:21,930:WARNING:Summarize dataset:  78%|################################1        | 51/65 [00:04<00:01,  8.72it/s, scatter absences, G2]
2023-07-11 10:06:22,069:WARNING:Summarize dataset:  80%|################################8        | 52/65 [00:05<00:01,  8.44it/s, scatter absences, G2]
2023-07-11 10:06:22,069:WARNING:Summarize dataset:  80%|#####################################6         | 52/65 [00:05<00:01,  8.44it/s, scatter G1, G2]
2023-07-11 10:06:22,196:WARNING:Summarize dataset:  82%|######################################3        | 53/65 [00:05<00:01,  8.33it/s, scatter G1, G2]
2023-07-11 10:06:22,196:WARNING:Summarize dataset:  82%|######################################3        | 53/65 [00:05<00:01,  8.33it/s, scatter G2, G2]
2023-07-11 10:06:22,329:WARNING:Summarize dataset:  83%|#######################################        | 54/65 [00:05<00:01,  8.15it/s, scatter G2, G2]
2023-07-11 10:06:22,329:WARNING:Summarize dataset:  83%|#######################################        | 54/65 [00:05<00:01,  8.15it/s, scatter G3, G2]
2023-07-11 10:06:22,429:WARNING:Summarize dataset:  85%|######################################9       | 55/65 [00:05<00:01,  8.15it/s, scatter age, G3]
2023-07-11 10:06:22,571:WARNING:Summarize dataset:  86%|#######################################6      | 56/65 [00:05<00:01,  8.20it/s, scatter age, G3]
2023-07-11 10:06:22,571:WARNING:Summarize dataset:  86%|###################################3     | 56/65 [00:05<00:01,  8.20it/s, scatter absences, G3]
2023-07-11 10:06:22,697:WARNING:Summarize dataset:  88%|###################################9     | 57/65 [00:05<00:00,  8.14it/s, scatter absences, G3]
2023-07-11 10:06:22,697:WARNING:Summarize dataset:  88%|#########################################2     | 57/65 [00:05<00:00,  8.14it/s, scatter G1, G3]
2023-07-11 10:06:22,820:WARNING:Summarize dataset:  89%|#########################################9     | 58/65 [00:05<00:00,  8.14it/s, scatter G1, G3]
2023-07-11 10:06:22,820:WARNING:Summarize dataset:  89%|#########################################9     | 58/65 [00:05<00:00,  8.14it/s, scatter G2, G3]
2023-07-11 10:06:22,930:WARNING:Summarize dataset:  91%|##########################################6    | 59/65 [00:05<00:00,  8.36it/s, scatter G2, G3]
2023-07-11 10:06:22,930:WARNING:Summarize dataset:  91%|##########################################6    | 59/65 [00:05<00:00,  8.36it/s, scatter G3, G3]
2023-07-11 10:06:23,064:WARNING:Summarize dataset:  92%|###########################################3   | 60/65 [00:06<00:00,  8.11it/s, scatter G3, G3]
2023-07-11 10:06:23,064:WARNING:Summarize dataset:  92%|##################################1  | 60/65 [00:06<00:00,  8.11it/s, Get dataframe statistics]
2023-07-11 10:06:23,064:WARNING:Summarize dataset:  91%|######################################2   | 61/67 [00:06<00:00,  8.11it/s, Missing diagram bar]
2023-07-11 10:06:23,614:WARNING:Summarize dataset:  93%|######################################8   | 62/67 [00:06<00:00,  5.32it/s, Missing diagram bar]
2023-07-11 10:06:23,614:WARNING:Summarize dataset:  93%|####################################   | 62/67 [00:06<00:00,  5.32it/s, Missing diagram matrix]
2023-07-11 10:06:23,836:WARNING:Summarize dataset:  94%|####################################6  | 63/67 [00:06<00:00,  5.11it/s, Missing diagram matrix]
2023-07-11 10:06:23,836:WARNING:Summarize dataset:  94%|###############################################   | 63/67 [00:06<00:00,  5.11it/s, Take sample]
2023-07-11 10:06:23,836:WARNING:Summarize dataset:  96%|#######################################1 | 64/67 [00:06<00:00,  5.11it/s, Detecting duplicates]
2023-07-11 10:06:23,846:WARNING:Summarize dataset:  97%|#################################################4 | 65/67 [00:06<00:00,  5.11it/s, Get alerts]
2023-07-11 10:06:23,846:WARNING:Summarize dataset:  99%|####################################4| 66/67 [00:06<00:00,  5.11it/s, Get reproduction details]
2023-07-11 10:06:23,846:WARNING:Summarize dataset: 100%|####################################################| 67/67 [00:06<00:00,  5.11it/s, Completed]
2023-07-11 10:06:23,846:WARNING:Summarize dataset: 100%|####################################################| 67/67 [00:06<00:00,  9.85it/s, Completed]
2023-07-11 10:06:23,846:WARNING:
2023-07-11 10:06:23,846:WARNING:
2023-07-11 10:06:23,846:WARNING:Generate report structure:   0%|                                                                 | 0/1 [00:00<?, ?it/s]
2023-07-11 10:06:23,846:WARNING:[A
2023-07-11 10:06:26,913:ERROR:create_model() for ada raised an exception or returned all 0.0:
2023-07-11 10:06:26,929:ERROR:Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 792, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\ensemble\_weight_boosting.py", line 126, in fit
    X, y = self._validate_data(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1106, in check_X_y
    X = check_array(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 778, in check_array
    dtype_orig = np.result_type(*dtypes_orig)
  File "<__array_function__ internals>", line 5, in result_type
ValueError: at least one array or dtype is required


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 808, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\ensemble\_weight_boosting.py", line 126, in fit
    X, y = self._validate_data(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1106, in check_X_y
    X = check_array(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 778, in check_array
    dtype_orig = np.result_type(*dtypes_orig)
  File "<__array_function__ internals>", line 5, in result_type
ValueError: at least one array or dtype is required


2023-07-11 10:06:31,512:WARNING:
2023-07-11 10:06:31,512:WARNING:Generate report structure: 100%|#########################################################| 1/1 [00:07<00:00,  7.67s/it]
2023-07-11 10:06:31,512:WARNING:[A
2023-07-11 10:06:31,512:WARNING:Generate report structure: 100%|#########################################################| 1/1 [00:07<00:00,  7.67s/it]
2023-07-11 10:06:31,512:WARNING:
2023-07-11 10:06:33,444:INFO:Initializing Gradient Boosting Regressor
2023-07-11 10:06:33,444:INFO:Total runtime is 2.760740633805593 minutes
2023-07-11 10:06:33,444:INFO:SubProcess create_model() called ==================================
2023-07-11 10:06:33,444:INFO:Initializing create_model()
2023-07-11 10:06:33,444:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1DD3B4EE0>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D98BB070>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 10:06:33,444:INFO:Checking exceptions
2023-07-11 10:06:33,444:INFO:Importing libraries
2023-07-11 10:06:33,445:INFO:Copying training dataset
2023-07-11 10:06:33,447:INFO:Defining folds
2023-07-11 10:06:33,447:INFO:Declaring metric variables
2023-07-11 10:06:33,447:INFO:Importing untrained model
2023-07-11 10:06:33,447:INFO:Gradient Boosting Regressor Imported successfully
2023-07-11 10:06:33,448:INFO:Starting cross validation
2023-07-11 10:06:33,448:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 10:06:37,905:WARNING:create_model() for gbr raised an exception or returned all 0.0, trying without fit_kwargs:
2023-07-11 10:06:37,905:WARNING:Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 792, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\ensemble\_gb.py", line 429, in fit
    X, y = self._validate_data(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1106, in check_X_y
    X = check_array(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 778, in check_array
    dtype_orig = np.result_type(*dtypes_orig)
  File "<__array_function__ internals>", line 5, in result_type
ValueError: at least one array or dtype is required


2023-07-11 10:06:37,905:INFO:Initializing create_model()
2023-07-11 10:06:37,905:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1DD3B4EE0>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D98BB070>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 10:06:37,905:INFO:Checking exceptions
2023-07-11 10:06:37,905:INFO:Importing libraries
2023-07-11 10:06:37,905:INFO:Copying training dataset
2023-07-11 10:06:37,920:INFO:Defining folds
2023-07-11 10:06:37,920:INFO:Declaring metric variables
2023-07-11 10:06:37,920:INFO:Importing untrained model
2023-07-11 10:06:37,920:INFO:Gradient Boosting Regressor Imported successfully
2023-07-11 10:06:37,920:INFO:Starting cross validation
2023-07-11 10:06:37,920:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 10:06:42,731:ERROR:create_model() for gbr raised an exception or returned all 0.0:
2023-07-11 10:06:42,731:ERROR:Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 792, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\ensemble\_gb.py", line 429, in fit
    X, y = self._validate_data(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1106, in check_X_y
    X = check_array(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 778, in check_array
    dtype_orig = np.result_type(*dtypes_orig)
  File "<__array_function__ internals>", line 5, in result_type
ValueError: at least one array or dtype is required


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 808, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\ensemble\_gb.py", line 429, in fit
    X, y = self._validate_data(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\base.py", line 584, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 1106, in check_X_y
    X = check_array(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 778, in check_array
    dtype_orig = np.result_type(*dtypes_orig)
  File "<__array_function__ internals>", line 5, in result_type
ValueError: at least one array or dtype is required


2023-07-11 10:06:44,000:INFO:Initializing Extreme Gradient Boosting
2023-07-11 10:06:44,001:INFO:Total runtime is 2.9366884271303815 minutes
2023-07-11 10:06:44,001:INFO:SubProcess create_model() called ==================================
2023-07-11 10:06:44,001:INFO:Initializing create_model()
2023-07-11 10:06:44,001:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1DD3B4EE0>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D98BB070>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 10:06:44,001:INFO:Checking exceptions
2023-07-11 10:06:44,001:INFO:Importing libraries
2023-07-11 10:06:44,001:INFO:Copying training dataset
2023-07-11 10:06:44,003:INFO:Defining folds
2023-07-11 10:06:44,003:INFO:Declaring metric variables
2023-07-11 10:06:44,003:INFO:Importing untrained model
2023-07-11 10:06:44,004:INFO:Extreme Gradient Boosting Imported successfully
2023-07-11 10:06:44,004:INFO:Starting cross validation
2023-07-11 10:06:44,004:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 10:06:48,481:WARNING:create_model() for xgboost raised an exception or returned all 0.0, trying without fit_kwargs:
2023-07-11 10:06:48,481:WARNING:Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 792, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pandas\core\arrays\categorical.py", line 551, in astype
    new_cats = new_cats.astype(dtype=dtype, copy=copy)
ValueError: could not convert string to float: 'GP;"F";15;"R";"GT3";"T";1;1;"at_home";"other";"home";"mother";2;4;1;"yes";"yes";"yes";"yes";"yes";"yes";"yes";"no";3;1;2;1;1;1;2;"7";"10";10'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\core.py", line 620, in inner_f
    return func(**kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\sklearn.py", line 988, in fit
    train_dmatrix, evals = _wrap_evaluation_matrices(
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\sklearn.py", line 448, in _wrap_evaluation_matrices
    train_dmatrix = create_dmatrix(
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\sklearn.py", line 908, in _create_dmatrix
    return DMatrix(**kwargs, nthread=self.n_jobs)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\core.py", line 620, in inner_f
    return func(**kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\core.py", line 754, in __init__
    self.set_info(
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\core.py", line 620, in inner_f
    return func(**kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\core.py", line 819, in set_info
    self.set_label(label)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\core.py", line 950, in set_label
    dispatch_meta_backend(self, label, 'label', 'float')
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\data.py", line 1134, in dispatch_meta_backend
    _meta_from_pandas_series(data, name, dtype, handle)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\data.py", line 438, in _meta_from_pandas_series
    data = data.values.astype('float')
  File "C:\Users\didit\anaconda3\lib\site-packages\pandas\core\arrays\categorical.py", line 562, in astype
    raise ValueError(msg)
ValueError: Cannot cast object dtype to float64


2023-07-11 10:06:48,481:INFO:Initializing create_model()
2023-07-11 10:06:48,481:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1DD3B4EE0>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D98BB070>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 10:06:48,481:INFO:Checking exceptions
2023-07-11 10:06:48,481:INFO:Importing libraries
2023-07-11 10:06:48,481:INFO:Copying training dataset
2023-07-11 10:06:48,481:INFO:Defining folds
2023-07-11 10:06:48,481:INFO:Declaring metric variables
2023-07-11 10:06:48,481:INFO:Importing untrained model
2023-07-11 10:06:48,481:INFO:Extreme Gradient Boosting Imported successfully
2023-07-11 10:06:48,481:INFO:Starting cross validation
2023-07-11 10:06:48,481:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 10:06:53,476:ERROR:create_model() for xgboost raised an exception or returned all 0.0:
2023-07-11 10:06:53,479:ERROR:Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 792, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pandas\core\arrays\categorical.py", line 551, in astype
    new_cats = new_cats.astype(dtype=dtype, copy=copy)
ValueError: could not convert string to float: 'GP;"F";15;"R";"GT3";"T";1;1;"at_home";"other";"home";"mother";2;4;1;"yes";"yes";"yes";"yes";"yes";"yes";"yes";"no";3;1;2;1;1;1;2;"7";"10";10'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\core.py", line 620, in inner_f
    return func(**kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\sklearn.py", line 988, in fit
    train_dmatrix, evals = _wrap_evaluation_matrices(
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\sklearn.py", line 448, in _wrap_evaluation_matrices
    train_dmatrix = create_dmatrix(
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\sklearn.py", line 908, in _create_dmatrix
    return DMatrix(**kwargs, nthread=self.n_jobs)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\core.py", line 620, in inner_f
    return func(**kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\core.py", line 754, in __init__
    self.set_info(
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\core.py", line 620, in inner_f
    return func(**kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\core.py", line 819, in set_info
    self.set_label(label)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\core.py", line 950, in set_label
    dispatch_meta_backend(self, label, 'label', 'float')
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\data.py", line 1134, in dispatch_meta_backend
    _meta_from_pandas_series(data, name, dtype, handle)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\data.py", line 438, in _meta_from_pandas_series
    data = data.values.astype('float')
  File "C:\Users\didit\anaconda3\lib\site-packages\pandas\core\arrays\categorical.py", line 562, in astype
    raise ValueError(msg)
ValueError: Cannot cast object dtype to float64


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 808, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pandas\core\arrays\categorical.py", line 551, in astype
    new_cats = new_cats.astype(dtype=dtype, copy=copy)
ValueError: could not convert string to float: 'GP;"F";15;"R";"GT3";"T";1;1;"at_home";"other";"home";"mother";2;4;1;"yes";"yes";"yes";"yes";"yes";"yes";"yes";"no";3;1;2;1;1;1;2;"7";"10";10'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\core.py", line 620, in inner_f
    return func(**kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\sklearn.py", line 988, in fit
    train_dmatrix, evals = _wrap_evaluation_matrices(
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\sklearn.py", line 448, in _wrap_evaluation_matrices
    train_dmatrix = create_dmatrix(
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\sklearn.py", line 908, in _create_dmatrix
    return DMatrix(**kwargs, nthread=self.n_jobs)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\core.py", line 620, in inner_f
    return func(**kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\core.py", line 754, in __init__
    self.set_info(
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\core.py", line 620, in inner_f
    return func(**kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\core.py", line 819, in set_info
    self.set_label(label)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\core.py", line 950, in set_label
    dispatch_meta_backend(self, label, 'label', 'float')
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\data.py", line 1134, in dispatch_meta_backend
    _meta_from_pandas_series(data, name, dtype, handle)
  File "C:\Users\didit\anaconda3\lib\site-packages\xgboost\data.py", line 438, in _meta_from_pandas_series
    data = data.values.astype('float')
  File "C:\Users\didit\anaconda3\lib\site-packages\pandas\core\arrays\categorical.py", line 562, in astype
    raise ValueError(msg)
ValueError: Cannot cast object dtype to float64


2023-07-11 10:06:54,566:INFO:Initializing Light Gradient Boosting Machine
2023-07-11 10:06:54,566:INFO:Total runtime is 3.1127702593803406 minutes
2023-07-11 10:06:54,566:INFO:SubProcess create_model() called ==================================
2023-07-11 10:06:54,566:INFO:Initializing create_model()
2023-07-11 10:06:54,566:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1DD3B4EE0>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D98BB070>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 10:06:54,566:INFO:Checking exceptions
2023-07-11 10:06:54,566:INFO:Importing libraries
2023-07-11 10:06:54,566:INFO:Copying training dataset
2023-07-11 10:06:54,566:INFO:Defining folds
2023-07-11 10:06:54,566:INFO:Declaring metric variables
2023-07-11 10:06:54,566:INFO:Importing untrained model
2023-07-11 10:06:54,566:INFO:Light Gradient Boosting Machine Imported successfully
2023-07-11 10:06:54,566:INFO:Starting cross validation
2023-07-11 10:06:54,566:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 10:06:59,032:WARNING:create_model() for lightgbm raised an exception or returned all 0.0, trying without fit_kwargs:
2023-07-11 10:06:59,032:WARNING:Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 792, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\sklearn.py", line 895, in fit
    super().fit(X, y, sample_weight=sample_weight, init_score=init_score,
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\sklearn.py", line 748, in fit
    self._Booster = train(
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\engine.py", line 271, in train
    booster = Booster(params=params, train_set=train_set)
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 2605, in __init__
    train_set.construct()
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 1815, in construct
    self._lazy_init(self.data, label=self.label,
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 1557, in _lazy_init
    self.set_label(label)
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 2163, in set_label
    label = list_to_1d_numpy(_label_from_pandas(label), name='label')
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 186, in list_to_1d_numpy
    raise ValueError('Series.dtypes must be int, float or bool')
ValueError: Series.dtypes must be int, float or bool


2023-07-11 10:06:59,032:INFO:Initializing create_model()
2023-07-11 10:06:59,032:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1DD3B4EE0>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D98BB070>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 10:06:59,032:INFO:Checking exceptions
2023-07-11 10:06:59,032:INFO:Importing libraries
2023-07-11 10:06:59,032:INFO:Copying training dataset
2023-07-11 10:06:59,032:INFO:Defining folds
2023-07-11 10:06:59,032:INFO:Declaring metric variables
2023-07-11 10:06:59,032:INFO:Importing untrained model
2023-07-11 10:06:59,032:INFO:Light Gradient Boosting Machine Imported successfully
2023-07-11 10:06:59,032:INFO:Starting cross validation
2023-07-11 10:06:59,032:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 10:07:03,807:ERROR:create_model() for lightgbm raised an exception or returned all 0.0:
2023-07-11 10:07:03,807:ERROR:Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 792, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\sklearn.py", line 895, in fit
    super().fit(X, y, sample_weight=sample_weight, init_score=init_score,
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\sklearn.py", line 748, in fit
    self._Booster = train(
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\engine.py", line 271, in train
    booster = Booster(params=params, train_set=train_set)
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 2605, in __init__
    train_set.construct()
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 1815, in construct
    self._lazy_init(self.data, label=self.label,
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 1557, in _lazy_init
    self.set_label(label)
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 2163, in set_label
    label = list_to_1d_numpy(_label_from_pandas(label), name='label')
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 186, in list_to_1d_numpy
    raise ValueError('Series.dtypes must be int, float or bool')
ValueError: Series.dtypes must be int, float or bool


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 808, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\sklearn.py", line 895, in fit
    super().fit(X, y, sample_weight=sample_weight, init_score=init_score,
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\sklearn.py", line 748, in fit
    self._Booster = train(
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\engine.py", line 271, in train
    booster = Booster(params=params, train_set=train_set)
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 2605, in __init__
    train_set.construct()
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 1815, in construct
    self._lazy_init(self.data, label=self.label,
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 1557, in _lazy_init
    self.set_label(label)
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 2163, in set_label
    label = list_to_1d_numpy(_label_from_pandas(label), name='label')
  File "C:\Users\didit\anaconda3\lib\site-packages\lightgbm\basic.py", line 186, in list_to_1d_numpy
    raise ValueError('Series.dtypes must be int, float or bool')
ValueError: Series.dtypes must be int, float or bool


2023-07-11 10:07:04,987:INFO:Initializing Dummy Regressor
2023-07-11 10:07:04,987:INFO:Total runtime is 3.2864659309387205 minutes
2023-07-11 10:07:04,987:INFO:SubProcess create_model() called ==================================
2023-07-11 10:07:04,987:INFO:Initializing create_model()
2023-07-11 10:07:04,987:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1DD3B4EE0>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D98BB070>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 10:07:04,987:INFO:Checking exceptions
2023-07-11 10:07:04,987:INFO:Importing libraries
2023-07-11 10:07:04,987:INFO:Copying training dataset
2023-07-11 10:07:04,990:INFO:Defining folds
2023-07-11 10:07:04,990:INFO:Declaring metric variables
2023-07-11 10:07:04,990:INFO:Importing untrained model
2023-07-11 10:07:04,991:INFO:Dummy Regressor Imported successfully
2023-07-11 10:07:04,991:INFO:Starting cross validation
2023-07-11 10:07:04,992:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 10:07:09,581:WARNING:create_model() for dummy raised an exception or returned all 0.0, trying without fit_kwargs:
2023-07-11 10:07:09,581:WARNING:Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 792, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\dummy.py", line 554, in fit
    self.constant_ = np.average(y, axis=0, weights=sample_weight)
  File "<__array_function__ internals>", line 5, in average
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\lib\function_base.py", line 380, in average
    avg = a.mean(axis)
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\_methods.py", line 181, in _mean
    ret = um.true_divide(
TypeError: unsupported operand type(s) for /: 'str' and 'int'


2023-07-11 10:07:09,581:INFO:Initializing create_model()
2023-07-11 10:07:09,581:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1DD3B4EE0>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1D98BB070>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 10:07:09,581:INFO:Checking exceptions
2023-07-11 10:07:09,581:INFO:Importing libraries
2023-07-11 10:07:09,581:INFO:Copying training dataset
2023-07-11 10:07:09,581:INFO:Defining folds
2023-07-11 10:07:09,581:INFO:Declaring metric variables
2023-07-11 10:07:09,581:INFO:Importing untrained model
2023-07-11 10:07:09,581:INFO:Dummy Regressor Imported successfully
2023-07-11 10:07:09,581:INFO:Starting cross validation
2023-07-11 10:07:09,581:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 10:07:14,144:INFO:PyCaret RegressionExperiment
2023-07-11 10:07:14,144:INFO:Logging name: reg-default-name
2023-07-11 10:07:14,144:INFO:ML Usecase: MLUsecase.REGRESSION
2023-07-11 10:07:14,144:INFO:version 3.0.2
2023-07-11 10:07:14,144:INFO:Initializing setup()
2023-07-11 10:07:14,144:INFO:self.USI: 6326
2023-07-11 10:07:14,144:INFO:self._variable_keys: {'_ml_usecase', 'log_plots_param', 'USI', 'X_train', 'pipeline', 'gpu_param', 'exp_name_log', 'y', 'html_param', 'X', 'data', 'exp_id', 'y_train', 'target_param', 'X_test', 'fold_groups_param', 'n_jobs_param', 'idx', 'fold_generator', 'logging_param', 'memory', 'gpu_n_jobs_param', 'fold_shuffle_param', '_available_plots', 'seed', 'y_test', 'transform_target_param'}
2023-07-11 10:07:14,144:INFO:Checking environment
2023-07-11 10:07:14,144:INFO:python_version: 3.9.13
2023-07-11 10:07:14,144:INFO:python_build: ('main', 'Oct 13 2022 21:23:06')
2023-07-11 10:07:14,144:INFO:machine: AMD64
2023-07-11 10:07:14,144:INFO:platform: Windows-10-10.0.22621-SP0
2023-07-11 10:07:14,144:INFO:Memory: svmem(total=16893358080, available=3428929536, percent=79.7, used=13464428544, free=3428929536)
2023-07-11 10:07:14,144:INFO:Physical Core: 8
2023-07-11 10:07:14,144:INFO:Logical Core: 16
2023-07-11 10:07:14,144:INFO:Checking libraries
2023-07-11 10:07:14,144:INFO:System:
2023-07-11 10:07:14,144:INFO:    python: 3.9.13 (main, Oct 13 2022, 21:23:06) [MSC v.1916 64 bit (AMD64)]
2023-07-11 10:07:14,144:INFO:executable: C:\Users\didit\anaconda3\python.exe
2023-07-11 10:07:14,144:INFO:   machine: Windows-10-10.0.22621-SP0
2023-07-11 10:07:14,144:INFO:PyCaret required dependencies:
2023-07-11 10:07:14,144:INFO:                 pip: 23.0.1
2023-07-11 10:07:14,144:INFO:          setuptools: 67.8.0
2023-07-11 10:07:14,144:INFO:             pycaret: 3.0.2
2023-07-11 10:07:14,144:INFO:             IPython: 8.12.0
2023-07-11 10:07:14,144:INFO:          ipywidgets: 8.0.4
2023-07-11 10:07:14,144:INFO:                tqdm: 4.65.0
2023-07-11 10:07:14,144:INFO:               numpy: 1.21.5
2023-07-11 10:07:14,144:INFO:              pandas: 1.5.3
2023-07-11 10:07:14,144:INFO:              jinja2: 3.1.2
2023-07-11 10:07:14,144:INFO:               scipy: 1.10.1
2023-07-11 10:07:14,144:INFO:              joblib: 1.2.0
2023-07-11 10:07:14,144:INFO:             sklearn: 1.2.2
2023-07-11 10:07:14,144:INFO:                pyod: 1.0.9
2023-07-11 10:07:14,144:INFO:            imblearn: 0.10.1
2023-07-11 10:07:14,144:INFO:   category_encoders: 2.6.1
2023-07-11 10:07:14,144:INFO:            lightgbm: 3.3.5
2023-07-11 10:07:14,144:INFO:               numba: 0.57.0
2023-07-11 10:07:14,144:INFO:            requests: 2.29.0
2023-07-11 10:07:14,144:INFO:          matplotlib: 3.7.1
2023-07-11 10:07:14,144:INFO:          scikitplot: 0.3.7
2023-07-11 10:07:14,144:INFO:         yellowbrick: 1.5
2023-07-11 10:07:14,144:INFO:              plotly: 5.9.0
2023-07-11 10:07:14,144:INFO:             kaleido: 0.2.1
2023-07-11 10:07:14,144:INFO:         statsmodels: 0.13.5
2023-07-11 10:07:14,144:INFO:              sktime: 0.17.0
2023-07-11 10:07:14,144:INFO:               tbats: 1.1.3
2023-07-11 10:07:14,144:INFO:            pmdarima: 2.0.3
2023-07-11 10:07:14,144:INFO:              psutil: 5.9.0
2023-07-11 10:07:14,144:INFO:PyCaret optional dependencies:
2023-07-11 10:07:14,144:INFO:                shap: 0.41.0
2023-07-11 10:07:14,144:INFO:           interpret: Not installed
2023-07-11 10:07:14,144:INFO:                umap: Not installed
2023-07-11 10:07:14,144:INFO:    pandas_profiling: 4.3.1
2023-07-11 10:07:14,144:INFO:  explainerdashboard: Not installed
2023-07-11 10:07:14,144:INFO:             autoviz: Not installed
2023-07-11 10:07:14,144:INFO:           fairlearn: Not installed
2023-07-11 10:07:14,144:INFO:             xgboost: 1.7.6
2023-07-11 10:07:14,144:INFO:            catboost: Not installed
2023-07-11 10:07:14,144:INFO:              kmodes: Not installed
2023-07-11 10:07:14,144:INFO:             mlxtend: Not installed
2023-07-11 10:07:14,144:INFO:       statsforecast: Not installed
2023-07-11 10:07:14,144:INFO:        tune_sklearn: Not installed
2023-07-11 10:07:14,144:INFO:                 ray: Not installed
2023-07-11 10:07:14,144:INFO:            hyperopt: Not installed
2023-07-11 10:07:14,144:INFO:              optuna: Not installed
2023-07-11 10:07:14,144:INFO:               skopt: Not installed
2023-07-11 10:07:14,144:INFO:              mlflow: 2.4.2
2023-07-11 10:07:14,144:INFO:              gradio: Not installed
2023-07-11 10:07:14,144:INFO:             fastapi: 0.95.2
2023-07-11 10:07:14,144:INFO:             uvicorn: 0.22.0
2023-07-11 10:07:14,144:INFO:              m2cgen: Not installed
2023-07-11 10:07:14,144:INFO:           evidently: Not installed
2023-07-11 10:07:14,144:INFO:               fugue: Not installed
2023-07-11 10:07:14,144:INFO:           streamlit: 1.23.1
2023-07-11 10:07:14,144:INFO:             prophet: Not installed
2023-07-11 10:07:14,144:INFO:None
2023-07-11 10:07:14,144:INFO:Set up data.
2023-07-11 10:07:14,160:INFO:Set up train/test split.
2023-07-11 10:07:14,176:INFO:Set up index.
2023-07-11 10:07:14,176:INFO:Set up folding strategy.
2023-07-11 10:07:14,176:INFO:Assigning column types.
2023-07-11 10:07:14,176:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-07-11 10:07:14,176:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-07-11 10:07:14,176:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-07-11 10:07:14,191:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 10:07:14,223:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 10:07:14,264:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 10:07:14,264:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 10:07:14,264:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 10:07:14,264:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-07-11 10:07:14,264:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-07-11 10:07:14,281:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 10:07:14,313:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 10:07:14,360:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 10:07:14,360:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 10:07:14,360:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 10:07:14,360:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-07-11 10:07:14,360:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-07-11 10:07:14,360:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 10:07:14,408:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 10:07:14,439:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 10:07:14,439:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 10:07:14,439:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 10:07:14,458:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-07-11 10:07:14,462:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 10:07:14,504:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 10:07:14,529:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 10:07:14,529:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 10:07:14,529:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 10:07:14,529:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-07-11 10:07:14,545:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 10:07:14,576:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 10:07:14,623:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 10:07:14,623:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 10:07:14,623:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 10:07:14,623:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-11 10:07:14,691:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 10:07:14,728:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 10:07:14,728:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 10:07:14,728:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 10:07:14,728:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-07-11 10:07:14,790:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 10:07:14,822:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 10:07:14,822:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 10:07:14,822:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 10:07:14,893:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 10:07:14,914:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 10:07:14,914:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 10:07:14,929:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 10:07:14,929:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-07-11 10:07:14,976:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 10:07:15,008:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 10:07:15,008:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 10:07:15,074:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-11 10:07:15,104:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 10:07:15,112:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 10:07:15,112:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-07-11 10:07:15,191:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 10:07:15,191:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 10:07:15,290:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 10:07:15,292:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 10:07:15,293:INFO:Preparing preprocessing pipeline...
2023-07-11 10:07:15,293:INFO:Set up simple imputation.
2023-07-11 10:07:15,297:INFO:Set up encoding of ordinal features.
2023-07-11 10:07:15,304:INFO:Set up encoding of categorical features.
2023-07-11 10:07:15,472:INFO:Finished creating preprocessing pipeline.
2023-07-11 10:07:15,591:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\didit\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'Medu', 'Fedu',
                                             'traveltime', 'studytime',
                                             'failures', 'famrel', 'freetime',
                                             'goout', 'Dalc', 'Walc', 'health',
                                             'absences', 'G1', 'G2'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['schoo...
                                                                        {'col': 'internet',
                                                                         'data_type': dtype('O'),
                                                                         'mapping': no     0
yes    1
NaN   -1
dtype: int64},
                                                                        {'col': 'romantic',
                                                                         'data_type': dtype('O'),
                                                                         'mapping': no     0
yes    1
NaN   -1
dtype: int64}]))),
                ('onehot_encoding',
                 TransformerWrapper(include=['Mjob', 'Fjob', 'reason',
                                             'guardian'],
                                    transformer=OneHotEncoder(cols=['Mjob',
                                                                    'Fjob',
                                                                    'reason',
                                                                    'guardian'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True)))])
2023-07-11 10:07:15,591:INFO:Creating final display dataframe.
2023-07-11 10:07:15,991:INFO:Setup _display_container:                     Description             Value
0                    Session id              7257
1                        Target                G3
2                   Target type        Regression
3           Original data shape         (649, 33)
4        Transformed data shape         (649, 46)
5   Transformed train set shape         (454, 46)
6    Transformed test set shape         (195, 46)
7              Ordinal features                13
8              Numeric features                15
9          Categorical features                17
10                   Preprocess              True
11              Imputation type            simple
12           Numeric imputation              mean
13       Categorical imputation              mode
14     Maximum one-hot encoding                25
15              Encoding method              None
16               Fold Generator             KFold
17                  Fold Number                10
18                     CPU Jobs                -1
19                      Use GPU             False
20               Log Experiment             False
21              Experiment Name  reg-default-name
22                          USI              6326
2023-07-11 10:07:15,991:INFO:                    Description             Value
2023-07-11 10:07:15,991:INFO:0                    Session id              7257
2023-07-11 10:07:15,991:INFO:1                        Target                G3
2023-07-11 10:07:15,991:INFO:2                   Target type        Regression
2023-07-11 10:07:15,991:INFO:3           Original data shape         (649, 33)
2023-07-11 10:07:15,991:INFO:4        Transformed data shape         (649, 46)
2023-07-11 10:07:15,991:INFO:5   Transformed train set shape         (454, 46)
2023-07-11 10:07:15,991:INFO:6    Transformed test set shape         (195, 46)
2023-07-11 10:07:15,991:INFO:7              Ordinal features                13
2023-07-11 10:07:15,991:INFO:8              Numeric features                15
2023-07-11 10:07:15,991:INFO:9          Categorical features                17
2023-07-11 10:07:15,991:INFO:10                   Preprocess              True
2023-07-11 10:07:15,991:INFO:11              Imputation type            simple
2023-07-11 10:07:15,991:INFO:12           Numeric imputation              mean
2023-07-11 10:07:15,991:INFO:13       Categorical imputation              mode
2023-07-11 10:07:15,991:INFO:14     Maximum one-hot encoding                25
2023-07-11 10:07:15,991:INFO:15              Encoding method              None
2023-07-11 10:07:15,991:INFO:16               Fold Generator             KFold
2023-07-11 10:07:15,991:INFO:17                  Fold Number                10
2023-07-11 10:07:15,991:INFO:18                     CPU Jobs                -1
2023-07-11 10:07:15,991:INFO:19                      Use GPU             False
2023-07-11 10:07:15,991:INFO:20               Log Experiment             False
2023-07-11 10:07:15,991:INFO:21              Experiment Name  reg-default-name
2023-07-11 10:07:15,991:INFO:22                          USI              6326
2023-07-11 10:07:16,083:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 10:07:16,085:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 10:07:16,175:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 10:07:16,175:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 10:07:16,175:INFO:setup() successfully completed in 2.71s...............
2023-07-11 10:07:16,183:INFO:Initializing compare_models()
2023-07-11 10:07:16,184:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D76A1880>, include=None, fold=None, round=4, cross_validation=True, sort=R2, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x000001E1D76A1880>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2023-07-11 10:07:16,184:INFO:Checking exceptions
2023-07-11 10:07:16,185:INFO:Preparing display monitor
2023-07-11 10:07:16,187:WARNING:
2023-07-11 10:07:16,187:WARNING:Processing:   0%|                                                                               | 0/81 [00:00<?, ?it/s]
2023-07-11 10:07:16,187:WARNING:[A
2023-07-11 10:07:16,187:INFO:Initializing Linear Regression
2023-07-11 10:07:16,187:INFO:Total runtime is 0.0 minutes
2023-07-11 10:07:16,187:INFO:SubProcess create_model() called ==================================
2023-07-11 10:07:16,187:INFO:Initializing create_model()
2023-07-11 10:07:16,188:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D76A1880>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1A8E32190>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 10:07:16,188:INFO:Checking exceptions
2023-07-11 10:07:16,188:INFO:Importing libraries
2023-07-11 10:07:16,188:INFO:Copying training dataset
2023-07-11 10:07:16,191:INFO:Defining folds
2023-07-11 10:07:16,191:INFO:Declaring metric variables
2023-07-11 10:07:16,191:INFO:Importing untrained model
2023-07-11 10:07:16,192:INFO:Linear Regression Imported successfully
2023-07-11 10:07:16,192:INFO:Starting cross validation
2023-07-11 10:07:16,193:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 10:07:17,391:ERROR:create_model() for dummy raised an exception or returned all 0.0:
2023-07-11 10:07:17,391:ERROR:Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 792, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\dummy.py", line 554, in fit
    self.constant_ = np.average(y, axis=0, weights=sample_weight)
  File "<__array_function__ internals>", line 5, in average
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\lib\function_base.py", line 380, in average
    avg = a.mean(axis)
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\_methods.py", line 181, in _mean
    ret = um.true_divide(
TypeError: unsupported operand type(s) for /: 'str' and 'int'


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 808, in compare_models
    model, model_fit_time = self._create_model(**create_model_args)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1519, in _create_model
    model, model_fit_time, model_results, _ = self._create_model_with_cv(
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 1114, in _create_model_with_cv
    scores = cross_validate(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "C:\Users\didit\anaconda3\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\dummy.py", line 554, in fit
    self.constant_ = np.average(y, axis=0, weights=sample_weight)
  File "<__array_function__ internals>", line 5, in average
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\lib\function_base.py", line 380, in average
    avg = a.mean(axis)
  File "C:\Users\didit\anaconda3\lib\site-packages\numpy\core\_methods.py", line 181, in _mean
    ret = um.true_divide(
TypeError: unsupported operand type(s) for /: 'str' and 'int'


2023-07-11 10:07:19,231:INFO:_master_model_container: 0
2023-07-11 10:07:19,231:INFO:_display_container: 2
2023-07-11 10:07:19,231:INFO:[]
2023-07-11 10:07:19,231:INFO:compare_models() successfully completed......................................
2023-07-11 10:07:22,497:INFO:Calculating mean and std
2023-07-11 10:07:22,497:WARNING:
2023-07-11 10:07:22,497:WARNING:Processing:   6%|####3                                                                  | 5/81 [00:06<01:35,  1.26s/it]
2023-07-11 10:07:22,497:WARNING:[A
2023-07-11 10:07:22,497:INFO:Creating metrics dataframe
2023-07-11 10:07:23,192:WARNING:
2023-07-11 10:07:23,192:WARNING:Processing:   7%|#####2                                                                 | 6/81 [00:07<01:25,  1.14s/it]
2023-07-11 10:07:23,192:WARNING:[A
2023-07-11 10:07:23,192:INFO:Uploading results into container
2023-07-11 10:07:23,192:INFO:Uploading model into container now
2023-07-11 10:07:23,192:INFO:_master_model_container: 1
2023-07-11 10:07:23,192:INFO:_display_container: 2
2023-07-11 10:07:23,192:INFO:LinearRegression(n_jobs=-1)
2023-07-11 10:07:23,192:INFO:create_model() successfully completed......................................
2023-07-11 10:07:23,328:INFO:SubProcess create_model() end ==================================
2023-07-11 10:07:23,328:INFO:Creating metrics dataframe
2023-07-11 10:07:23,328:INFO:Initializing Lasso Regression
2023-07-11 10:07:23,328:INFO:Total runtime is 0.11900633573532104 minutes
2023-07-11 10:07:23,328:INFO:SubProcess create_model() called ==================================
2023-07-11 10:07:23,328:INFO:Initializing create_model()
2023-07-11 10:07:23,328:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D76A1880>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1A8E32190>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 10:07:23,328:INFO:Checking exceptions
2023-07-11 10:07:23,328:INFO:Importing libraries
2023-07-11 10:07:23,328:INFO:Copying training dataset
2023-07-11 10:07:23,328:WARNING:
2023-07-11 10:07:23,328:WARNING:Processing:   9%|######1                                                                | 7/81 [00:07<01:06,  1.12it/s]
2023-07-11 10:07:23,328:WARNING:[A
2023-07-11 10:07:23,328:INFO:Defining folds
2023-07-11 10:07:23,328:INFO:Declaring metric variables
2023-07-11 10:07:23,328:INFO:Importing untrained model
2023-07-11 10:07:23,328:INFO:Lasso Regression Imported successfully
2023-07-11 10:07:23,328:INFO:Starting cross validation
2023-07-11 10:07:23,328:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 10:07:27,792:INFO:Calculating mean and std
2023-07-11 10:07:27,792:WARNING:
2023-07-11 10:07:27,792:WARNING:Processing:  11%|#######8                                                               | 9/81 [00:11<01:43,  1.44s/it]
2023-07-11 10:07:27,792:WARNING:[A
2023-07-11 10:07:27,792:INFO:Creating metrics dataframe
2023-07-11 10:07:28,622:WARNING:
2023-07-11 10:07:28,622:WARNING:Processing:  12%|########6                                                             | 10/81 [00:12<01:32,  1.30s/it]
2023-07-11 10:07:28,622:WARNING:[A
2023-07-11 10:07:28,622:INFO:Uploading results into container
2023-07-11 10:07:28,622:INFO:Uploading model into container now
2023-07-11 10:07:28,622:INFO:_master_model_container: 2
2023-07-11 10:07:28,622:INFO:_display_container: 2
2023-07-11 10:07:28,622:INFO:Lasso(random_state=7257)
2023-07-11 10:07:28,622:INFO:create_model() successfully completed......................................
2023-07-11 10:07:28,745:INFO:SubProcess create_model() end ==================================
2023-07-11 10:07:28,745:INFO:Creating metrics dataframe
2023-07-11 10:07:28,761:INFO:Initializing Ridge Regression
2023-07-11 10:07:28,761:INFO:Total runtime is 0.20956111351648965 minutes
2023-07-11 10:07:28,761:INFO:SubProcess create_model() called ==================================
2023-07-11 10:07:28,761:INFO:Initializing create_model()
2023-07-11 10:07:28,761:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D76A1880>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1A8E32190>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 10:07:28,761:INFO:Checking exceptions
2023-07-11 10:07:28,761:INFO:Importing libraries
2023-07-11 10:07:28,761:INFO:Copying training dataset
2023-07-11 10:07:28,761:WARNING:
2023-07-11 10:07:28,761:WARNING:Processing:  14%|#########5                                                            | 11/81 [00:12<01:11,  1.02s/it]
2023-07-11 10:07:28,761:WARNING:[A
2023-07-11 10:07:28,761:INFO:Defining folds
2023-07-11 10:07:28,761:INFO:Declaring metric variables
2023-07-11 10:07:28,761:INFO:Importing untrained model
2023-07-11 10:07:28,761:INFO:Ridge Regression Imported successfully
2023-07-11 10:07:28,761:INFO:Starting cross validation
2023-07-11 10:07:28,761:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 10:07:33,288:INFO:Calculating mean and std
2023-07-11 10:07:33,288:WARNING:
2023-07-11 10:07:33,288:WARNING:Processing:  16%|###########2                                                          | 13/81 [00:17<01:44,  1.53s/it]
2023-07-11 10:07:33,288:WARNING:[A
2023-07-11 10:07:33,288:INFO:Creating metrics dataframe
2023-07-11 10:07:34,075:WARNING:
2023-07-11 10:07:34,075:WARNING:Processing:  17%|############                                                          | 14/81 [00:17<01:31,  1.36s/it]
2023-07-11 10:07:34,075:WARNING:[A
2023-07-11 10:07:34,075:INFO:Uploading results into container
2023-07-11 10:07:34,076:INFO:Uploading model into container now
2023-07-11 10:07:34,076:INFO:_master_model_container: 3
2023-07-11 10:07:34,076:INFO:_display_container: 2
2023-07-11 10:07:34,077:INFO:Ridge(random_state=7257)
2023-07-11 10:07:34,077:INFO:create_model() successfully completed......................................
2023-07-11 10:07:34,206:INFO:SubProcess create_model() end ==================================
2023-07-11 10:07:34,206:INFO:Creating metrics dataframe
2023-07-11 10:07:34,206:INFO:Initializing Elastic Net
2023-07-11 10:07:34,206:INFO:Total runtime is 0.300321360429128 minutes
2023-07-11 10:07:34,206:INFO:SubProcess create_model() called ==================================
2023-07-11 10:07:34,206:INFO:Initializing create_model()
2023-07-11 10:07:34,206:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D76A1880>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1A8E32190>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 10:07:34,206:INFO:Checking exceptions
2023-07-11 10:07:34,206:INFO:Importing libraries
2023-07-11 10:07:34,206:INFO:Copying training dataset
2023-07-11 10:07:34,206:WARNING:
2023-07-11 10:07:34,206:WARNING:Processing:  19%|############9                                                         | 15/81 [00:18<01:09,  1.06s/it]
2023-07-11 10:07:34,206:WARNING:[A
2023-07-11 10:07:34,206:INFO:Defining folds
2023-07-11 10:07:34,206:INFO:Declaring metric variables
2023-07-11 10:07:34,206:INFO:Importing untrained model
2023-07-11 10:07:34,206:INFO:Elastic Net Imported successfully
2023-07-11 10:07:34,206:INFO:Starting cross validation
2023-07-11 10:07:34,206:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 10:07:38,745:INFO:Calculating mean and std
2023-07-11 10:07:38,745:WARNING:
2023-07-11 10:07:38,745:WARNING:Processing:  21%|##############6                                                       | 17/81 [00:22<01:39,  1.56s/it]
2023-07-11 10:07:38,745:WARNING:[A
2023-07-11 10:07:38,745:INFO:Creating metrics dataframe
2023-07-11 10:07:39,512:WARNING:
2023-07-11 10:07:39,512:WARNING:Processing:  22%|###############5                                                      | 18/81 [00:23<01:26,  1.38s/it]
2023-07-11 10:07:39,512:WARNING:[A
2023-07-11 10:07:39,512:INFO:Uploading results into container
2023-07-11 10:07:39,512:INFO:Uploading model into container now
2023-07-11 10:07:39,512:INFO:_master_model_container: 4
2023-07-11 10:07:39,512:INFO:_display_container: 2
2023-07-11 10:07:39,512:INFO:ElasticNet(random_state=7257)
2023-07-11 10:07:39,512:INFO:create_model() successfully completed......................................
2023-07-11 10:07:39,639:INFO:SubProcess create_model() end ==================================
2023-07-11 10:07:39,639:INFO:Creating metrics dataframe
2023-07-11 10:07:39,643:INFO:Initializing Least Angle Regression
2023-07-11 10:07:39,643:INFO:Total runtime is 0.39092601140340166 minutes
2023-07-11 10:07:39,643:INFO:SubProcess create_model() called ==================================
2023-07-11 10:07:39,643:INFO:Initializing create_model()
2023-07-11 10:07:39,643:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D76A1880>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1A8E32190>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 10:07:39,643:INFO:Checking exceptions
2023-07-11 10:07:39,643:INFO:Importing libraries
2023-07-11 10:07:39,643:INFO:Copying training dataset
2023-07-11 10:07:39,646:WARNING:
2023-07-11 10:07:39,646:WARNING:Processing:  23%|################4                                                     | 19/81 [00:23<01:06,  1.07s/it]
2023-07-11 10:07:39,646:WARNING:[A
2023-07-11 10:07:39,646:INFO:Defining folds
2023-07-11 10:07:39,646:INFO:Declaring metric variables
2023-07-11 10:07:39,646:INFO:Importing untrained model
2023-07-11 10:07:39,646:INFO:Least Angle Regression Imported successfully
2023-07-11 10:07:39,647:INFO:Starting cross validation
2023-07-11 10:07:39,648:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 10:07:44,279:INFO:Calculating mean and std
2023-07-11 10:07:44,279:WARNING:
2023-07-11 10:07:44,279:WARNING:Processing:  26%|##################1                                                   | 21/81 [00:28<01:35,  1.59s/it]
2023-07-11 10:07:44,279:WARNING:[A
2023-07-11 10:07:44,280:INFO:Creating metrics dataframe
2023-07-11 10:07:45,023:WARNING:
2023-07-11 10:07:45,023:WARNING:Processing:  27%|###################                                                   | 22/81 [00:28<01:22,  1.39s/it]
2023-07-11 10:07:45,023:WARNING:[A
2023-07-11 10:07:45,023:INFO:Uploading results into container
2023-07-11 10:07:45,023:INFO:Uploading model into container now
2023-07-11 10:07:45,023:INFO:_master_model_container: 5
2023-07-11 10:07:45,023:INFO:_display_container: 2
2023-07-11 10:07:45,023:INFO:Lars(random_state=7257)
2023-07-11 10:07:45,023:INFO:create_model() successfully completed......................................
2023-07-11 10:07:45,161:INFO:SubProcess create_model() end ==================================
2023-07-11 10:07:45,161:INFO:Creating metrics dataframe
2023-07-11 10:07:45,161:INFO:Initializing Lasso Least Angle Regression
2023-07-11 10:07:45,161:INFO:Total runtime is 0.48289585908253985 minutes
2023-07-11 10:07:45,161:INFO:SubProcess create_model() called ==================================
2023-07-11 10:07:45,161:INFO:Initializing create_model()
2023-07-11 10:07:45,161:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D76A1880>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1A8E32190>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 10:07:45,161:INFO:Checking exceptions
2023-07-11 10:07:45,161:INFO:Importing libraries
2023-07-11 10:07:45,161:INFO:Copying training dataset
2023-07-11 10:07:45,161:WARNING:
2023-07-11 10:07:45,161:WARNING:Processing:  28%|###################8                                                  | 23/81 [00:28<01:02,  1.09s/it]
2023-07-11 10:07:45,161:WARNING:[A
2023-07-11 10:07:45,161:INFO:Defining folds
2023-07-11 10:07:45,161:INFO:Declaring metric variables
2023-07-11 10:07:45,161:INFO:Importing untrained model
2023-07-11 10:07:45,161:INFO:Lasso Least Angle Regression Imported successfully
2023-07-11 10:07:45,161:INFO:Starting cross validation
2023-07-11 10:07:45,161:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 10:07:49,712:INFO:Calculating mean and std
2023-07-11 10:07:49,712:WARNING:
2023-07-11 10:07:49,712:WARNING:Processing:  31%|#####################6                                                | 25/81 [00:33<01:28,  1.58s/it]
2023-07-11 10:07:49,712:WARNING:[A
2023-07-11 10:07:49,712:INFO:Creating metrics dataframe
2023-07-11 10:07:50,424:WARNING:
2023-07-11 10:07:50,424:WARNING:Processing:  32%|######################4                                               | 26/81 [00:34<01:15,  1.38s/it]
2023-07-11 10:07:50,424:WARNING:[A
2023-07-11 10:07:50,424:INFO:Uploading results into container
2023-07-11 10:07:50,424:INFO:Uploading model into container now
2023-07-11 10:07:50,424:INFO:_master_model_container: 6
2023-07-11 10:07:50,424:INFO:_display_container: 2
2023-07-11 10:07:50,424:INFO:LassoLars(random_state=7257)
2023-07-11 10:07:50,424:INFO:create_model() successfully completed......................................
2023-07-11 10:07:50,562:INFO:SubProcess create_model() end ==================================
2023-07-11 10:07:50,562:INFO:Creating metrics dataframe
2023-07-11 10:07:50,562:INFO:Initializing Orthogonal Matching Pursuit
2023-07-11 10:07:50,562:INFO:Total runtime is 0.572919491926829 minutes
2023-07-11 10:07:50,562:INFO:SubProcess create_model() called ==================================
2023-07-11 10:07:50,562:INFO:Initializing create_model()
2023-07-11 10:07:50,562:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D76A1880>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1A8E32190>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 10:07:50,562:INFO:Checking exceptions
2023-07-11 10:07:50,562:INFO:Importing libraries
2023-07-11 10:07:50,562:INFO:Copying training dataset
2023-07-11 10:07:50,562:WARNING:
2023-07-11 10:07:50,562:WARNING:Processing:  33%|#######################3                                              | 27/81 [00:34<00:58,  1.08s/it]
2023-07-11 10:07:50,562:WARNING:[A
2023-07-11 10:07:50,562:INFO:Defining folds
2023-07-11 10:07:50,562:INFO:Declaring metric variables
2023-07-11 10:07:50,562:INFO:Importing untrained model
2023-07-11 10:07:50,562:INFO:Orthogonal Matching Pursuit Imported successfully
2023-07-11 10:07:50,562:INFO:Starting cross validation
2023-07-11 10:07:50,576:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 10:07:55,061:INFO:Calculating mean and std
2023-07-11 10:07:55,062:WARNING:
2023-07-11 10:07:55,062:WARNING:Processing:  36%|#########################                                             | 29/81 [00:38<01:21,  1.56s/it]
2023-07-11 10:07:55,062:WARNING:[A
2023-07-11 10:07:55,062:INFO:Creating metrics dataframe
2023-07-11 10:07:55,714:WARNING:
2023-07-11 10:07:55,714:WARNING:Processing:  37%|#########################9                                            | 30/81 [00:39<01:09,  1.35s/it]
2023-07-11 10:07:55,714:WARNING:[A
2023-07-11 10:07:55,714:INFO:Uploading results into container
2023-07-11 10:07:55,714:INFO:Uploading model into container now
2023-07-11 10:07:55,714:INFO:_master_model_container: 7
2023-07-11 10:07:55,714:INFO:_display_container: 2
2023-07-11 10:07:55,714:INFO:OrthogonalMatchingPursuit()
2023-07-11 10:07:55,714:INFO:create_model() successfully completed......................................
2023-07-11 10:07:55,859:INFO:SubProcess create_model() end ==================================
2023-07-11 10:07:55,859:INFO:Creating metrics dataframe
2023-07-11 10:07:55,868:INFO:Initializing Bayesian Ridge
2023-07-11 10:07:55,869:INFO:Total runtime is 0.6613708694775899 minutes
2023-07-11 10:07:55,869:INFO:SubProcess create_model() called ==================================
2023-07-11 10:07:55,869:INFO:Initializing create_model()
2023-07-11 10:07:55,869:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D76A1880>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1A8E32190>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 10:07:55,869:INFO:Checking exceptions
2023-07-11 10:07:55,869:INFO:Importing libraries
2023-07-11 10:07:55,869:INFO:Copying training dataset
2023-07-11 10:07:55,874:WARNING:
2023-07-11 10:07:55,874:WARNING:Processing:  38%|##########################7                                           | 31/81 [00:39<00:53,  1.06s/it]
2023-07-11 10:07:55,874:WARNING:[A
2023-07-11 10:07:55,874:INFO:Defining folds
2023-07-11 10:07:55,874:INFO:Declaring metric variables
2023-07-11 10:07:55,874:INFO:Importing untrained model
2023-07-11 10:07:55,875:INFO:Bayesian Ridge Imported successfully
2023-07-11 10:07:55,875:INFO:Starting cross validation
2023-07-11 10:07:55,876:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 10:08:00,497:INFO:Calculating mean and std
2023-07-11 10:08:00,497:WARNING:
2023-07-11 10:08:00,497:WARNING:Processing:  41%|############################5                                         | 33/81 [00:44<01:15,  1.58s/it]
2023-07-11 10:08:00,497:WARNING:[A
2023-07-11 10:08:00,497:INFO:Creating metrics dataframe
2023-07-11 10:08:01,299:WARNING:
2023-07-11 10:08:01,299:WARNING:Processing:  42%|#############################3                                        | 34/81 [00:45<01:05,  1.40s/it]
2023-07-11 10:08:01,299:WARNING:[A
2023-07-11 10:08:01,299:INFO:Uploading results into container
2023-07-11 10:08:01,299:INFO:Uploading model into container now
2023-07-11 10:08:01,299:INFO:_master_model_container: 8
2023-07-11 10:08:01,299:INFO:_display_container: 2
2023-07-11 10:08:01,299:INFO:BayesianRidge()
2023-07-11 10:08:01,299:INFO:create_model() successfully completed......................................
2023-07-11 10:08:01,424:INFO:SubProcess create_model() end ==================================
2023-07-11 10:08:01,424:INFO:Creating metrics dataframe
2023-07-11 10:08:01,443:INFO:Initializing Passive Aggressive Regressor
2023-07-11 10:08:01,444:INFO:Total runtime is 0.7542756915092468 minutes
2023-07-11 10:08:01,444:INFO:SubProcess create_model() called ==================================
2023-07-11 10:08:01,444:INFO:Initializing create_model()
2023-07-11 10:08:01,445:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D76A1880>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1A8E32190>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 10:08:01,445:INFO:Checking exceptions
2023-07-11 10:08:01,445:INFO:Importing libraries
2023-07-11 10:08:01,445:INFO:Copying training dataset
2023-07-11 10:08:01,448:WARNING:
2023-07-11 10:08:01,448:WARNING:Processing:  43%|##############################2                                       | 35/81 [00:45<00:50,  1.09s/it]
2023-07-11 10:08:01,448:WARNING:[A
2023-07-11 10:08:01,448:INFO:Defining folds
2023-07-11 10:08:01,448:INFO:Declaring metric variables
2023-07-11 10:08:01,448:INFO:Importing untrained model
2023-07-11 10:08:01,448:INFO:Passive Aggressive Regressor Imported successfully
2023-07-11 10:08:01,448:INFO:Starting cross validation
2023-07-11 10:08:01,448:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 10:08:06,077:INFO:Calculating mean and std
2023-07-11 10:08:06,078:WARNING:
2023-07-11 10:08:06,078:WARNING:Processing:  46%|###############################9                                      | 37/81 [00:49<01:10,  1.60s/it]
2023-07-11 10:08:06,078:WARNING:[A
2023-07-11 10:08:06,079:INFO:Creating metrics dataframe
2023-07-11 10:08:06,848:WARNING:
2023-07-11 10:08:06,848:WARNING:Processing:  47%|################################8                                     | 38/81 [00:50<01:00,  1.41s/it]
2023-07-11 10:08:06,848:WARNING:[A
2023-07-11 10:08:06,848:INFO:Uploading results into container
2023-07-11 10:08:06,848:INFO:Uploading model into container now
2023-07-11 10:08:06,848:INFO:_master_model_container: 9
2023-07-11 10:08:06,848:INFO:_display_container: 2
2023-07-11 10:08:06,848:INFO:PassiveAggressiveRegressor(random_state=7257)
2023-07-11 10:08:06,848:INFO:create_model() successfully completed......................................
2023-07-11 10:08:06,976:INFO:SubProcess create_model() end ==================================
2023-07-11 10:08:06,976:INFO:Creating metrics dataframe
2023-07-11 10:08:06,976:INFO:Initializing Huber Regressor
2023-07-11 10:08:06,976:INFO:Total runtime is 0.846486485004425 minutes
2023-07-11 10:08:06,976:INFO:SubProcess create_model() called ==================================
2023-07-11 10:08:06,992:INFO:Initializing create_model()
2023-07-11 10:08:06,992:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D76A1880>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1A8E32190>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 10:08:06,992:INFO:Checking exceptions
2023-07-11 10:08:06,992:INFO:Importing libraries
2023-07-11 10:08:06,992:INFO:Copying training dataset
2023-07-11 10:08:06,992:WARNING:
2023-07-11 10:08:06,992:WARNING:Processing:  48%|#################################7                                    | 39/81 [00:50<00:46,  1.10s/it]
2023-07-11 10:08:06,992:WARNING:[A
2023-07-11 10:08:06,992:INFO:Defining folds
2023-07-11 10:08:06,992:INFO:Declaring metric variables
2023-07-11 10:08:06,992:INFO:Importing untrained model
2023-07-11 10:08:06,992:INFO:Huber Regressor Imported successfully
2023-07-11 10:08:06,992:INFO:Starting cross validation
2023-07-11 10:08:06,992:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 10:08:07,314:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-07-11 10:08:07,376:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-07-11 10:08:07,393:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-07-11 10:08:07,408:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-07-11 10:08:07,424:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-07-11 10:08:07,441:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-07-11 10:08:07,441:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-07-11 10:08:07,493:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-07-11 10:08:07,513:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-07-11 10:08:07,545:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-07-11 10:08:11,729:INFO:Calculating mean and std
2023-07-11 10:08:11,729:WARNING:
2023-07-11 10:08:11,729:WARNING:Processing:  51%|###################################4                                  | 41/81 [00:55<01:04,  1.62s/it]
2023-07-11 10:08:11,729:WARNING:[A
2023-07-11 10:08:11,729:INFO:Creating metrics dataframe
2023-07-11 10:08:12,545:WARNING:
2023-07-11 10:08:12,545:WARNING:Processing:  52%|####################################2                                 | 42/81 [00:56<00:56,  1.44s/it]
2023-07-11 10:08:12,545:WARNING:[A
2023-07-11 10:08:12,545:INFO:Uploading results into container
2023-07-11 10:08:12,545:INFO:Uploading model into container now
2023-07-11 10:08:12,545:INFO:_master_model_container: 10
2023-07-11 10:08:12,545:INFO:_display_container: 2
2023-07-11 10:08:12,545:INFO:HuberRegressor()
2023-07-11 10:08:12,545:INFO:create_model() successfully completed......................................
2023-07-11 10:08:12,697:INFO:SubProcess create_model() end ==================================
2023-07-11 10:08:12,697:INFO:Creating metrics dataframe
2023-07-11 10:08:12,702:INFO:Initializing K Neighbors Regressor
2023-07-11 10:08:12,702:INFO:Total runtime is 0.941908856232961 minutes
2023-07-11 10:08:12,702:INFO:SubProcess create_model() called ==================================
2023-07-11 10:08:12,702:INFO:Initializing create_model()
2023-07-11 10:08:12,702:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D76A1880>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1A8E32190>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 10:08:12,702:INFO:Checking exceptions
2023-07-11 10:08:12,702:INFO:Importing libraries
2023-07-11 10:08:12,702:INFO:Copying training dataset
2023-07-11 10:08:12,702:WARNING:
2023-07-11 10:08:12,702:WARNING:Processing:  53%|#####################################1                                | 43/81 [00:56<00:42,  1.12s/it]
2023-07-11 10:08:12,702:WARNING:[A
2023-07-11 10:08:12,702:INFO:Defining folds
2023-07-11 10:08:12,702:INFO:Declaring metric variables
2023-07-11 10:08:12,702:INFO:Importing untrained model
2023-07-11 10:08:12,702:INFO:K Neighbors Regressor Imported successfully
2023-07-11 10:08:12,702:INFO:Starting cross validation
2023-07-11 10:08:12,702:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 10:08:17,345:INFO:Calculating mean and std
2023-07-11 10:08:17,345:WARNING:
2023-07-11 10:08:17,345:WARNING:Processing:  56%|######################################8                               | 45/81 [01:01<00:58,  1.62s/it]
2023-07-11 10:08:17,345:WARNING:[A
2023-07-11 10:08:17,345:INFO:Creating metrics dataframe
2023-07-11 10:08:18,097:WARNING:
2023-07-11 10:08:18,097:WARNING:Processing:  57%|#######################################7                              | 46/81 [01:01<00:49,  1.42s/it]
2023-07-11 10:08:18,097:WARNING:[A
2023-07-11 10:08:18,097:INFO:Uploading results into container
2023-07-11 10:08:18,097:INFO:Uploading model into container now
2023-07-11 10:08:18,097:INFO:_master_model_container: 11
2023-07-11 10:08:18,097:INFO:_display_container: 2
2023-07-11 10:08:18,097:INFO:KNeighborsRegressor(n_jobs=-1)
2023-07-11 10:08:18,097:INFO:create_model() successfully completed......................................
2023-07-11 10:08:18,223:INFO:SubProcess create_model() end ==================================
2023-07-11 10:08:18,223:INFO:Creating metrics dataframe
2023-07-11 10:08:18,240:INFO:Initializing Decision Tree Regressor
2023-07-11 10:08:18,240:INFO:Total runtime is 1.0342081467310587 minutes
2023-07-11 10:08:18,240:INFO:SubProcess create_model() called ==================================
2023-07-11 10:08:18,240:INFO:Initializing create_model()
2023-07-11 10:08:18,240:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D76A1880>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1A8E32190>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 10:08:18,240:INFO:Checking exceptions
2023-07-11 10:08:18,240:INFO:Importing libraries
2023-07-11 10:08:18,240:INFO:Copying training dataset
2023-07-11 10:08:18,245:WARNING:
2023-07-11 10:08:18,245:WARNING:Processing:  58%|########################################6                             | 47/81 [01:02<00:37,  1.11s/it]
2023-07-11 10:08:18,245:WARNING:[A
2023-07-11 10:08:18,245:INFO:Defining folds
2023-07-11 10:08:18,245:INFO:Declaring metric variables
2023-07-11 10:08:18,246:INFO:Importing untrained model
2023-07-11 10:08:18,246:INFO:Decision Tree Regressor Imported successfully
2023-07-11 10:08:18,246:INFO:Starting cross validation
2023-07-11 10:08:18,247:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 10:08:22,898:INFO:Calculating mean and std
2023-07-11 10:08:22,898:WARNING:
2023-07-11 10:08:22,898:WARNING:Processing:  60%|##########################################3                           | 49/81 [01:06<00:51,  1.61s/it]
2023-07-11 10:08:22,898:WARNING:[A
2023-07-11 10:08:22,898:INFO:Creating metrics dataframe
2023-07-11 10:08:23,675:WARNING:
2023-07-11 10:08:23,676:WARNING:Processing:  62%|###########################################2                          | 50/81 [01:07<00:44,  1.42s/it]
2023-07-11 10:08:23,676:WARNING:[A
2023-07-11 10:08:23,676:INFO:Uploading results into container
2023-07-11 10:08:23,676:INFO:Uploading model into container now
2023-07-11 10:08:23,676:INFO:_master_model_container: 12
2023-07-11 10:08:23,677:INFO:_display_container: 2
2023-07-11 10:08:23,677:INFO:DecisionTreeRegressor(random_state=7257)
2023-07-11 10:08:23,677:INFO:create_model() successfully completed......................................
2023-07-11 10:08:23,793:INFO:SubProcess create_model() end ==================================
2023-07-11 10:08:23,793:INFO:Creating metrics dataframe
2023-07-11 10:08:23,809:INFO:Initializing Random Forest Regressor
2023-07-11 10:08:23,809:INFO:Total runtime is 1.1270253936449686 minutes
2023-07-11 10:08:23,809:INFO:SubProcess create_model() called ==================================
2023-07-11 10:08:23,809:INFO:Initializing create_model()
2023-07-11 10:08:23,809:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D76A1880>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1A8E32190>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 10:08:23,809:INFO:Checking exceptions
2023-07-11 10:08:23,809:INFO:Importing libraries
2023-07-11 10:08:23,809:INFO:Copying training dataset
2023-07-11 10:08:23,809:WARNING:
2023-07-11 10:08:23,809:WARNING:Processing:  63%|############################################                          | 51/81 [01:07<00:33,  1.11s/it]
2023-07-11 10:08:23,809:WARNING:[A
2023-07-11 10:08:23,809:INFO:Defining folds
2023-07-11 10:08:23,809:INFO:Declaring metric variables
2023-07-11 10:08:23,809:INFO:Importing untrained model
2023-07-11 10:08:23,809:INFO:Random Forest Regressor Imported successfully
2023-07-11 10:08:23,809:INFO:Starting cross validation
2023-07-11 10:08:23,809:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 10:08:29,100:INFO:Calculating mean and std
2023-07-11 10:08:29,100:WARNING:
2023-07-11 10:08:29,100:WARNING:Processing:  65%|#############################################8                        | 53/81 [01:12<00:48,  1.74s/it]
2023-07-11 10:08:29,100:WARNING:[A
2023-07-11 10:08:29,100:INFO:Creating metrics dataframe
2023-07-11 10:08:29,944:WARNING:
2023-07-11 10:08:29,944:WARNING:Processing:  67%|##############################################6                       | 54/81 [01:13<00:41,  1.54s/it]
2023-07-11 10:08:29,944:WARNING:[A
2023-07-11 10:08:29,944:INFO:Uploading results into container
2023-07-11 10:08:29,944:INFO:Uploading model into container now
2023-07-11 10:08:29,944:INFO:_master_model_container: 13
2023-07-11 10:08:29,944:INFO:_display_container: 2
2023-07-11 10:08:29,944:INFO:RandomForestRegressor(n_jobs=-1, random_state=7257)
2023-07-11 10:08:29,944:INFO:create_model() successfully completed......................................
2023-07-11 10:08:30,099:INFO:SubProcess create_model() end ==================================
2023-07-11 10:08:30,099:INFO:Creating metrics dataframe
2023-07-11 10:08:30,099:INFO:Initializing Extra Trees Regressor
2023-07-11 10:08:30,099:INFO:Total runtime is 1.2318580110867816 minutes
2023-07-11 10:08:30,099:INFO:SubProcess create_model() called ==================================
2023-07-11 10:08:30,099:INFO:Initializing create_model()
2023-07-11 10:08:30,099:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D76A1880>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1A8E32190>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 10:08:30,099:INFO:Checking exceptions
2023-07-11 10:08:30,099:INFO:Importing libraries
2023-07-11 10:08:30,099:INFO:Copying training dataset
2023-07-11 10:08:30,099:WARNING:
2023-07-11 10:08:30,099:WARNING:Processing:  68%|###############################################5                      | 55/81 [01:13<00:31,  1.20s/it]
2023-07-11 10:08:30,099:WARNING:[A
2023-07-11 10:08:30,099:INFO:Defining folds
2023-07-11 10:08:30,099:INFO:Declaring metric variables
2023-07-11 10:08:30,099:INFO:Importing untrained model
2023-07-11 10:08:30,099:INFO:Extra Trees Regressor Imported successfully
2023-07-11 10:08:30,099:INFO:Starting cross validation
2023-07-11 10:08:30,099:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 10:08:35,625:INFO:Calculating mean and std
2023-07-11 10:08:35,625:WARNING:
2023-07-11 10:08:35,625:WARNING:Processing:  70%|#################################################2                    | 57/81 [01:19<00:44,  1.84s/it]
2023-07-11 10:08:35,625:WARNING:[A
2023-07-11 10:08:35,625:INFO:Creating metrics dataframe
2023-07-11 10:08:36,329:WARNING:
2023-07-11 10:08:36,329:WARNING:Processing:  72%|##################################################1                   | 58/81 [01:20<00:36,  1.58s/it]
2023-07-11 10:08:36,329:WARNING:[A
2023-07-11 10:08:36,329:INFO:Uploading results into container
2023-07-11 10:08:36,329:INFO:Uploading model into container now
2023-07-11 10:08:36,329:INFO:_master_model_container: 14
2023-07-11 10:08:36,329:INFO:_display_container: 2
2023-07-11 10:08:36,329:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=7257)
2023-07-11 10:08:36,329:INFO:create_model() successfully completed......................................
2023-07-11 10:08:36,449:INFO:SubProcess create_model() end ==================================
2023-07-11 10:08:36,449:INFO:Creating metrics dataframe
2023-07-11 10:08:36,470:INFO:Initializing AdaBoost Regressor
2023-07-11 10:08:36,470:INFO:Total runtime is 1.338050635655721 minutes
2023-07-11 10:08:36,470:INFO:SubProcess create_model() called ==================================
2023-07-11 10:08:36,471:INFO:Initializing create_model()
2023-07-11 10:08:36,471:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D76A1880>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1A8E32190>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 10:08:36,471:INFO:Checking exceptions
2023-07-11 10:08:36,471:INFO:Importing libraries
2023-07-11 10:08:36,471:INFO:Copying training dataset
2023-07-11 10:08:36,473:WARNING:
2023-07-11 10:08:36,473:WARNING:Processing:  73%|##################################################9                   | 59/81 [01:20<00:27,  1.23s/it]
2023-07-11 10:08:36,473:WARNING:[A
2023-07-11 10:08:36,473:INFO:Defining folds
2023-07-11 10:08:36,473:INFO:Declaring metric variables
2023-07-11 10:08:36,473:INFO:Importing untrained model
2023-07-11 10:08:36,474:INFO:AdaBoost Regressor Imported successfully
2023-07-11 10:08:36,474:INFO:Starting cross validation
2023-07-11 10:08:36,475:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 10:08:38,041:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.58s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-11 10:08:41,609:INFO:Calculating mean and std
2023-07-11 10:08:41,609:WARNING:
2023-07-11 10:08:41,609:WARNING:Processing:  75%|####################################################7                 | 61/81 [01:25<00:35,  1.78s/it]
2023-07-11 10:08:41,609:WARNING:[A
2023-07-11 10:08:41,609:INFO:Creating metrics dataframe
2023-07-11 10:08:42,448:WARNING:
2023-07-11 10:08:42,448:WARNING:Processing:  77%|#####################################################5                | 62/81 [01:26<00:29,  1.57s/it]
2023-07-11 10:08:42,448:WARNING:[A
2023-07-11 10:08:42,448:INFO:Uploading results into container
2023-07-11 10:08:42,449:INFO:Uploading model into container now
2023-07-11 10:08:42,449:INFO:_master_model_container: 15
2023-07-11 10:08:42,449:INFO:_display_container: 2
2023-07-11 10:08:42,449:INFO:AdaBoostRegressor(random_state=7257)
2023-07-11 10:08:42,449:INFO:create_model() successfully completed......................................
2023-07-11 10:08:42,594:INFO:SubProcess create_model() end ==================================
2023-07-11 10:08:42,594:INFO:Creating metrics dataframe
2023-07-11 10:08:42,594:INFO:Initializing Gradient Boosting Regressor
2023-07-11 10:08:42,594:INFO:Total runtime is 1.4401152571042377 minutes
2023-07-11 10:08:42,594:INFO:SubProcess create_model() called ==================================
2023-07-11 10:08:42,594:INFO:Initializing create_model()
2023-07-11 10:08:42,594:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D76A1880>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1A8E32190>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 10:08:42,594:INFO:Checking exceptions
2023-07-11 10:08:42,594:INFO:Importing libraries
2023-07-11 10:08:42,594:INFO:Copying training dataset
2023-07-11 10:08:42,594:WARNING:
2023-07-11 10:08:42,594:WARNING:Processing:  78%|######################################################4               | 63/81 [01:26<00:21,  1.22s/it]
2023-07-11 10:08:42,594:WARNING:[A
2023-07-11 10:08:42,594:INFO:Defining folds
2023-07-11 10:08:42,594:INFO:Declaring metric variables
2023-07-11 10:08:42,594:INFO:Importing untrained model
2023-07-11 10:08:42,594:INFO:Gradient Boosting Regressor Imported successfully
2023-07-11 10:08:42,594:INFO:Starting cross validation
2023-07-11 10:08:42,594:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 10:08:47,565:INFO:Calculating mean and std
2023-07-11 10:08:47,565:WARNING:
2023-07-11 10:08:47,565:WARNING:Processing:  80%|########################################################1             | 65/81 [01:31<00:27,  1.74s/it]
2023-07-11 10:08:47,565:WARNING:[A
2023-07-11 10:08:47,565:INFO:Creating metrics dataframe
2023-07-11 10:08:48,297:WARNING:
2023-07-11 10:08:48,297:WARNING:Processing:  81%|#########################################################             | 66/81 [01:32<00:22,  1.51s/it]
2023-07-11 10:08:48,297:WARNING:[A
2023-07-11 10:08:48,297:INFO:Uploading results into container
2023-07-11 10:08:48,297:INFO:Uploading model into container now
2023-07-11 10:08:48,297:INFO:_master_model_container: 16
2023-07-11 10:08:48,297:INFO:_display_container: 2
2023-07-11 10:08:48,297:INFO:GradientBoostingRegressor(random_state=7257)
2023-07-11 10:08:48,297:INFO:create_model() successfully completed......................................
2023-07-11 10:08:48,423:INFO:SubProcess create_model() end ==================================
2023-07-11 10:08:48,423:INFO:Creating metrics dataframe
2023-07-11 10:08:48,423:INFO:Initializing Extreme Gradient Boosting
2023-07-11 10:08:48,423:INFO:Total runtime is 1.537264835834503 minutes
2023-07-11 10:08:48,439:INFO:SubProcess create_model() called ==================================
2023-07-11 10:08:48,440:INFO:Initializing create_model()
2023-07-11 10:08:48,440:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D76A1880>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1A8E32190>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 10:08:48,440:INFO:Checking exceptions
2023-07-11 10:08:48,440:INFO:Importing libraries
2023-07-11 10:08:48,440:INFO:Copying training dataset
2023-07-11 10:08:48,449:WARNING:
2023-07-11 10:08:48,449:WARNING:Processing:  83%|#########################################################9            | 67/81 [01:32<00:16,  1.18s/it]
2023-07-11 10:08:48,449:WARNING:[A
2023-07-11 10:08:48,449:INFO:Defining folds
2023-07-11 10:08:48,449:INFO:Declaring metric variables
2023-07-11 10:08:48,450:INFO:Importing untrained model
2023-07-11 10:08:48,451:INFO:Extreme Gradient Boosting Imported successfully
2023-07-11 10:08:48,451:INFO:Starting cross validation
2023-07-11 10:08:48,455:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 10:08:53,482:INFO:Calculating mean and std
2023-07-11 10:08:53,482:WARNING:
2023-07-11 10:08:53,482:WARNING:Processing:  85%|###########################################################6          | 69/81 [01:37<00:20,  1.73s/it]
2023-07-11 10:08:53,482:WARNING:[A
2023-07-11 10:08:53,482:INFO:Creating metrics dataframe
2023-07-11 10:08:54,277:WARNING:
2023-07-11 10:08:54,277:WARNING:Processing:  86%|############################################################4         | 70/81 [01:38<00:16,  1.52s/it]
2023-07-11 10:08:54,277:WARNING:[A
2023-07-11 10:08:54,277:INFO:Uploading results into container
2023-07-11 10:08:54,278:INFO:Uploading model into container now
2023-07-11 10:08:54,278:INFO:_master_model_container: 17
2023-07-11 10:08:54,278:INFO:_display_container: 2
2023-07-11 10:08:54,279:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             n_estimators=100, n_jobs=-1, num_parallel_tree=None,
             predictor=None, random_state=7257, ...)
2023-07-11 10:08:54,279:INFO:create_model() successfully completed......................................
2023-07-11 10:08:54,408:INFO:SubProcess create_model() end ==================================
2023-07-11 10:08:54,408:INFO:Creating metrics dataframe
2023-07-11 10:08:54,424:INFO:Initializing Light Gradient Boosting Machine
2023-07-11 10:08:54,424:INFO:Total runtime is 1.6372745831807451 minutes
2023-07-11 10:08:54,424:INFO:SubProcess create_model() called ==================================
2023-07-11 10:08:54,424:INFO:Initializing create_model()
2023-07-11 10:08:54,424:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D76A1880>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1A8E32190>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 10:08:54,424:INFO:Checking exceptions
2023-07-11 10:08:54,424:INFO:Importing libraries
2023-07-11 10:08:54,424:INFO:Copying training dataset
2023-07-11 10:08:54,424:WARNING:
2023-07-11 10:08:54,424:WARNING:Processing:  88%|#############################################################3        | 71/81 [01:38<00:11,  1.18s/it]
2023-07-11 10:08:54,424:WARNING:[A
2023-07-11 10:08:54,424:INFO:Defining folds
2023-07-11 10:08:54,424:INFO:Declaring metric variables
2023-07-11 10:08:54,424:INFO:Importing untrained model
2023-07-11 10:08:54,424:INFO:Light Gradient Boosting Machine Imported successfully
2023-07-11 10:08:54,424:INFO:Starting cross validation
2023-07-11 10:08:54,424:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 10:08:59,193:INFO:Calculating mean and std
2023-07-11 10:08:59,193:WARNING:
2023-07-11 10:08:59,193:WARNING:Processing:  90%|###############################################################       | 73/81 [01:43<00:13,  1.68s/it]
2023-07-11 10:08:59,193:WARNING:[A
2023-07-11 10:08:59,193:INFO:Creating metrics dataframe
2023-07-11 10:09:00,079:WARNING:
2023-07-11 10:09:00,079:WARNING:Processing:  91%|###############################################################9      | 74/81 [01:43<00:10,  1.50s/it]
2023-07-11 10:09:00,079:WARNING:[A
2023-07-11 10:09:00,079:INFO:Uploading results into container
2023-07-11 10:09:00,080:INFO:Uploading model into container now
2023-07-11 10:09:00,080:INFO:_master_model_container: 18
2023-07-11 10:09:00,080:INFO:_display_container: 2
2023-07-11 10:09:00,081:INFO:LGBMRegressor(random_state=7257)
2023-07-11 10:09:00,081:INFO:create_model() successfully completed......................................
2023-07-11 10:09:00,225:INFO:SubProcess create_model() end ==================================
2023-07-11 10:09:00,225:INFO:Creating metrics dataframe
2023-07-11 10:09:00,225:INFO:Initializing Dummy Regressor
2023-07-11 10:09:00,225:INFO:Total runtime is 1.7339623808860776 minutes
2023-07-11 10:09:00,225:INFO:SubProcess create_model() called ==================================
2023-07-11 10:09:00,225:INFO:Initializing create_model()
2023-07-11 10:09:00,225:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D76A1880>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1A8E32190>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 10:09:00,225:INFO:Checking exceptions
2023-07-11 10:09:00,225:INFO:Importing libraries
2023-07-11 10:09:00,225:INFO:Copying training dataset
2023-07-11 10:09:00,242:WARNING:
2023-07-11 10:09:00,242:WARNING:Processing:  93%|################################################################8     | 75/81 [01:44<00:07,  1.17s/it]
2023-07-11 10:09:00,242:WARNING:[A
2023-07-11 10:09:00,242:INFO:Defining folds
2023-07-11 10:09:00,242:INFO:Declaring metric variables
2023-07-11 10:09:00,242:INFO:Importing untrained model
2023-07-11 10:09:00,242:INFO:Dummy Regressor Imported successfully
2023-07-11 10:09:00,242:INFO:Starting cross validation
2023-07-11 10:09:00,249:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 10:09:04,994:INFO:Calculating mean and std
2023-07-11 10:09:04,994:WARNING:
2023-07-11 10:09:04,994:WARNING:Processing:  95%|##################################################################5   | 77/81 [01:48<00:06,  1.67s/it]
2023-07-11 10:09:04,994:WARNING:[A
2023-07-11 10:09:04,994:INFO:Creating metrics dataframe
2023-07-11 10:09:05,874:WARNING:
2023-07-11 10:09:05,875:WARNING:Processing:  96%|###################################################################4  | 78/81 [01:49<00:04,  1.49s/it]
2023-07-11 10:09:05,875:WARNING:[A
2023-07-11 10:09:05,875:INFO:Uploading results into container
2023-07-11 10:09:05,875:INFO:Uploading model into container now
2023-07-11 10:09:05,876:INFO:_master_model_container: 19
2023-07-11 10:09:05,876:INFO:_display_container: 2
2023-07-11 10:09:05,876:INFO:DummyRegressor()
2023-07-11 10:09:05,876:INFO:create_model() successfully completed......................................
2023-07-11 10:09:06,009:INFO:SubProcess create_model() end ==================================
2023-07-11 10:09:06,009:INFO:Creating metrics dataframe
2023-07-11 10:09:06,009:WARNING:
2023-07-11 10:09:06,009:WARNING:Processing:  98%|####################################################################2 | 79/81 [01:49<00:02,  1.16s/it]
2023-07-11 10:09:06,009:WARNING:[A
2023-07-11 10:09:06,009:INFO:Initializing create_model()
2023-07-11 10:09:06,009:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001E1D76A1880>, estimator=OrthogonalMatchingPursuit(), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-07-11 10:09:06,009:INFO:Checking exceptions
2023-07-11 10:09:06,009:INFO:Importing libraries
2023-07-11 10:09:06,009:INFO:Copying training dataset
2023-07-11 10:09:06,009:INFO:Defining folds
2023-07-11 10:09:06,009:INFO:Declaring metric variables
2023-07-11 10:09:06,009:INFO:Importing untrained model
2023-07-11 10:09:06,009:INFO:Declaring custom model
2023-07-11 10:09:06,009:INFO:Orthogonal Matching Pursuit Imported successfully
2023-07-11 10:09:06,009:INFO:Cross validation set to False
2023-07-11 10:09:06,009:INFO:Fitting Model
2023-07-11 10:09:06,624:INFO:OrthogonalMatchingPursuit()
2023-07-11 10:09:06,624:INFO:create_model() successfully completed......................................
2023-07-11 10:09:06,747:WARNING:
2023-07-11 10:09:06,747:WARNING:Processing: 100%|######################################################################| 81/81 [01:50<00:00,  1.20it/s]
2023-07-11 10:09:06,747:WARNING:[A
2023-07-11 10:09:06,747:WARNING:
2023-07-11 10:09:06,747:WARNING:                                                                                                                       
2023-07-11 10:09:06,747:WARNING:[A
2023-07-11 10:09:06,763:INFO:                                    Model      MAE         MSE      RMSE          R2   RMSLE    MAPE  TT (Sec)
2023-07-11 10:09:06,763:INFO:omp           Orthogonal Matching Pursuit   0.8029      1.6549    1.2341      0.8354  0.2159  0.0836     0.448
2023-07-11 10:09:06,763:INFO:huber                     Huber Regressor   0.7941      1.6666    1.2399      0.8330  0.2402  0.0818     0.474
2023-07-11 10:09:06,763:INFO:br                         Bayesian Ridge   0.8094      1.6785    1.2412      0.8321  0.2275  0.0840     0.462
2023-07-11 10:09:06,763:INFO:en                            Elastic Net   0.7862      1.7291    1.2603      0.8304  0.2525  0.0807     0.454
2023-07-11 10:09:06,763:INFO:ridge                    Ridge Regression   0.8305      1.7196    1.2603      0.8262  0.2260  0.0860     0.453
2023-07-11 10:09:06,763:INFO:lr                      Linear Regression   0.8325      1.7235    1.2621      0.8257  0.2261  0.0862     0.630
2023-07-11 10:09:06,763:INFO:lasso                    Lasso Regression   0.8063      1.7641    1.2740      0.8255  0.2443  0.0832     0.446
2023-07-11 10:09:06,763:INFO:llar         Lasso Least Angle Regression   0.8064      1.7642    1.2741      0.8255  0.2442  0.0832     0.455
2023-07-11 10:09:06,763:INFO:et                  Extra Trees Regressor   0.8566      1.8664    1.2941      0.8106  0.2294  0.0926     0.553
2023-07-11 10:09:06,763:INFO:rf                Random Forest Regressor   0.8583      1.9162    1.3283      0.8074  0.2424  0.0888     0.529
2023-07-11 10:09:06,763:INFO:ada                    AdaBoost Regressor   0.9401      1.9577    1.3392      0.8058  0.2417  0.0992     0.513
2023-07-11 10:09:06,763:INFO:gbr           Gradient Boosting Regressor   0.8599      2.0104    1.3634      0.8035  0.2441  0.0923     0.497
2023-07-11 10:09:06,763:INFO:par          Passive Aggressive Regressor   0.9882      2.0728    1.3926      0.7910  0.2272  0.1009     0.463
2023-07-11 10:09:06,763:INFO:lightgbm  Light Gradient Boosting Machine   0.9300      2.1836    1.4207      0.7867  0.2703  0.0968     0.477
2023-07-11 10:09:06,763:INFO:xgboost         Extreme Gradient Boosting   0.9222      2.1542    1.4181      0.7828  0.2541  0.0990     0.503
2023-07-11 10:09:06,763:INFO:knn                 K Neighbors Regressor   0.9818      2.2730    1.4667      0.7717  0.2473  0.1012     0.464
2023-07-11 10:09:06,763:INFO:dt                Decision Tree Regressor   1.0644      3.4734    1.7788      0.6378  0.3009  0.1142     0.465
2023-07-11 10:09:06,763:INFO:dummy                     Dummy Regressor   2.3958     10.3901    3.2017     -0.0232  0.4219  0.2090     0.474
2023-07-11 10:09:06,763:INFO:lar                Least Angle Regression  84.0409  99246.7407  101.1498 -10066.4295  0.6564  7.2645     0.463
2023-07-11 10:09:06,763:INFO:_master_model_container: 19
2023-07-11 10:09:06,763:INFO:_display_container: 2
2023-07-11 10:09:06,763:INFO:OrthogonalMatchingPursuit()
2023-07-11 10:09:06,763:INFO:compare_models() successfully completed......................................
2023-07-11 10:11:15,816:INFO:PyCaret ClassificationExperiment
2023-07-11 10:11:15,816:INFO:Logging name: clf-default-name
2023-07-11 10:11:15,816:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2023-07-11 10:11:15,816:INFO:version 3.0.2
2023-07-11 10:11:15,816:INFO:Initializing setup()
2023-07-11 10:11:15,816:INFO:self.USI: 8450
2023-07-11 10:11:15,816:INFO:self._variable_keys: {'_ml_usecase', 'log_plots_param', 'USI', 'X_train', 'pipeline', 'gpu_param', 'exp_name_log', 'y', 'is_multiclass', 'html_param', 'X', 'data', 'exp_id', 'y_train', 'target_param', 'fix_imbalance', 'X_test', 'fold_groups_param', 'n_jobs_param', 'idx', 'fold_generator', 'logging_param', 'memory', 'gpu_n_jobs_param', 'fold_shuffle_param', '_available_plots', 'seed', 'y_test'}
2023-07-11 10:11:15,816:INFO:Checking environment
2023-07-11 10:11:15,816:INFO:python_version: 3.9.13
2023-07-11 10:11:15,816:INFO:python_build: ('main', 'Oct 13 2022 21:23:06')
2023-07-11 10:11:15,816:INFO:machine: AMD64
2023-07-11 10:11:15,816:INFO:platform: Windows-10-10.0.22621-SP0
2023-07-11 10:11:15,816:INFO:Memory: svmem(total=16893358080, available=2720653312, percent=83.9, used=14172704768, free=2720653312)
2023-07-11 10:11:15,816:INFO:Physical Core: 8
2023-07-11 10:11:15,816:INFO:Logical Core: 16
2023-07-11 10:11:15,816:INFO:Checking libraries
2023-07-11 10:11:15,816:INFO:System:
2023-07-11 10:11:15,816:INFO:    python: 3.9.13 (main, Oct 13 2022, 21:23:06) [MSC v.1916 64 bit (AMD64)]
2023-07-11 10:11:15,816:INFO:executable: C:\Users\didit\anaconda3\python.exe
2023-07-11 10:11:15,816:INFO:   machine: Windows-10-10.0.22621-SP0
2023-07-11 10:11:15,816:INFO:PyCaret required dependencies:
2023-07-11 10:11:15,816:INFO:                 pip: 23.0.1
2023-07-11 10:11:15,816:INFO:          setuptools: 67.8.0
2023-07-11 10:11:15,816:INFO:             pycaret: 3.0.2
2023-07-11 10:11:15,816:INFO:             IPython: 8.12.0
2023-07-11 10:11:15,816:INFO:          ipywidgets: 8.0.4
2023-07-11 10:11:15,816:INFO:                tqdm: 4.65.0
2023-07-11 10:11:15,816:INFO:               numpy: 1.21.5
2023-07-11 10:11:15,816:INFO:              pandas: 1.5.3
2023-07-11 10:11:15,816:INFO:              jinja2: 3.1.2
2023-07-11 10:11:15,816:INFO:               scipy: 1.10.1
2023-07-11 10:11:15,816:INFO:              joblib: 1.2.0
2023-07-11 10:11:15,816:INFO:             sklearn: 1.2.2
2023-07-11 10:11:15,816:INFO:                pyod: 1.0.9
2023-07-11 10:11:15,816:INFO:            imblearn: 0.10.1
2023-07-11 10:11:15,816:INFO:   category_encoders: 2.6.1
2023-07-11 10:11:15,816:INFO:            lightgbm: 3.3.5
2023-07-11 10:11:15,816:INFO:               numba: 0.57.0
2023-07-11 10:11:15,816:INFO:            requests: 2.29.0
2023-07-11 10:11:15,816:INFO:          matplotlib: 3.7.1
2023-07-11 10:11:15,816:INFO:          scikitplot: 0.3.7
2023-07-11 10:11:15,816:INFO:         yellowbrick: 1.5
2023-07-11 10:11:15,816:INFO:              plotly: 5.9.0
2023-07-11 10:11:15,816:INFO:             kaleido: 0.2.1
2023-07-11 10:11:15,816:INFO:         statsmodels: 0.13.5
2023-07-11 10:11:15,816:INFO:              sktime: 0.17.0
2023-07-11 10:11:15,816:INFO:               tbats: 1.1.3
2023-07-11 10:11:15,816:INFO:            pmdarima: 2.0.3
2023-07-11 10:11:15,816:INFO:              psutil: 5.9.0
2023-07-11 10:11:15,816:INFO:PyCaret optional dependencies:
2023-07-11 10:11:15,816:INFO:                shap: 0.41.0
2023-07-11 10:11:15,816:INFO:           interpret: Not installed
2023-07-11 10:11:15,816:INFO:                umap: Not installed
2023-07-11 10:11:15,816:INFO:    pandas_profiling: 4.3.1
2023-07-11 10:11:15,816:INFO:  explainerdashboard: Not installed
2023-07-11 10:11:15,816:INFO:             autoviz: Not installed
2023-07-11 10:11:15,816:INFO:           fairlearn: Not installed
2023-07-11 10:11:15,816:INFO:             xgboost: 1.7.6
2023-07-11 10:11:15,816:INFO:            catboost: Not installed
2023-07-11 10:11:15,816:INFO:              kmodes: Not installed
2023-07-11 10:11:15,816:INFO:             mlxtend: Not installed
2023-07-11 10:11:15,816:INFO:       statsforecast: Not installed
2023-07-11 10:11:15,816:INFO:        tune_sklearn: Not installed
2023-07-11 10:11:15,816:INFO:                 ray: Not installed
2023-07-11 10:11:15,816:INFO:            hyperopt: Not installed
2023-07-11 10:11:15,816:INFO:              optuna: Not installed
2023-07-11 10:11:15,816:INFO:               skopt: Not installed
2023-07-11 10:11:15,816:INFO:              mlflow: 2.4.2
2023-07-11 10:11:15,816:INFO:              gradio: Not installed
2023-07-11 10:11:15,816:INFO:             fastapi: 0.95.2
2023-07-11 10:11:15,816:INFO:             uvicorn: 0.22.0
2023-07-11 10:11:15,816:INFO:              m2cgen: Not installed
2023-07-11 10:11:15,816:INFO:           evidently: Not installed
2023-07-11 10:11:15,816:INFO:               fugue: Not installed
2023-07-11 10:11:15,816:INFO:           streamlit: 1.23.1
2023-07-11 10:11:15,816:INFO:             prophet: Not installed
2023-07-11 10:11:15,816:INFO:None
2023-07-11 10:11:15,816:INFO:Set up data.
2023-07-11 10:11:15,847:INFO:Set up train/test split.
2023-07-11 10:11:39,713:INFO:PyCaret ClassificationExperiment
2023-07-11 10:11:39,713:INFO:Logging name: clf-default-name
2023-07-11 10:11:39,713:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2023-07-11 10:11:39,713:INFO:version 3.0.2
2023-07-11 10:11:39,713:INFO:Initializing setup()
2023-07-11 10:11:39,713:INFO:self.USI: 791d
2023-07-11 10:11:39,713:INFO:self._variable_keys: {'_ml_usecase', 'log_plots_param', 'USI', 'X_train', 'pipeline', 'gpu_param', 'exp_name_log', 'y', 'is_multiclass', 'html_param', 'X', 'data', 'exp_id', 'y_train', 'target_param', 'fix_imbalance', 'X_test', 'fold_groups_param', 'n_jobs_param', 'idx', 'fold_generator', 'logging_param', 'memory', 'gpu_n_jobs_param', 'fold_shuffle_param', '_available_plots', 'seed', 'y_test'}
2023-07-11 10:11:39,713:INFO:Checking environment
2023-07-11 10:11:39,713:INFO:python_version: 3.9.13
2023-07-11 10:11:39,713:INFO:python_build: ('main', 'Oct 13 2022 21:23:06')
2023-07-11 10:11:39,713:INFO:machine: AMD64
2023-07-11 10:11:39,713:INFO:platform: Windows-10-10.0.22621-SP0
2023-07-11 10:11:39,713:INFO:Memory: svmem(total=16893358080, available=3009302528, percent=82.2, used=13884055552, free=3009302528)
2023-07-11 10:11:39,713:INFO:Physical Core: 8
2023-07-11 10:11:39,713:INFO:Logical Core: 16
2023-07-11 10:11:39,713:INFO:Checking libraries
2023-07-11 10:11:39,713:INFO:System:
2023-07-11 10:11:39,713:INFO:    python: 3.9.13 (main, Oct 13 2022, 21:23:06) [MSC v.1916 64 bit (AMD64)]
2023-07-11 10:11:39,713:INFO:executable: C:\Users\didit\anaconda3\python.exe
2023-07-11 10:11:39,713:INFO:   machine: Windows-10-10.0.22621-SP0
2023-07-11 10:11:39,713:INFO:PyCaret required dependencies:
2023-07-11 10:11:39,713:INFO:                 pip: 23.0.1
2023-07-11 10:11:39,713:INFO:          setuptools: 67.8.0
2023-07-11 10:11:39,713:INFO:             pycaret: 3.0.2
2023-07-11 10:11:39,713:INFO:             IPython: 8.12.0
2023-07-11 10:11:39,713:INFO:          ipywidgets: 8.0.4
2023-07-11 10:11:39,713:INFO:                tqdm: 4.65.0
2023-07-11 10:11:39,713:INFO:               numpy: 1.21.5
2023-07-11 10:11:39,713:INFO:              pandas: 1.5.3
2023-07-11 10:11:39,713:INFO:              jinja2: 3.1.2
2023-07-11 10:11:39,713:INFO:               scipy: 1.10.1
2023-07-11 10:11:39,713:INFO:              joblib: 1.2.0
2023-07-11 10:11:39,713:INFO:             sklearn: 1.2.2
2023-07-11 10:11:39,713:INFO:                pyod: 1.0.9
2023-07-11 10:11:39,713:INFO:            imblearn: 0.10.1
2023-07-11 10:11:39,713:INFO:   category_encoders: 2.6.1
2023-07-11 10:11:39,713:INFO:            lightgbm: 3.3.5
2023-07-11 10:11:39,713:INFO:               numba: 0.57.0
2023-07-11 10:11:39,713:INFO:            requests: 2.29.0
2023-07-11 10:11:39,713:INFO:          matplotlib: 3.7.1
2023-07-11 10:11:39,713:INFO:          scikitplot: 0.3.7
2023-07-11 10:11:39,713:INFO:         yellowbrick: 1.5
2023-07-11 10:11:39,713:INFO:              plotly: 5.9.0
2023-07-11 10:11:39,713:INFO:             kaleido: 0.2.1
2023-07-11 10:11:39,713:INFO:         statsmodels: 0.13.5
2023-07-11 10:11:39,713:INFO:              sktime: 0.17.0
2023-07-11 10:11:39,713:INFO:               tbats: 1.1.3
2023-07-11 10:11:39,713:INFO:            pmdarima: 2.0.3
2023-07-11 10:11:39,713:INFO:              psutil: 5.9.0
2023-07-11 10:11:39,713:INFO:PyCaret optional dependencies:
2023-07-11 10:11:39,713:INFO:                shap: 0.41.0
2023-07-11 10:11:39,713:INFO:           interpret: Not installed
2023-07-11 10:11:39,713:INFO:                umap: Not installed
2023-07-11 10:11:39,713:INFO:    pandas_profiling: 4.3.1
2023-07-11 10:11:39,713:INFO:  explainerdashboard: Not installed
2023-07-11 10:11:39,713:INFO:             autoviz: Not installed
2023-07-11 10:11:39,713:INFO:           fairlearn: Not installed
2023-07-11 10:11:39,713:INFO:             xgboost: 1.7.6
2023-07-11 10:11:39,713:INFO:            catboost: Not installed
2023-07-11 10:11:39,713:INFO:              kmodes: Not installed
2023-07-11 10:11:39,713:INFO:             mlxtend: Not installed
2023-07-11 10:11:39,713:INFO:       statsforecast: Not installed
2023-07-11 10:11:39,713:INFO:        tune_sklearn: Not installed
2023-07-11 10:11:39,713:INFO:                 ray: Not installed
2023-07-11 10:11:39,713:INFO:            hyperopt: Not installed
2023-07-11 10:11:39,713:INFO:              optuna: Not installed
2023-07-11 10:11:39,713:INFO:               skopt: Not installed
2023-07-11 10:11:39,713:INFO:              mlflow: 2.4.2
2023-07-11 10:11:39,713:INFO:              gradio: Not installed
2023-07-11 10:11:39,713:INFO:             fastapi: 0.95.2
2023-07-11 10:11:39,713:INFO:             uvicorn: 0.22.0
2023-07-11 10:11:39,713:INFO:              m2cgen: Not installed
2023-07-11 10:11:39,713:INFO:           evidently: Not installed
2023-07-11 10:11:39,713:INFO:               fugue: Not installed
2023-07-11 10:11:39,713:INFO:           streamlit: 1.23.1
2023-07-11 10:11:39,713:INFO:             prophet: Not installed
2023-07-11 10:11:39,713:INFO:None
2023-07-11 10:11:39,713:INFO:Set up data.
2023-07-11 10:11:39,728:INFO:Set up train/test split.
2023-07-11 10:11:39,751:INFO:Set up index.
2023-07-11 10:11:39,751:INFO:Set up folding strategy.
2023-07-11 10:11:39,751:INFO:Assigning column types.
2023-07-11 10:11:39,753:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-07-11 10:11:39,783:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 10:11:39,785:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-11 10:11:39,804:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 10:11:39,804:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 10:11:39,835:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-11 10:11:39,835:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-11 10:11:39,851:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 10:11:39,851:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 10:11:39,851:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-07-11 10:11:39,883:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-11 10:11:39,914:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 10:11:39,914:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 10:11:39,950:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-11 10:11:39,972:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 10:11:39,974:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 10:11:39,974:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2023-07-11 10:11:40,019:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 10:11:40,035:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 10:11:40,098:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 10:11:40,098:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 10:11:40,098:INFO:Preparing preprocessing pipeline...
2023-07-11 10:11:40,098:INFO:Set up label encoding.
2023-07-11 10:11:40,098:INFO:Set up simple imputation.
2023-07-11 10:11:40,098:INFO:Set up encoding of ordinal features.
2023-07-11 10:11:40,113:INFO:Set up encoding of categorical features.
2023-07-11 10:11:40,233:INFO:Finished creating preprocessing pipeline.
2023-07-11 10:11:40,312:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\didit\AppData\Local\Temp\joblib),
         steps=[('label_encoding',
                 TransformerWrapperWithInverse(exclude=None, include=None,
                                               transformer=LabelEncoder())),
                ('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['age', 'Medu', 'Fedu',
                                             'traveltime', 'studytime',
                                             'failures', 'famrel', 'freetime',
                                             'goout', 'Dalc', 'Walc', 'h...
                                                                         'mapping': no     0
yes    1
NaN   -1
dtype: int64}],
                                                               return_df=True,
                                                               verbose=0))),
                ('onehot_encoding',
                 TransformerWrapper(exclude=None,
                                    include=['Mjob', 'Fjob', 'reason',
                                             'guardian'],
                                    transformer=OneHotEncoder(cols=['Mjob',
                                                                    'Fjob',
                                                                    'reason',
                                                                    'guardian'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0)))],
         verbose=False)
2023-07-11 10:11:40,312:INFO:Creating final display dataframe.
2023-07-11 10:11:40,650:INFO:Setup _display_container:                     Description             Value
0                    Session id              5501
1                        Target              paid
2                   Target type            Binary
3                Target mapping     no: 0, yes: 1
4           Original data shape         (649, 33)
5        Transformed data shape         (649, 46)
6   Transformed train set shape         (454, 46)
7    Transformed test set shape         (195, 46)
8              Ordinal features                12
9              Numeric features                16
10         Categorical features                16
11                   Preprocess              True
12              Imputation type            simple
13           Numeric imputation              mean
14       Categorical imputation              mode
15     Maximum one-hot encoding                25
16              Encoding method              None
17               Fold Generator   StratifiedKFold
18                  Fold Number                10
19                     CPU Jobs                -1
20                      Use GPU             False
21               Log Experiment             False
22              Experiment Name  clf-default-name
23                          USI              791d
2023-07-11 10:11:40,650:INFO:                    Description             Value
2023-07-11 10:11:40,650:INFO:0                    Session id              5501
2023-07-11 10:11:40,650:INFO:1                        Target              paid
2023-07-11 10:11:40,665:INFO:2                   Target type            Binary
2023-07-11 10:11:40,665:INFO:3                Target mapping     no: 0, yes: 1
2023-07-11 10:11:40,665:INFO:4           Original data shape         (649, 33)
2023-07-11 10:11:40,665:INFO:5        Transformed data shape         (649, 46)
2023-07-11 10:11:40,665:INFO:6   Transformed train set shape         (454, 46)
2023-07-11 10:11:40,665:INFO:7    Transformed test set shape         (195, 46)
2023-07-11 10:11:40,665:INFO:8              Ordinal features                12
2023-07-11 10:11:40,665:INFO:9              Numeric features                16
2023-07-11 10:11:40,665:INFO:10         Categorical features                16
2023-07-11 10:11:40,665:INFO:11                   Preprocess              True
2023-07-11 10:11:40,665:INFO:12              Imputation type            simple
2023-07-11 10:11:40,665:INFO:13           Numeric imputation              mean
2023-07-11 10:11:40,665:INFO:14       Categorical imputation              mode
2023-07-11 10:11:40,665:INFO:15     Maximum one-hot encoding                25
2023-07-11 10:11:40,665:INFO:16              Encoding method              None
2023-07-11 10:11:40,665:INFO:17               Fold Generator   StratifiedKFold
2023-07-11 10:11:40,665:INFO:18                  Fold Number                10
2023-07-11 10:11:40,665:INFO:19                     CPU Jobs                -1
2023-07-11 10:11:40,665:INFO:20                      Use GPU             False
2023-07-11 10:11:40,665:INFO:21               Log Experiment             False
2023-07-11 10:11:40,665:INFO:22              Experiment Name  clf-default-name
2023-07-11 10:11:40,665:INFO:23                          USI              791d
2023-07-11 10:11:40,713:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 10:11:40,713:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 10:11:40,777:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-11 10:11:40,781:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-11 10:11:40,781:INFO:setup() successfully completed in 1.43s...............
2023-07-11 10:11:40,784:INFO:Initializing compare_models()
2023-07-11 10:11:40,784:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E1D8B218B0>, include=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000001E1D8B218B0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2023-07-11 10:11:40,784:INFO:Checking exceptions
2023-07-11 10:11:40,786:INFO:Preparing display monitor
2023-07-11 10:11:40,789:WARNING:Processing:   0%|                                                                               | 0/65 [00:00<?, ?it/s]
2023-07-11 10:11:40,790:INFO:Initializing Logistic Regression
2023-07-11 10:11:40,790:INFO:Total runtime is 0.0 minutes
2023-07-11 10:11:40,790:INFO:SubProcess create_model() called ==================================
2023-07-11 10:11:40,791:INFO:Initializing create_model()
2023-07-11 10:11:40,791:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E1D8B218B0>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1DD0C5B20>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 10:11:40,791:INFO:Checking exceptions
2023-07-11 10:11:40,791:INFO:Importing libraries
2023-07-11 10:11:40,791:INFO:Copying training dataset
2023-07-11 10:11:40,795:INFO:Defining folds
2023-07-11 10:11:40,795:INFO:Declaring metric variables
2023-07-11 10:11:40,795:INFO:Importing untrained model
2023-07-11 10:11:40,795:INFO:Logistic Regression Imported successfully
2023-07-11 10:11:40,795:INFO:Starting cross validation
2023-07-11 10:11:40,798:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 10:11:41,603:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:11:41,634:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:11:41,650:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:11:41,650:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:11:41,650:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:11:41,650:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:11:41,665:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:11:41,681:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:11:41,684:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:11:45,345:INFO:Calculating mean and std
2023-07-11 10:11:45,345:WARNING:Processing:   8%|#####4                                                                 | 5/65 [00:04<00:54,  1.10it/s]
2023-07-11 10:11:45,345:INFO:Creating metrics dataframe
2023-07-11 10:11:46,180:WARNING:Processing:   9%|######5                                                                | 6/65 [00:05<00:52,  1.12it/s]
2023-07-11 10:11:46,180:INFO:Uploading results into container
2023-07-11 10:11:46,181:INFO:Uploading model into container now
2023-07-11 10:11:46,181:INFO:_master_model_container: 1
2023-07-11 10:11:46,181:INFO:_display_container: 2
2023-07-11 10:11:46,182:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=5501, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2023-07-11 10:11:46,182:INFO:create_model() successfully completed......................................
2023-07-11 10:11:46,298:INFO:SubProcess create_model() end ==================================
2023-07-11 10:11:46,298:INFO:Creating metrics dataframe
2023-07-11 10:11:46,298:INFO:Initializing K Neighbors Classifier
2023-07-11 10:11:46,298:INFO:Total runtime is 0.0917995810508728 minutes
2023-07-11 10:11:46,298:INFO:SubProcess create_model() called ==================================
2023-07-11 10:11:46,298:INFO:Initializing create_model()
2023-07-11 10:11:46,298:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E1D8B218B0>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1DD0C5B20>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 10:11:46,298:INFO:Checking exceptions
2023-07-11 10:11:46,298:INFO:Importing libraries
2023-07-11 10:11:46,298:INFO:Copying training dataset
2023-07-11 10:11:46,314:WARNING:Processing:  11%|#######6                                                               | 7/65 [00:05<00:41,  1.41it/s]
2023-07-11 10:11:46,314:INFO:Defining folds
2023-07-11 10:11:46,314:INFO:Declaring metric variables
2023-07-11 10:11:46,314:INFO:Importing untrained model
2023-07-11 10:11:46,314:INFO:K Neighbors Classifier Imported successfully
2023-07-11 10:11:46,314:INFO:Starting cross validation
2023-07-11 10:11:46,314:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 10:11:47,034:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:11:47,081:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:11:47,145:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:11:47,153:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:11:47,191:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:11:47,195:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:11:47,198:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:11:47,203:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:11:47,203:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:11:47,281:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:11:50,802:INFO:Calculating mean and std
2023-07-11 10:11:50,802:WARNING:Processing:  14%|#########8                                                             | 9/65 [00:10<01:14,  1.34s/it]
2023-07-11 10:11:50,802:INFO:Creating metrics dataframe
2023-07-11 10:11:51,819:WARNING:Processing:  15%|##########7                                                           | 10/65 [00:11<01:09,  1.26s/it]
2023-07-11 10:11:51,819:INFO:Uploading results into container
2023-07-11 10:11:51,819:INFO:Uploading model into container now
2023-07-11 10:11:51,819:INFO:_master_model_container: 2
2023-07-11 10:11:51,819:INFO:_display_container: 2
2023-07-11 10:11:51,819:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2023-07-11 10:11:51,819:INFO:create_model() successfully completed......................................
2023-07-11 10:11:51,961:INFO:SubProcess create_model() end ==================================
2023-07-11 10:11:51,961:INFO:Creating metrics dataframe
2023-07-11 10:11:51,966:INFO:Initializing Naive Bayes
2023-07-11 10:11:51,966:INFO:Total runtime is 0.18626407782236734 minutes
2023-07-11 10:11:51,966:INFO:SubProcess create_model() called ==================================
2023-07-11 10:11:51,966:INFO:Initializing create_model()
2023-07-11 10:11:51,966:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E1D8B218B0>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1DD0C5B20>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 10:11:51,967:INFO:Checking exceptions
2023-07-11 10:11:51,967:INFO:Importing libraries
2023-07-11 10:11:51,967:INFO:Copying training dataset
2023-07-11 10:11:51,970:WARNING:Processing:  17%|###########8                                                          | 11/65 [00:11<00:53,  1.01it/s]
2023-07-11 10:11:51,970:INFO:Defining folds
2023-07-11 10:11:51,970:INFO:Declaring metric variables
2023-07-11 10:11:51,970:INFO:Importing untrained model
2023-07-11 10:11:51,970:INFO:Naive Bayes Imported successfully
2023-07-11 10:11:51,971:INFO:Starting cross validation
2023-07-11 10:11:51,972:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 10:11:56,450:INFO:Calculating mean and std
2023-07-11 10:11:56,450:WARNING:Processing:  20%|##############                                                        | 13/65 [00:15<01:18,  1.50s/it]
2023-07-11 10:11:56,450:INFO:Creating metrics dataframe
2023-07-11 10:11:57,418:WARNING:Processing:  22%|###############                                                       | 14/65 [00:16<01:10,  1.38s/it]
2023-07-11 10:11:57,418:INFO:Uploading results into container
2023-07-11 10:11:57,418:INFO:Uploading model into container now
2023-07-11 10:11:57,418:INFO:_master_model_container: 3
2023-07-11 10:11:57,418:INFO:_display_container: 2
2023-07-11 10:11:57,418:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2023-07-11 10:11:57,418:INFO:create_model() successfully completed......................................
2023-07-11 10:11:57,546:INFO:SubProcess create_model() end ==================================
2023-07-11 10:11:57,546:INFO:Creating metrics dataframe
2023-07-11 10:11:57,546:INFO:Initializing Decision Tree Classifier
2023-07-11 10:11:57,546:INFO:Total runtime is 0.2792625586191813 minutes
2023-07-11 10:11:57,546:INFO:SubProcess create_model() called ==================================
2023-07-11 10:11:57,546:INFO:Initializing create_model()
2023-07-11 10:11:57,546:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E1D8B218B0>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1DD0C5B20>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 10:11:57,546:INFO:Checking exceptions
2023-07-11 10:11:57,546:INFO:Importing libraries
2023-07-11 10:11:57,546:INFO:Copying training dataset
2023-07-11 10:11:57,554:WARNING:Processing:  23%|################1                                                     | 15/65 [00:16<00:53,  1.08s/it]
2023-07-11 10:11:57,554:INFO:Defining folds
2023-07-11 10:11:57,554:INFO:Declaring metric variables
2023-07-11 10:11:57,554:INFO:Importing untrained model
2023-07-11 10:11:57,555:INFO:Decision Tree Classifier Imported successfully
2023-07-11 10:11:57,555:INFO:Starting cross validation
2023-07-11 10:11:57,556:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 10:11:58,381:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:12:02,251:INFO:Calculating mean and std
2023-07-11 10:12:02,251:WARNING:Processing:  26%|##################3                                                   | 17/65 [00:21<01:16,  1.60s/it]
2023-07-11 10:12:02,251:INFO:Creating metrics dataframe
2023-07-11 10:12:03,129:WARNING:Processing:  28%|###################3                                                  | 18/65 [00:22<01:07,  1.44s/it]
2023-07-11 10:12:03,129:INFO:Uploading results into container
2023-07-11 10:12:03,129:INFO:Uploading model into container now
2023-07-11 10:12:03,129:INFO:_master_model_container: 4
2023-07-11 10:12:03,129:INFO:_display_container: 2
2023-07-11 10:12:03,129:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=5501, splitter='best')
2023-07-11 10:12:03,129:INFO:create_model() successfully completed......................................
2023-07-11 10:12:03,267:INFO:SubProcess create_model() end ==================================
2023-07-11 10:12:03,267:INFO:Creating metrics dataframe
2023-07-11 10:12:03,267:INFO:Initializing SVM - Linear Kernel
2023-07-11 10:12:03,267:INFO:Total runtime is 0.37461246649424235 minutes
2023-07-11 10:12:03,267:INFO:SubProcess create_model() called ==================================
2023-07-11 10:12:03,267:INFO:Initializing create_model()
2023-07-11 10:12:03,267:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E1D8B218B0>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1DD0C5B20>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 10:12:03,267:INFO:Checking exceptions
2023-07-11 10:12:03,267:INFO:Importing libraries
2023-07-11 10:12:03,267:INFO:Copying training dataset
2023-07-11 10:12:03,267:WARNING:Processing:  29%|####################4                                                 | 19/65 [00:22<00:51,  1.12s/it]
2023-07-11 10:12:03,267:INFO:Defining folds
2023-07-11 10:12:03,267:INFO:Declaring metric variables
2023-07-11 10:12:03,267:INFO:Importing untrained model
2023-07-11 10:12:03,267:INFO:SVM - Linear Kernel Imported successfully
2023-07-11 10:12:03,267:INFO:Starting cross validation
2023-07-11 10:12:03,267:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 10:12:03,730:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-11 10:12:03,730:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:12:03,747:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-11 10:12:03,757:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:12:03,986:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-11 10:12:03,991:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:12:03,996:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-11 10:12:03,999:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-11 10:12:04,001:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:12:04,004:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:12:04,035:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-11 10:12:04,035:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-11 10:12:04,051:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:12:04,051:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-11 10:12:04,067:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-11 10:12:04,067:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-11 10:12:04,082:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:12:07,635:INFO:Calculating mean and std
2023-07-11 10:12:07,635:WARNING:Processing:  32%|######################6                                               | 21/65 [00:26<01:08,  1.56s/it]
2023-07-11 10:12:07,635:INFO:Creating metrics dataframe
2023-07-11 10:12:08,437:WARNING:Processing:  34%|#######################6                                              | 22/65 [00:27<00:59,  1.39s/it]
2023-07-11 10:12:08,437:INFO:Uploading results into container
2023-07-11 10:12:08,437:INFO:Uploading model into container now
2023-07-11 10:12:08,452:INFO:_master_model_container: 5
2023-07-11 10:12:08,452:INFO:_display_container: 2
2023-07-11 10:12:08,452:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=5501, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2023-07-11 10:12:08,452:INFO:create_model() successfully completed......................................
2023-07-11 10:12:08,588:INFO:SubProcess create_model() end ==================================
2023-07-11 10:12:08,588:INFO:Creating metrics dataframe
2023-07-11 10:12:08,588:INFO:Initializing Ridge Classifier
2023-07-11 10:12:08,588:INFO:Total runtime is 0.4633052945137024 minutes
2023-07-11 10:12:08,604:INFO:SubProcess create_model() called ==================================
2023-07-11 10:12:08,604:INFO:Initializing create_model()
2023-07-11 10:12:08,604:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E1D8B218B0>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1DD0C5B20>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 10:12:08,604:INFO:Checking exceptions
2023-07-11 10:12:08,604:INFO:Importing libraries
2023-07-11 10:12:08,604:INFO:Copying training dataset
2023-07-11 10:12:08,604:WARNING:Processing:  35%|########################7                                             | 23/65 [00:27<00:45,  1.09s/it]
2023-07-11 10:12:08,604:INFO:Defining folds
2023-07-11 10:12:08,604:INFO:Declaring metric variables
2023-07-11 10:12:08,604:INFO:Importing untrained model
2023-07-11 10:12:08,604:INFO:Ridge Classifier Imported successfully
2023-07-11 10:12:08,604:INFO:Starting cross validation
2023-07-11 10:12:08,604:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 10:12:09,152:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-11 10:12:09,154:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:12:09,267:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-11 10:12:09,267:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:12:09,298:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-11 10:12:09,298:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-11 10:12:09,298:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:12:09,298:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:12:09,314:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-11 10:12:09,334:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:12:09,345:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-11 10:12:09,346:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:12:09,363:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-11 10:12:09,368:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:12:09,370:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-11 10:12:09,374:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-11 10:12:09,375:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:12:09,378:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:12:09,418:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-11 10:12:09,418:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:12:13,043:INFO:Calculating mean and std
2023-07-11 10:12:13,043:WARNING:Processing:  38%|##########################9                                           | 25/65 [00:32<01:02,  1.55s/it]
2023-07-11 10:12:13,043:INFO:Creating metrics dataframe
2023-07-11 10:12:13,851:WARNING:Processing:  40%|############################                                          | 26/65 [00:33<00:53,  1.38s/it]
2023-07-11 10:12:13,851:INFO:Uploading results into container
2023-07-11 10:12:13,851:INFO:Uploading model into container now
2023-07-11 10:12:13,851:INFO:_master_model_container: 6
2023-07-11 10:12:13,851:INFO:_display_container: 2
2023-07-11 10:12:13,851:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=5501, solver='auto',
                tol=0.0001)
2023-07-11 10:12:13,851:INFO:create_model() successfully completed......................................
2023-07-11 10:12:13,988:INFO:SubProcess create_model() end ==================================
2023-07-11 10:12:13,988:INFO:Creating metrics dataframe
2023-07-11 10:12:13,988:INFO:Initializing Random Forest Classifier
2023-07-11 10:12:13,988:INFO:Total runtime is 0.5532944242159525 minutes
2023-07-11 10:12:13,988:INFO:SubProcess create_model() called ==================================
2023-07-11 10:12:13,988:INFO:Initializing create_model()
2023-07-11 10:12:13,988:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E1D8B218B0>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1DD0C5B20>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 10:12:13,988:INFO:Checking exceptions
2023-07-11 10:12:13,988:INFO:Importing libraries
2023-07-11 10:12:13,988:INFO:Copying training dataset
2023-07-11 10:12:14,003:WARNING:Processing:  42%|#############################                                         | 27/65 [00:33<00:41,  1.08s/it]
2023-07-11 10:12:14,003:INFO:Defining folds
2023-07-11 10:12:14,003:INFO:Declaring metric variables
2023-07-11 10:12:14,003:INFO:Importing untrained model
2023-07-11 10:12:14,003:INFO:Random Forest Classifier Imported successfully
2023-07-11 10:12:14,003:INFO:Starting cross validation
2023-07-11 10:12:14,003:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 10:12:15,387:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:12:15,434:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:12:15,466:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:12:15,466:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:12:15,497:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:12:15,513:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:12:15,545:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:12:15,554:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:12:15,571:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:12:15,882:WARNING:C:\Users\didit\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:319: UserWarning: Persisting input arguments took 0.68s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-11 10:12:15,929:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:12:19,348:INFO:Calculating mean and std
2023-07-11 10:12:19,348:WARNING:Processing:  45%|###############################2                                      | 29/65 [00:38<01:02,  1.74s/it]
2023-07-11 10:12:19,348:INFO:Creating metrics dataframe
2023-07-11 10:12:20,180:WARNING:Processing:  46%|################################3                                     | 30/65 [00:39<00:53,  1.53s/it]
2023-07-11 10:12:20,180:INFO:Uploading results into container
2023-07-11 10:12:20,181:INFO:Uploading model into container now
2023-07-11 10:12:20,181:INFO:_master_model_container: 7
2023-07-11 10:12:20,181:INFO:_display_container: 2
2023-07-11 10:12:20,182:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=5501, verbose=0, warm_start=False)
2023-07-11 10:12:20,182:INFO:create_model() successfully completed......................................
2023-07-11 10:12:20,314:INFO:SubProcess create_model() end ==================================
2023-07-11 10:12:20,314:INFO:Creating metrics dataframe
2023-07-11 10:12:20,314:INFO:Initializing Quadratic Discriminant Analysis
2023-07-11 10:12:20,314:INFO:Total runtime is 0.6587375124295552 minutes
2023-07-11 10:12:20,314:INFO:SubProcess create_model() called ==================================
2023-07-11 10:12:20,314:INFO:Initializing create_model()
2023-07-11 10:12:20,314:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E1D8B218B0>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1DD0C5B20>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 10:12:20,314:INFO:Checking exceptions
2023-07-11 10:12:20,314:INFO:Importing libraries
2023-07-11 10:12:20,314:INFO:Copying training dataset
2023-07-11 10:12:20,314:WARNING:Processing:  48%|#################################3                                    | 31/65 [00:39<00:40,  1.19s/it]
2023-07-11 10:12:20,314:INFO:Defining folds
2023-07-11 10:12:20,314:INFO:Declaring metric variables
2023-07-11 10:12:20,314:INFO:Importing untrained model
2023-07-11 10:12:20,314:INFO:Quadratic Discriminant Analysis Imported successfully
2023-07-11 10:12:20,314:INFO:Starting cross validation
2023-07-11 10:12:20,314:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 10:12:20,636:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-07-11 10:12:20,636:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-07-11 10:12:20,652:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-07-11 10:12:20,652:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-07-11 10:12:20,655:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-07-11 10:12:20,684:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-07-11 10:12:20,684:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-07-11 10:12:20,699:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-07-11 10:12:20,715:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-07-11 10:12:20,914:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:12:21,081:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:12:21,097:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:12:21,112:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:12:21,128:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:12:21,128:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:12:21,154:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:12:21,204:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:12:21,220:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:12:21,235:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:12:24,913:INFO:Calculating mean and std
2023-07-11 10:12:24,913:WARNING:Processing:  51%|###################################5                                  | 33/65 [00:44<00:52,  1.65s/it]
2023-07-11 10:12:24,913:INFO:Creating metrics dataframe
2023-07-11 10:12:25,683:WARNING:Processing:  52%|####################################6                                 | 34/65 [00:44<00:44,  1.45s/it]
2023-07-11 10:12:25,683:INFO:Uploading results into container
2023-07-11 10:12:25,683:INFO:Uploading model into container now
2023-07-11 10:12:25,683:INFO:_master_model_container: 8
2023-07-11 10:12:25,683:INFO:_display_container: 2
2023-07-11 10:12:25,683:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2023-07-11 10:12:25,683:INFO:create_model() successfully completed......................................
2023-07-11 10:12:25,819:INFO:SubProcess create_model() end ==================================
2023-07-11 10:12:25,819:INFO:Creating metrics dataframe
2023-07-11 10:12:25,819:INFO:Initializing Ada Boost Classifier
2023-07-11 10:12:25,819:INFO:Total runtime is 0.7504908402760824 minutes
2023-07-11 10:12:25,819:INFO:SubProcess create_model() called ==================================
2023-07-11 10:12:25,835:INFO:Initializing create_model()
2023-07-11 10:12:25,835:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E1D8B218B0>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1DD0C5B20>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 10:12:25,835:INFO:Checking exceptions
2023-07-11 10:12:25,835:INFO:Importing libraries
2023-07-11 10:12:25,835:INFO:Copying training dataset
2023-07-11 10:12:25,835:WARNING:Processing:  54%|#####################################6                                | 35/65 [00:45<00:33,  1.13s/it]
2023-07-11 10:12:25,835:INFO:Defining folds
2023-07-11 10:12:25,835:INFO:Declaring metric variables
2023-07-11 10:12:25,835:INFO:Importing untrained model
2023-07-11 10:12:25,835:INFO:Ada Boost Classifier Imported successfully
2023-07-11 10:12:25,835:INFO:Starting cross validation
2023-07-11 10:12:25,835:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 10:12:26,851:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:12:26,974:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:12:27,035:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:12:27,035:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:12:27,035:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:12:30,852:INFO:Calculating mean and std
2023-07-11 10:12:30,852:WARNING:Processing:  57%|#######################################8                              | 37/65 [00:50<00:47,  1.70s/it]
2023-07-11 10:12:30,852:INFO:Creating metrics dataframe
2023-07-11 10:12:31,730:WARNING:Processing:  58%|########################################9                             | 38/65 [00:50<00:40,  1.51s/it]
2023-07-11 10:12:31,730:INFO:Uploading results into container
2023-07-11 10:12:31,730:INFO:Uploading model into container now
2023-07-11 10:12:31,730:INFO:_master_model_container: 9
2023-07-11 10:12:31,730:INFO:_display_container: 2
2023-07-11 10:12:31,730:INFO:AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=5501)
2023-07-11 10:12:31,730:INFO:create_model() successfully completed......................................
2023-07-11 10:12:31,881:INFO:SubProcess create_model() end ==================================
2023-07-11 10:12:31,881:INFO:Creating metrics dataframe
2023-07-11 10:12:31,881:INFO:Initializing Gradient Boosting Classifier
2023-07-11 10:12:31,881:INFO:Total runtime is 0.8515254616737366 minutes
2023-07-11 10:12:31,881:INFO:SubProcess create_model() called ==================================
2023-07-11 10:12:31,881:INFO:Initializing create_model()
2023-07-11 10:12:31,881:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E1D8B218B0>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1DD0C5B20>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 10:12:31,881:INFO:Checking exceptions
2023-07-11 10:12:31,881:INFO:Importing libraries
2023-07-11 10:12:31,881:INFO:Copying training dataset
2023-07-11 10:12:31,881:WARNING:Processing:  60%|##########################################                            | 39/65 [00:51<00:30,  1.18s/it]
2023-07-11 10:12:31,881:INFO:Defining folds
2023-07-11 10:12:31,881:INFO:Declaring metric variables
2023-07-11 10:12:31,881:INFO:Importing untrained model
2023-07-11 10:12:31,881:INFO:Gradient Boosting Classifier Imported successfully
2023-07-11 10:12:31,881:INFO:Starting cross validation
2023-07-11 10:12:31,881:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 10:12:33,130:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:12:33,155:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:12:33,165:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:12:33,180:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:12:33,188:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:12:33,251:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:12:37,050:INFO:Calculating mean and std
2023-07-11 10:12:37,050:WARNING:Processing:  63%|############################################1                         | 41/65 [00:56<00:42,  1.76s/it]
2023-07-11 10:12:37,050:INFO:Creating metrics dataframe
2023-07-11 10:12:37,928:WARNING:Processing:  65%|#############################################2                        | 42/65 [00:57<00:35,  1.56s/it]
2023-07-11 10:12:37,928:INFO:Uploading results into container
2023-07-11 10:12:37,928:INFO:Uploading model into container now
2023-07-11 10:12:37,928:INFO:_master_model_container: 10
2023-07-11 10:12:37,928:INFO:_display_container: 2
2023-07-11 10:12:37,928:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=5501, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2023-07-11 10:12:37,928:INFO:create_model() successfully completed......................................
2023-07-11 10:12:38,051:INFO:SubProcess create_model() end ==================================
2023-07-11 10:12:38,051:INFO:Creating metrics dataframe
2023-07-11 10:12:38,067:INFO:Initializing Linear Discriminant Analysis
2023-07-11 10:12:38,067:INFO:Total runtime is 0.9546172142028808 minutes
2023-07-11 10:12:38,067:INFO:SubProcess create_model() called ==================================
2023-07-11 10:12:38,067:INFO:Initializing create_model()
2023-07-11 10:12:38,067:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E1D8B218B0>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1DD0C5B20>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 10:12:38,067:INFO:Checking exceptions
2023-07-11 10:12:38,067:INFO:Importing libraries
2023-07-11 10:12:38,067:INFO:Copying training dataset
2023-07-11 10:12:38,067:WARNING:Processing:  66%|##############################################3                       | 43/65 [00:57<00:26,  1.21s/it]
2023-07-11 10:12:38,067:INFO:Defining folds
2023-07-11 10:12:38,067:INFO:Declaring metric variables
2023-07-11 10:12:38,067:INFO:Importing untrained model
2023-07-11 10:12:38,067:INFO:Linear Discriminant Analysis Imported successfully
2023-07-11 10:12:38,067:INFO:Starting cross validation
2023-07-11 10:12:38,067:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 10:12:38,851:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:12:38,867:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:12:38,867:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:12:38,882:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:12:38,899:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:12:38,914:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:12:38,963:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:12:42,851:INFO:Calculating mean and std
2023-07-11 10:12:42,851:WARNING:Processing:  69%|################################################4                     | 45/65 [01:02<00:33,  1.70s/it]
2023-07-11 10:12:42,851:INFO:Creating metrics dataframe
2023-07-11 10:12:43,799:WARNING:Processing:  71%|#################################################5                    | 46/65 [01:03<00:29,  1.53s/it]
2023-07-11 10:12:43,799:INFO:Uploading results into container
2023-07-11 10:12:43,800:INFO:Uploading model into container now
2023-07-11 10:12:43,800:INFO:_master_model_container: 11
2023-07-11 10:12:43,800:INFO:_display_container: 2
2023-07-11 10:12:43,800:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2023-07-11 10:12:43,800:INFO:create_model() successfully completed......................................
2023-07-11 10:12:43,931:INFO:SubProcess create_model() end ==================================
2023-07-11 10:12:43,931:INFO:Creating metrics dataframe
2023-07-11 10:12:43,931:INFO:Initializing Extra Trees Classifier
2023-07-11 10:12:43,931:INFO:Total runtime is 1.0523496349652608 minutes
2023-07-11 10:12:43,931:INFO:SubProcess create_model() called ==================================
2023-07-11 10:12:43,931:INFO:Initializing create_model()
2023-07-11 10:12:43,931:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E1D8B218B0>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1DD0C5B20>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 10:12:43,931:INFO:Checking exceptions
2023-07-11 10:12:43,931:INFO:Importing libraries
2023-07-11 10:12:43,931:INFO:Copying training dataset
2023-07-11 10:12:43,947:WARNING:Processing:  72%|##################################################6                   | 47/65 [01:03<00:21,  1.19s/it]
2023-07-11 10:12:43,948:INFO:Defining folds
2023-07-11 10:12:43,948:INFO:Declaring metric variables
2023-07-11 10:12:43,948:INFO:Importing untrained model
2023-07-11 10:12:43,948:INFO:Extra Trees Classifier Imported successfully
2023-07-11 10:12:43,948:INFO:Starting cross validation
2023-07-11 10:12:43,948:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 10:12:45,251:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:12:45,300:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:12:45,315:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:12:45,361:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:12:45,377:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:12:45,386:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:12:45,387:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:12:45,450:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:12:45,450:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:12:49,299:INFO:Calculating mean and std
2023-07-11 10:12:49,299:WARNING:Processing:  75%|####################################################7                 | 49/65 [01:08<00:28,  1.80s/it]
2023-07-11 10:12:49,299:INFO:Creating metrics dataframe
2023-07-11 10:12:50,067:WARNING:Processing:  77%|#####################################################8                | 50/65 [01:09<00:23,  1.57s/it]
2023-07-11 10:12:50,067:INFO:Uploading results into container
2023-07-11 10:12:50,067:INFO:Uploading model into container now
2023-07-11 10:12:50,067:INFO:_master_model_container: 12
2023-07-11 10:12:50,067:INFO:_display_container: 2
2023-07-11 10:12:50,067:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=5501, verbose=0, warm_start=False)
2023-07-11 10:12:50,067:INFO:create_model() successfully completed......................................
2023-07-11 10:12:50,203:INFO:SubProcess create_model() end ==================================
2023-07-11 10:12:50,203:INFO:Creating metrics dataframe
2023-07-11 10:12:50,219:INFO:Initializing Extreme Gradient Boosting
2023-07-11 10:12:50,219:INFO:Total runtime is 1.1571469863255819 minutes
2023-07-11 10:12:50,219:INFO:SubProcess create_model() called ==================================
2023-07-11 10:12:50,219:INFO:Initializing create_model()
2023-07-11 10:12:50,219:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E1D8B218B0>, estimator=xgboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1DD0C5B20>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 10:12:50,219:INFO:Checking exceptions
2023-07-11 10:12:50,219:INFO:Importing libraries
2023-07-11 10:12:50,219:INFO:Copying training dataset
2023-07-11 10:12:50,219:WARNING:Processing:  78%|######################################################9               | 51/65 [01:09<00:17,  1.22s/it]
2023-07-11 10:12:50,219:INFO:Defining folds
2023-07-11 10:12:50,219:INFO:Declaring metric variables
2023-07-11 10:12:50,219:INFO:Importing untrained model
2023-07-11 10:12:50,219:INFO:Extreme Gradient Boosting Imported successfully
2023-07-11 10:12:50,219:INFO:Starting cross validation
2023-07-11 10:12:50,219:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 10:12:51,154:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:12:51,177:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:12:51,205:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:12:51,251:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:12:51,330:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:12:51,381:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:12:55,185:INFO:Calculating mean and std
2023-07-11 10:12:55,186:WARNING:Processing:  82%|#########################################################             | 53/65 [01:14<00:20,  1.74s/it]
2023-07-11 10:12:55,186:INFO:Creating metrics dataframe
2023-07-11 10:12:56,130:WARNING:Processing:  83%|##########################################################1           | 54/65 [01:15<00:17,  1.56s/it]
2023-07-11 10:12:56,130:INFO:Uploading results into container
2023-07-11 10:12:56,130:INFO:Uploading model into container now
2023-07-11 10:12:56,130:INFO:_master_model_container: 13
2023-07-11 10:12:56,130:INFO:_display_container: 2
2023-07-11 10:12:56,130:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=100, n_jobs=-1, num_parallel_tree=None,
              objective='binary:logistic', predictor=None, ...)
2023-07-11 10:12:56,130:INFO:create_model() successfully completed......................................
2023-07-11 10:12:56,268:INFO:SubProcess create_model() end ==================================
2023-07-11 10:12:56,268:INFO:Creating metrics dataframe
2023-07-11 10:12:56,268:INFO:Initializing Light Gradient Boosting Machine
2023-07-11 10:12:56,268:INFO:Total runtime is 1.257966693242391 minutes
2023-07-11 10:12:56,268:INFO:SubProcess create_model() called ==================================
2023-07-11 10:12:56,268:INFO:Initializing create_model()
2023-07-11 10:12:56,268:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E1D8B218B0>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1DD0C5B20>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 10:12:56,268:INFO:Checking exceptions
2023-07-11 10:12:56,268:INFO:Importing libraries
2023-07-11 10:12:56,268:INFO:Copying training dataset
2023-07-11 10:12:56,268:WARNING:Processing:  85%|###########################################################2          | 55/65 [01:15<00:12,  1.21s/it]
2023-07-11 10:12:56,268:INFO:Defining folds
2023-07-11 10:12:56,268:INFO:Declaring metric variables
2023-07-11 10:12:56,268:INFO:Importing untrained model
2023-07-11 10:12:56,268:INFO:Light Gradient Boosting Machine Imported successfully
2023-07-11 10:12:56,268:INFO:Starting cross validation
2023-07-11 10:12:56,268:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 10:12:57,099:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:12:57,158:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:12:57,166:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:12:57,173:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:12:57,176:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:12:57,237:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:13:01,237:INFO:Calculating mean and std
2023-07-11 10:13:01,237:WARNING:Processing:  88%|#############################################################3        | 57/65 [01:20<00:13,  1.74s/it]
2023-07-11 10:13:01,237:INFO:Creating metrics dataframe
2023-07-11 10:13:02,179:WARNING:Processing:  89%|##############################################################4       | 58/65 [01:21<00:10,  1.56s/it]
2023-07-11 10:13:02,179:INFO:Uploading results into container
2023-07-11 10:13:02,179:INFO:Uploading model into container now
2023-07-11 10:13:02,180:INFO:_master_model_container: 14
2023-07-11 10:13:02,180:INFO:_display_container: 2
2023-07-11 10:13:02,180:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=5501, reg_alpha=0.0, reg_lambda=0.0, silent='warn',
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
2023-07-11 10:13:02,180:INFO:create_model() successfully completed......................................
2023-07-11 10:13:02,300:INFO:SubProcess create_model() end ==================================
2023-07-11 10:13:02,300:INFO:Creating metrics dataframe
2023-07-11 10:13:02,300:INFO:Initializing Dummy Classifier
2023-07-11 10:13:02,300:INFO:Total runtime is 1.3584954142570496 minutes
2023-07-11 10:13:02,300:INFO:SubProcess create_model() called ==================================
2023-07-11 10:13:02,300:INFO:Initializing create_model()
2023-07-11 10:13:02,300:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E1D8B218B0>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E1DD0C5B20>, model_only=True, return_train_score=False, kwargs={})
2023-07-11 10:13:02,300:INFO:Checking exceptions
2023-07-11 10:13:02,300:INFO:Importing libraries
2023-07-11 10:13:02,300:INFO:Copying training dataset
2023-07-11 10:13:02,300:WARNING:Processing:  91%|###############################################################5      | 59/65 [01:21<00:07,  1.20s/it]
2023-07-11 10:13:02,300:INFO:Defining folds
2023-07-11 10:13:02,300:INFO:Declaring metric variables
2023-07-11 10:13:02,300:INFO:Importing untrained model
2023-07-11 10:13:02,300:INFO:Dummy Classifier Imported successfully
2023-07-11 10:13:02,300:INFO:Starting cross validation
2023-07-11 10:13:02,315:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-11 10:13:02,929:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:13:03,037:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:13:03,052:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:13:03,068:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:13:03,084:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:13:03,084:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:13:03,084:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:13:03,115:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:13:03,131:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:13:03,148:WARNING:C:\Users\didit\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-11 10:13:07,004:INFO:Calculating mean and std
2023-07-11 10:13:07,004:WARNING:Processing:  94%|#################################################################6    | 61/65 [01:26<00:06,  1.68s/it]
2023-07-11 10:13:07,004:INFO:Creating metrics dataframe
2023-07-11 10:13:07,899:WARNING:Processing:  95%|##################################################################7   | 62/65 [01:27<00:04,  1.50s/it]
2023-07-11 10:13:07,899:INFO:Uploading results into container
2023-07-11 10:13:07,899:INFO:Uploading model into container now
2023-07-11 10:13:07,899:INFO:_master_model_container: 15
2023-07-11 10:13:07,899:INFO:_display_container: 2
2023-07-11 10:13:07,899:INFO:DummyClassifier(constant=None, random_state=5501, strategy='prior')
2023-07-11 10:13:07,899:INFO:create_model() successfully completed......................................
2023-07-11 10:13:08,037:INFO:SubProcess create_model() end ==================================
2023-07-11 10:13:08,037:INFO:Creating metrics dataframe
2023-07-11 10:13:08,053:WARNING:Processing:  97%|###################################################################8  | 63/65 [01:27<00:02,  1.17s/it]
2023-07-11 10:13:08,053:INFO:Initializing create_model()
2023-07-11 10:13:08,053:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E1D8B218B0>, estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform'), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-07-11 10:13:08,053:INFO:Checking exceptions
2023-07-11 10:13:08,053:INFO:Importing libraries
2023-07-11 10:13:08,053:INFO:Copying training dataset
2023-07-11 10:13:08,053:INFO:Defining folds
2023-07-11 10:13:08,053:INFO:Declaring metric variables
2023-07-11 10:13:08,053:INFO:Importing untrained model
2023-07-11 10:13:08,053:INFO:Declaring custom model
2023-07-11 10:13:08,053:INFO:K Neighbors Classifier Imported successfully
2023-07-11 10:13:08,053:INFO:Cross validation set to False
2023-07-11 10:13:08,053:INFO:Fitting Model
2023-07-11 10:13:08,593:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2023-07-11 10:13:08,593:INFO:create_model() successfully completed......................................
2023-07-11 10:13:08,716:WARNING:Processing: 100%|######################################################################| 65/65 [01:27<00:00,  1.21it/s]
2023-07-11 10:13:08,716:WARNING:                                                                                                                       
2023-07-11 10:13:08,716:INFO:                                    Model  Accuracy     AUC  Recall   Prec.  \
2023-07-11 10:13:08,716:INFO:knn                K Neighbors Classifier    0.9406  0.6057  0.0000  0.0000   
2023-07-11 10:13:08,716:INFO:ridge                    Ridge Classifier    0.9406  0.0000  0.0000  0.0000   
2023-07-11 10:13:08,716:INFO:rf               Random Forest Classifier    0.9406  0.5249  0.0000  0.0000   
2023-07-11 10:13:08,716:INFO:qda       Quadratic Discriminant Analysis    0.9406  0.5000  0.0000  0.0000   
2023-07-11 10:13:08,716:INFO:dummy                    Dummy Classifier    0.9406  0.5000  0.0000  0.0000   
2023-07-11 10:13:08,716:INFO:lr                    Logistic Regression    0.9384  0.7142  0.0000  0.0000   
2023-07-11 10:13:08,716:INFO:et                 Extra Trees Classifier    0.9384  0.5214  0.0000  0.0000   
2023-07-11 10:13:08,716:INFO:gbc          Gradient Boosting Classifier    0.9362  0.7016  0.0500  0.1000   
2023-07-11 10:13:08,716:INFO:lightgbm  Light Gradient Boosting Machine    0.9362  0.6525  0.0500  0.1000   
2023-07-11 10:13:08,716:INFO:xgboost         Extreme Gradient Boosting    0.9341  0.6783  0.0500  0.1000   
2023-07-11 10:13:08,716:INFO:lda          Linear Discriminant Analysis    0.9318  0.7002  0.0000  0.0000   
2023-07-11 10:13:08,716:INFO:ada                  Ada Boost Classifier    0.9275  0.6911  0.0833  0.1333   
2023-07-11 10:13:08,716:INFO:svm                   SVM - Linear Kernel    0.9057  0.0000  0.0833  0.1059   
2023-07-11 10:13:08,716:INFO:dt               Decision Tree Classifier    0.8720  0.5029  0.0833  0.0310   
2023-07-11 10:13:08,716:INFO:nb                            Naive Bayes    0.3721  0.5988  0.8667  0.0778   
2023-07-11 10:13:08,716:INFO:
2023-07-11 10:13:08,716:INFO:              F1   Kappa     MCC  TT (Sec)  
2023-07-11 10:13:08,716:INFO:knn       0.0000  0.0000  0.0000     0.449  
2023-07-11 10:13:08,716:INFO:ridge     0.0000  0.0000  0.0000     0.444  
2023-07-11 10:13:08,716:INFO:rf        0.0000  0.0000  0.0000     0.534  
2023-07-11 10:13:08,716:INFO:qda       0.0000  0.0000  0.0000     0.460  
2023-07-11 10:13:08,716:INFO:dummy     0.0000  0.0000  0.0000     0.469  
2023-07-11 10:13:08,716:INFO:lr        0.0000 -0.0034 -0.0040     0.455  
2023-07-11 10:13:08,716:INFO:et        0.0000 -0.0034 -0.0039     0.535  
2023-07-11 10:13:08,716:INFO:gbc       0.0667  0.0558  0.0587     0.517  
2023-07-11 10:13:08,716:INFO:lightgbm  0.0667  0.0559  0.0588     0.497  
2023-07-11 10:13:08,716:INFO:xgboost   0.0667  0.0537  0.0571     0.497  
2023-07-11 10:13:08,716:INFO:lda       0.0000 -0.0123 -0.0136     0.478  
2023-07-11 10:13:08,716:INFO:ada       0.1000  0.0793  0.0824     0.502  
2023-07-11 10:13:08,716:INFO:svm       0.0767  0.0588  0.0622     0.437  
2023-07-11 10:13:08,716:INFO:dt        0.0450 -0.0123 -0.0099     0.469  
2023-07-11 10:13:08,716:INFO:nb        0.1422  0.0375  0.0980     0.448  
2023-07-11 10:13:08,716:INFO:_master_model_container: 15
2023-07-11 10:13:08,716:INFO:_display_container: 2
2023-07-11 10:13:08,716:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2023-07-11 10:13:08,716:INFO:compare_models() successfully completed......................................
